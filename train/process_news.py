import json
import re
# âœ… å¼•å…¥å…±ç”¨çš„ tokenizer
from data_utils.tokenizer import smart_tokenize 

# ==========================================
# 1. å®šç¾©è¦æ¨™è¨»çš„æ¸…å–®
# ==========================================

# A. æ©Ÿæ§‹ (ORG)
target_orgs = [
    "ä¸­åœ‹åœ‹å®¶éµè·¯é›†åœ˜æœ‰é™å…¬å¸",
    "åœ‹éµé›†åœ˜"
]

# B. åœ°å/åœ°å€ (ADDRESS) - âœ… æ–°å¢é€™å€‹ï¼
# æ ¹æ“šä½ çš„æ–°èå…§å®¹ï¼Œæˆ‘æå–äº†å‡ºç¾éçš„åœ°é»
target_addrs = [
    "å¤§å¶¼å±±", "å±¯é–€", "åŒ—éƒ½", 
    "é•·è´›", "ç€‹ç™½", # é›–ç„¶æ˜¯é«˜éµç·šåï¼Œä½†é€šå¸¸åŒ…å«åœ°åï¼Œè¦–ä¹ä½ æƒ³ä¸æƒ³æ¨™
    "é¦™æ¸¯", "ä¸­åœ‹"
]

# ==========================================
# 2. åŸå§‹æ–‡æœ¬ (ä¸è®Š)
# ==========================================
raw_content = """æ“šä¸­åœ‹åœ‹å®¶éµè·¯é›†åœ˜æœ‰é™å…¬å¸ä»Šï¼ˆ4æ—¥ï¼‰æŠ«éœ²ï¼Œéµè·¯ã€Œåå››äº”ã€å¯¦ç¾åœ“æ»¿æ”¶å®˜ã€‚ã€Œåå››äº”ã€æœŸé–“ï¼Œå…¨åœ‹éµè·¯ç‡Ÿæ¥­é‡Œç¨‹ç”±14.63è¬å…¬é‡Œå¢è‡³16.5è¬å…¬é‡Œã€å¢é•·12.8%ï¼Œé«˜éµç”±3.79è¬å…¬é‡Œå¢è‡³5.04è¬å…¬é‡Œã€å¢é•·32.98%ï¼Œä¸­åœ‹å»ºæˆä¸–ç•Œè¦æ¨¡æœ€å¤§ã€å…ˆé€²ç™¼é”çš„é«˜é€Ÿéµè·¯ç¶²ã€‚

2025å¹´ï¼Œåœ‹éµé›†åœ˜åŠ å¿«å»ºè¨­ç¾ä»£åŒ–éµè·¯åŸºç¤è¨­æ–½é«”ç³»ï¼Œåœ“æ»¿å®Œæˆéµè·¯å»ºè¨­ä»»å‹™ï¼Œå…¨åœ‹éµè·¯å®Œæˆå›ºå®šè³‡ç”¢æŠ•è³‡9015å„„å…ƒäººæ°‘å¹£ã€åŒæ¯”å¢é•·6%ï¼ŒæŠ•ç”¢æ–°ç·š3109å…¬é‡Œï¼Œå…¶ä¸­é«˜éµ2862å…¬é‡Œï¼Œéµè·¯æŠ•è³‡æ‹‰å‹•ä½œç”¨å……åˆ†é¡¯ç¾ã€‚

2025å¹´ï¼Œåœ‹éµé›†åœ˜ä»¥åœ‹å®¶ã€Œåå››äº”ã€è¦åŠƒç¶±è¦ç¢ºå®šçš„102é …é‡å¤§å·¥ç¨‹éµè·¯é …ç›®å’Œã€Œå…©é‡ã€é …ç›®ç‚ºé‡é»ï¼ŒåŠ å¤§å¯¦æ–½åŠ›åº¦ï¼Œé•·è´›é«˜éµç­‰8å€‹é …ç›®é–‹å·¥å»ºè¨­ï¼Œç€‹ç™½é«˜éµç­‰25å€‹é …ç›®é–‹é€šé‹ç‡Ÿã€‚åŠ å¿«ç‰©æµåŸºç¤è¨­æ–½å»ºè¨­ï¼Œå»ºæˆéµè·¯å°ˆç”¨ç·š52æ¢ã€‚

ã€Œåäº”äº”ã€æœŸé–“ï¼Œåœ‹éµé›†åœ˜å°‡é€²ä¸€æ­¥æ¨é€²éµè·¯ç¶²å»ºè¨­ã€‚åˆ°2030å¹´ï¼Œå…¨åœ‹éµè·¯ç‡Ÿæ¥­é‡Œç¨‹é”åˆ°18è¬å…¬é‡Œå·¦å³ï¼Œå…¶ä¸­é«˜éµ6è¬å…¬é‡Œå·¦å³ï¼Œå¾©ç·šç‡å’Œé›»æ°£åŒ–ç‡åˆ†åˆ¥é”åˆ°64%å’Œ78%ï¼Œæˆ°ç•¥éª¨å¹¹é€šé“å…¨é¢åŠ å¼·ï¼Œã€Œå…«ç¸±å…«æ©«ã€é«˜éµç³»çµ±æˆç¶²ï¼Œå€åŸŸäº’è¯äº’é€šæ°´å¹³é¡¯è‘—æå‡ï¼Œè²¨é‹ç¶²çµ¡èƒ½åŠ›å¤§å¹…å¢å¼·ï¼ŒåŸºæœ¬å»ºæˆä¸–ç•Œä¸€æµçš„ç¾ä»£åŒ–éµè·¯ç¶²ã€‚

åŸæ–‡ç¶²å€ï¼šhttps://news.mingpao.com/ins/%E5%85%A9%E5%B2%B8/article/20260104/s00004/1767518315152"""

# ==========================================
# 3. è™•ç†é‚è¼¯
# ==========================================
def process_news(text):
    # A. ç°¡å–®æ¸…æ´—
    if "åŸæ–‡ç¶²å€ï¼š" in text:
        text = text.split("åŸæ–‡ç¶²å€ï¼š")[0]

    # B. åˆ†å¥
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n])', text)
    segments = []
    current_sent = ""
    for s in sentences:
        current_sent += s
        if re.match(r'[ã€‚ï¼ï¼Ÿ\n]', s):
            if current_sent.strip():
                segments.append(current_sent.strip())
            current_sent = ""
    if current_sent.strip(): segments.append(current_sent.strip())

    # C. è‡ªå‹•æ¨™è¨»
    label2id = {
        "O": 0, "B-NAME": 1, "I-NAME": 2, 
        "B-ADDRESS": 3, "I-ADDRESS": 4, 
        "B-PHONE": 5, "I-PHONE": 6, 
        "B-ID": 7, "I-ID": 8, 
        "B-ACCOUNT": 9, "I-ACCOUNT": 10,
        "B-LICENSE_PLATE": 11, "I-LICENSE_PLATE": 12,
        "B-ORG": 13, "I-ORG": 14
    }

    final_data = []

    for sent in segments:
        tokens = smart_tokenize(sent)
        tags = [label2id["O"]] * len(tokens)
        
        # å»ºç«‹ Alignment æ˜ å°„
        token_spans = []
        search_start = 0
        for token in tokens:
            start = sent.find(token, search_start)
            if start == -1:
                token_spans.append(None)
                continue
            end = start + len(token)
            token_spans.append((start, end))
            search_start = end

        # å®šç¾©æ¨™è¨»å‡½æ•¸
        def apply_labels(targets, label_b, label_i):
            for target in targets:
                for match in re.finditer(re.escape(target), sent):
                    match_start, match_end = match.span()
                    
                    for idx, span in enumerate(token_spans):
                        if span is None: continue
                        t_start, t_end = span
                        
                        if t_start >= match_start and t_end <= match_end:
                            if t_start == match_start:
                                if tags[idx] == label2id["O"]:
                                    tags[idx] = label2id[label_b]
                            else:
                                if tags[idx] == label2id["O"]:
                                    tags[idx] = label2id[label_i]

        # åŸ·è¡Œæ¨™è¨»
        # 1. æ¨™è¨»æ©Ÿæ§‹
        apply_labels(target_orgs, "B-ORG", "I-ORG")
        
        # 2. æ¨™è¨»åœ°å âœ… (æ–°å¢é€™è¡Œ)
        apply_labels(target_addrs, "B-ADDRESS", "I-ADDRESS")
        
        if len(tokens) > 0:
            final_data.append({
                "tokens": tokens,
                "ner_tags": tags
            })
            
    return final_data

# ==========================================
# 4. åŸ·è¡Œèˆ‡å„²å­˜
# ==========================================
news_json_data = process_news(raw_content)

output_file = "news_data.json"
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(news_json_data, f, ensure_ascii=False, indent=2)

print(f"âœ… è™•ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(news_json_data)} æ¢æ–°èæ•¸æ“šã€‚")
print(f"   - å·²æ¨™è¨» ORG: {target_orgs}")
print(f"   - å·²æ¨™è¨» ADDRESS: {target_addrs}") # é¡¯ç¤ºå·²æ¨™è¨»çš„åœ°å
print(f"ğŸ“ æª”æ¡ˆå·²å„²å­˜ç‚º: {output_file}")