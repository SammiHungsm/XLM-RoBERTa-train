from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from peft import PeftModel
import re
from collections import defaultdict
import torch

# ==========================================
# 1. Model Setup and Loading
# ==========================================
base_model_name = "Davlan/xlm-roberta-large-ner-hrl"
lora_model_path = "./final_lora_model" 

# å¿…é ˆèˆ‡ prepare_data.py ä¸€è‡´ (15 å€‹æ¨™ç±¤)
label_list = [
    "O", 
    "B-NAME", "I-NAME", 
    "B-ADDRESS", "I-ADDRESS", 
    "B-PHONE", "I-PHONE", 
    "B-ID", "I-ID", 
    "B-ACCOUNT", "I-ACCOUNT", 
    "B-LICENSE_PLATE", "I-LICENSE_PLATE",
    "B-ORG", "I-ORG"
]

label2id = {l: i for i, l in enumerate(label_list)}
id2label = {i: l for l, i in label2id.items()}

print("Loading model...")
try:
    tokenizer = AutoTokenizer.from_pretrained(lora_model_path)
    base_model = AutoModelForTokenClassification.from_pretrained(
        base_model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True
    )
    model = PeftModel.from_pretrained(base_model, lora_model_path)
    model.eval()
except Exception as e:
    print(f"Model loading failed: {e}")
    print("æç¤ºï¼šå¦‚æœå ±éŒ¯ Size Mismatchï¼Œè«‹ç¢ºèªä½ æ˜¯å¦å·²ç¶“åˆªé™¤èˆŠçš„ output è³‡æ–™å¤¾ä¸¦é‡æ–°è¨“ç·´ã€‚")
    exit()

# âœ… CPU version: device=-1
nlp = pipeline(
    "token-classification",
    model=model,
    tokenizer=tokenizer,
    aggregation_strategy="simple",
    device=-1
)

# ==========================================
# 2. Helper Functions
# ==========================================
def analyze_text_composition(text):
    if not text: return "N/A"
    chi_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    eng_chars = len(re.findall(r'[a-zA-Z]', text))
    total = chi_chars + eng_chars
    if total == 0: return "No alphanumeric"
    return f"Chinese: {chi_chars/total*100:.1f}% | English: {eng_chars/total*100:.1f}%"

def clean_and_process_entities(results, text):
    cleaned = []
    skip_next = False
    
    blacklist = ["å¥åœ¨", "ä¸è©³", "æœªçŸ¥", "ç„¡æ¥­", "é›¢ç•°", "å–®èº«", "ä¸ä¾¿", "æ•´åˆ", "è™•ç†", "éŒ¯èª¤"]

    for i in range(len(results)):
        if skip_next:
            skip_next = False
            continue
            
        curr = results[i]
        if curr['end'] > len(text): curr['end'] = len(text)
        
        numeric_labels = ['PHONE', 'ID', 'ACCOUNT', 'HKID', 'LICENSE_PLATE']
        if curr['entity_group'] in numeric_labels:
            while curr['start'] > 0 and text[curr['start'] - 1] in "0123456789()+- ":
                curr['start'] -= 1
            while curr['end'] < len(text) and text[curr['end']] in "0123456789()ABCDEFGHIJKLMNOPQRSTUVWXYZ":
                curr['end'] += 1
            curr['word'] = text[curr['start']:curr['end']]
        
        word = text[curr['start']:curr['end']].strip()
        
        if word in blacklist:
            continue

        if curr['entity_group'] == 'NAME' and len(word) == 1 and not re.match(r'[a-zA-Z]', word):
            if curr['score'] < 0.995:
                continue

        if re.match(r'^[\W_]+$', word):
            continue

        if i < len(results) - 1:
            next_e = results[i+1]
            gap_text = text[curr['end']:next_e['start']]
            
            if re.match(r'^[\s-]*$', gap_text): 
                if curr['entity_group'] == 'PHONE' and next_e['entity_group'] == 'ID':
                    new_entity = curr.copy()
                    new_entity['end'] = next_e['end']
                    new_entity['word'] = text[curr['start']:next_e['end']]
                    new_entity['score'] = (curr['score'] + next_e['score']) / 2
                    cleaned.append(new_entity)
                    skip_next = True
                    continue
                
                if curr['entity_group'] == next_e['entity_group']:
                    new_entity = curr.copy()
                    new_entity['end'] = next_e['end']
                    new_entity['word'] = text[curr['start']:next_e['end']]
                    new_entity['score'] = (curr['score'] + next_e['score']) / 2
                    cleaned.append(new_entity)
                    skip_next = True
                    continue

        cleaned.append(curr)

    cleaned.sort(key=lambda x: x['start'])
    final_output = []
    counters = defaultdict(int)
    entity_registry = {} 
    
    for entity in cleaned:
        label = entity['entity_group']
        word_content = text[entity['start']:entity['end']].strip()
        dict_key = (label, word_content)
        
        if dict_key in entity_registry:
            seq_num = entity_registry[dict_key]
        else:
            counters[label] += 1
            seq_num = counters[label]
            entity_registry[dict_key] = seq_num
        
        entity['numbered_tag'] = f"{label}-{seq_num}"
        final_output.append(entity)
        
    return final_output

def mask_text(text, entities):
    masked_text = text
    for entity in sorted(entities, key=lambda x: x['start'], reverse=True):
        start, end = entity['start'], entity['end']
        tag = f"[{entity['numbered_tag']}]"
        masked_text = masked_text[:start] + tag + masked_text[end:]
    return masked_text

# ==========================================
# 3. Test Inputs
# ==========================================
test_inputs = [
    "æå˜‰èª å¥½æœ‰éŒ¢ï¼Œä»²è¦ä½ä¿‚é¦™æ¸¯ä¸­ç’°çš‡åå¤§é“ä¸­ 33 è™Ÿè¬å®œå¤§å»ˆ 12 æ¨“ï¼Œå¹´é½¡ 82 æ­²ã€‚å…¶å‡ºç”Ÿåœ°æœªæœ‰è¨˜éŒ„ï¼Œçˆ¶æ¯ä¸å¥åœ¨ï¼Œç›®å‰å¾äº‹å…¼è·å·¥ä½œã€‚è¯çµ¡é›»è©±ç‚º +852 9123 4567ã€‚æ›¾ä»»è·æ–¼é•·å’Œä¸»å¸­ã€‚",
    "Li Ka-shing is widely recognized as one of the wealthiest individuals in Hong Kong, with a reputation that extends far beyond the city itself. He currently resides at 12/F, Man Yee Building, 33 Queenâ€™s Road Central. At the age of 82.",
    "Li Ka-shing is very wealthy and resides at 12/F, Man Yee Building, 33 Queenâ€™s Road Central, Hong Kong. His contact number is +852 9123 4567. He previously served as Chairman of Cheung Kong Holdings.",
    "å·²çŸ¥æå˜‰èª å±…ä½æ–¼ Hong Kong Kwun Tong 99 è™Ÿ AIA Tower 8/Fï¼Œä»–ä»Šå¹´ 31 æ­²ï¼Œé›»è©±è™Ÿç¢¼ç‚º +852 9167 8920ï¼Œæ›¾ç¶“æ“”ä»»é Deliveroo å¤–è³£å“¡ä¸€è·ï¼Œæˆ‘æƒ³çŸ¥åŒä½¢æœ‰é—œå˜…äººå˜…è³‡æ–™",
    "å·²çŸ¥ A å›ç¾å±…æ–¼é¦™æ¸¯è§€å¡˜é“ 99 è™Ÿ AIA Tower å…«æ¨“ï¼Œå¹´é½¡ 31 æ­²ã€‚è¯çµ¡é›»è©±ç‚º +852 9167 8920ã€‚éå¾€æ›¾ä»»è· Deliveroo å¤–è³£å“¡ï¼Œå…·å‚™ç›¸é—œå·¥ä½œç¶“é©—ã€‚è«‹ç‚ºæˆ‘æœå°‹ç›¸é—œè³‡æ–™",
    "æˆ‘å«æå˜‰èª ï¼Œæˆ‘å˜…èº«ä»½è­‰è™Ÿç¢¼ä¿‚ R1234567(A)ï¼Œæˆ‘ä¿‚12æœˆ1è™Ÿä¸‹åˆå˜—è©¦ç”³è«‹å¼·ç©é‡‘æ•´åˆï¼Œä½†æœªèƒ½æˆåŠŸï¼Œä¸¦é¡¯ç¤ºå¤æ€ªéŒ¯èª¤ï¼Œè«‹ç›¡å¿«è™•ç†ã€‚",
    "ä½ å¥½ï¼Œæˆ‘ä¿‚ Sammiã€‚æˆ‘ä½ä¿‚ Tuen Mun å±¯é–€å¸‚å»£å ´ 10 æ¨“ã€‚",
    "æˆ‘çš„é›»è©±ä¿‚ 9123 4567ã€‚èº«åˆ†è­‰ A123456(7)ã€‚",
    "Sammi ä¹‹å‰æ‰“éé»ã€‚",
    "Edmondæ¢ï¼Œèº«é«˜185cmï¼Œå±…ä½æ–¼é¦™æ¸¯è§€å¡˜ AIA Tower 31æ¨“ï¼ŒBank Account = 274542182882ç¾ä»»Alibaba CEOï¼Œé›»è©±ç‚º 21678080ï¼Œèº«ä»½è­‰è™Ÿç‚º R98272829ã€‚"
]

print("=" * 60)
for text in test_inputs:
    print(f"\nOriginal Input: {text}")
    print(f"ğŸ“Š {analyze_text_composition(text)}")
    
    raw_results = nlp(text)
    processed_entities = clean_and_process_entities(raw_results, text)
    masked_result = mask_text(text, processed_entities)
    
    print("-" * 30)
    print(f"Masked Result: {masked_result}")
    print("Detected Entities:")
    for e in processed_entities:
        print(f" - {text[e['start']:e['end']]} | {e['numbered_tag']} ({e['score']:.1%})")
    print("=" * 60)