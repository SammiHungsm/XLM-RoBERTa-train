import random
import json
from faker import Faker

# å¼•å…¥è‡ªå®šç¾©æ¨¡çµ„
# æ³¨æ„ï¼šä½ éœ€è¦ç¢ºä¿ train/data_utils/__init__.py å­˜åœ¨
from data_utils.generators import get_random_fillers
from data_utils.loaders import load_names, load_addresses, load_negative_samples, load_pre_annotated_data
from data_utils.templates import get_all_templates

fake = Faker(['en_US', 'zh_TW'])

def create_dataset_safe(names, addresses, label2id, negative_texts=[], target_count=None):
    data = []
    templates = get_all_templates() # å¾ templates.py ç²å–
    
    if target_count is None: target_count = len(addresses)

    # æ­£è² æ¨£æœ¬æ¯”ä¾‹ (85% : 15%)
    # ç•™æ„ï¼štemplates æœ¬èº«å·²ç¶“åŒ…å«äº†å¾ˆå¤šå°æŠ—æ¨£æœ¬ (Boundary/Anti-hallucination)
    # é€™è£¡çš„ negative_texts æ˜¯æŒ‡ã€Œç´”å°èªª/æ–°èã€æ–‡æœ¬
    pos_count = int(target_count * 0.85)
    neg_count = target_count - pos_count
    
    print(f"ğŸš€ ç”Ÿæˆæ•¸æ“šä¸­... (Templateç”Ÿæˆ: {pos_count}, ç´”æ–‡æœ¬è² æ¨£æœ¬: {neg_count})")
    
    for _ in range(pos_count):
        template_parts = random.choice(templates)
        
        # ä½¿ç”¨ generators.py çš„é‚è¼¯ç²å–å¡«å……å…§å®¹
        fillers = get_random_fillers(names, addresses)
        
        full_tokens = []
        full_tags = []
        
        for part in template_parts:
            # æª¢æŸ¥æ˜¯å¦ç‚ºè®Šæ•¸
            if part in fillers:
                entity_text = fillers[part]
                entity_type = "O"
                if part == "{name}": entity_type = "NAME"
                elif part == "{addr}": entity_type = "ADDRESS"
                elif part == "{phone}": entity_type = "PHONE"
                elif part == "{id_num}": entity_type = "ID"
                elif part == "{account}": entity_type = "ACCOUNT"
                elif part == "{plate}": entity_type = "LICENSE_PLATE"
                elif part == "{org}": entity_type = "ORG"
                
                chars = list(entity_text)
                if not chars: continue
                full_tokens.extend(chars)
                if entity_type != "O":
                    full_tags.append(label2id[f"B-{entity_type}"])
                    full_tags.extend([label2id[f"I-{entity_type}"]] * (len(chars) - 1))
                else:
                    full_tags.extend([label2id["O"]] * len(chars))
            else:
                # å›ºå®šæ–‡å­— (åŒ…æ‹¬é™·é˜±è©) æ¨™è¨˜ç‚º O
                chars = list(part)
                if not chars: continue
                full_tokens.extend(chars)
                full_tags.extend([label2id["O"]] * len(chars))
        
        data.append({"tokens": full_tokens, "ner_tags": full_tags})
    
    # åŠ å…¥ç´”æ–‡æœ¬è² æ¨£æœ¬ (Novel/News raw text)
    if negative_texts:
        for i in range(neg_count):
            sent = negative_texts[i % len(negative_texts)]
            full_tokens = list(sent)
            full_tags = [label2id["O"]] * len(full_tokens)
            data.append({"tokens": full_tokens, "ner_tags": full_tags})
    else:
        # Fallback Faker sentences
        for _ in range(neg_count):
            sent = fake.sentence()
            full_tokens = list(sent)
            full_tags = [label2id["O"]] * len(full_tokens)
            data.append({"tokens": full_tokens, "ner_tags": full_tags})

    random.shuffle(data)
    return data

if __name__ == "__main__":
    label_list = ["O", "B-NAME", "I-NAME", "B-ADDRESS", "I-ADDRESS", "B-PHONE", "I-PHONE", "B-ID", "I-ID", "B-ACCOUNT", "I-ACCOUNT", "B-LICENSE_PLATE", "I-LICENSE_PLATE", "B-ORG", "I-ORG"]
    label2id = {l: i for i, l in enumerate(label_list)}

    # 1. è¼‰å…¥å¤–éƒ¨æ•¸æ“š
    names_pool = load_names("./Chinese-Names-Corpus-master") 
    addr_pool = load_addresses("./geojson_files")
    negative_pool = load_negative_samples("./negative_corpus", max_samples=10000)

    # 2. ç”Ÿæˆåˆæˆæ•¸æ“š
    training_data = create_dataset_safe(
        names_pool, 
        addr_pool, 
        label2id, 
        negative_texts=negative_pool,
        target_count=50000 
    )
# 3. åˆä½µé è™•ç†æ•¸æ“š (é€²è¡Œ Upsampling / å€å¢)
    # æˆ‘å€‘å°‡çœŸå¯¦æ•¸æ“šé‡è¤‡å¤šæ¬¡ï¼Œç¢ºä¿æ¨¡å‹åœ¨è¨“ç·´æ™‚ã€Œå¤šçœ‹å¹¾çœ¼ã€
    
    novel_data = load_pre_annotated_data("novel_data.json")
    news_data = load_pre_annotated_data("news_data.json")
    mtr_data = load_pre_annotated_data("mtr_news_data.json")

    # å°èªªæ•¸æ“šé‡å°šå¯ï¼Œé‡è¤‡ 5 æ¬¡
    if novel_data:
        print(f"ğŸ“ˆ å°‡å°èªªæ•¸æ“šå€å¢ 5 å€ (ç¸½æ•¸: {len(novel_data) * 5})")
        training_data.extend(novel_data * 5)

    # æ–°èæ•¸æ“šæ¥µå°‘ä½†æ¥µé‡è¦ (æ•™å°å¿½ç•¥æ•¸å­—)ï¼Œé‡è¤‡ 50 æ¬¡ï¼
    if news_data:
        print(f"ğŸ“ˆ å°‡éµè·¯æ–°èæ•¸æ“šå€å¢ 50 å€ (ç¸½æ•¸: {len(news_data) * 50})")
        training_data.extend(news_data * 50)

    # æ¸¯éµæ•¸æ“šæ˜¯æ··åˆæ¨£æœ¬ï¼Œé‡è¤‡ 50 æ¬¡ï¼
    if mtr_data:
        print(f"ğŸ“ˆ å°‡æ¸¯éµæ–°èæ•¸æ“šå€å¢ 50 å€ (ç¸½æ•¸: {len(mtr_data) * 50})")
        training_data.extend(mtr_data * 50)
        
    random.shuffle(training_data)
    print(f"ğŸš€ æœ€çµ‚æ•¸æ“šé›†ç¸½é‡: {len(training_data)} æ¢")

    output_data = {
        "data": training_data, 
        "label2id": label2id, 
        "id2label": {str(v): k for k, v in label2id.items()}
    }
    
    with open("train_data_lora.json", "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False)
        
    print("âœ… æ•¸æ“šæº–å‚™å®Œæˆï¼train_data_lora.json å·²æ›´æ–°ã€‚")