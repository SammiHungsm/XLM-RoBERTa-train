import random
import json
import re
from faker import Faker
from data_utils.tokenizer import smart_tokenize

# å¼•å…¥è‡ªå®šç¾©æ¨¡çµ„
# æ³¨æ„ï¼šä½ éœ€è¦ç¢ºä¿ train/data_utils/__init__.py å­˜åœ¨
from data_utils.generators import get_random_fillers
from data_utils.loaders import load_names, load_addresses, load_negative_samples, load_pre_annotated_data
from data_utils.templates import get_all_templates

fake = Faker(['en_US', 'zh_TW'])

def smart_tokenize(text):
    """
    æ™ºèƒ½åˆ‡åˆ† (æ”¹è‰¯ç‰ˆ - å»é™¤ç´”ç©ºæ ¼ token)ï¼š
    1. ä¸­æ–‡/ç¬¦è™Ÿï¼šæŒ‰å­—åˆ‡åˆ†
    2. è‹±æ–‡/æ•¸å­—ï¼šæŒ‰å–®è©åˆ‡åˆ†
    3. éæ¿¾æ‰ç´”ç©ºæ ¼
    """
    result = []
    current_eng = ""
    
    # éæ­·æ¯ä¸€å€‹å­—ç¬¦
    for char in text:
        # å¦‚æœæ˜¯è‹±æ–‡æˆ–æ•¸å­—ï¼Œæš«å­˜åˆ° current_eng
        if re.match(r'[a-zA-Z0-9]', char):
            current_eng += char
        else:
            # å¦‚æœé‡åˆ°éè‹±æ–‡æ•¸å­— (å¦‚ä¸­æ–‡ã€ç©ºæ ¼ã€æ¨™é»)
            
            # 1. å…ˆçµç®—ä¹‹å‰çš„è‹±æ–‡è©
            if current_eng:
                result.append(current_eng)
                current_eng = ""
            
            # 2. è™•ç†ç•¶å‰å­—ç¬¦ï¼šåªæœ‰ç•¶å®ƒã€Œä¸æ˜¯ç©ºæ ¼ã€æ™‚æ‰åŠ å…¥
            if char.strip():  # <--- æ–°å¢é€™è¡Œæª¢æŸ¥ï¼
                result.append(char)
            
    # å¾ªç’°çµæŸå¾Œï¼Œæª¢æŸ¥æ˜¯å¦æœ‰éºç•™çš„è‹±æ–‡è©
    if current_eng:
        result.append(current_eng)
        
    return result

def create_dataset_safe(names, addresses, label2id, negative_texts=[], target_count=None):
    data = []
    templates = get_all_templates() # å¾ templates.py ç²å–
    
    if target_count is None: target_count = len(addresses)

    # æ­£è² æ¨£æœ¬æ¯”ä¾‹ (85% : 15%)
    pos_count = int(target_count * 0.85)
    neg_count = target_count - pos_count
    
    print(f"ğŸš€ ç”Ÿæˆæ•¸æ“šä¸­... (Templateç”Ÿæˆ: {pos_count}, ç´”æ–‡æœ¬è² æ¨£æœ¬: {neg_count})")
    
    for _ in range(pos_count):
        template_parts = random.choice(templates)
        
        # ä½¿ç”¨ generators.py çš„é‚è¼¯ç²å–å¡«å……å…§å®¹
        fillers = get_random_fillers(names, addresses)
        
        full_tokens = []
        full_tags = []
        
        for part in template_parts:
            # 1. ç²å–æ–‡æœ¬å…§å®¹
            text_segment = ""
            entity_type = "O"
            
            if part in fillers:
                text_segment = fillers[part]
                # åˆ¤æ–·å¯¦é«”é¡å‹
                if part == "{name}": entity_type = "NAME"
                elif part == "{addr}": entity_type = "ADDRESS"
                elif part == "{phone}": entity_type = "PHONE"
                elif part == "{id_num}": entity_type = "ID"
                elif part == "{account}": entity_type = "ACCOUNT"
                elif part == "{plate}": entity_type = "LICENSE_PLATE"
                elif part == "{org}": entity_type = "ORG"
            else:
                # å›ºå®šæ–‡å­— (åŒ…æ‹¬é™·é˜±è©)
                text_segment = part

            # 2. ä½¿ç”¨ Smart Tokenize ä»£æ›¿ list() [æ ¸å¿ƒä¿®æ”¹é»]
            tokens = smart_tokenize(text_segment)
            
            if not tokens: continue
            
            full_tokens.extend(tokens)
            
            # 3. ç”Ÿæˆå°æ‡‰çš„ Tags (æŒ‰ Token æ•¸é‡)
            if entity_type != "O":
                # ç¬¬ä¸€å€‹ token æ˜¯ B-TAG
                full_tags.append(label2id[f"B-{entity_type}"])
                # å‰©ä¸‹çš„ tokens æ˜¯ I-TAG
                if len(tokens) > 1:
                    full_tags.extend([label2id[f"I-{entity_type}"]] * (len(tokens) - 1))
            else:
                full_tags.extend([label2id["O"]] * len(tokens))
        
        data.append({"tokens": full_tokens, "ner_tags": full_tags})
    
    # åŠ å…¥ç´”æ–‡æœ¬è² æ¨£æœ¬ (Novel/News raw text)
    if negative_texts:
        for i in range(neg_count):
            sent = negative_texts[i % len(negative_texts)]
            # é€™è£¡ä¹Ÿæ”¹ç”¨ smart_tokenize
            full_tokens = smart_tokenize(sent)
            full_tags = [label2id["O"]] * len(full_tokens)
            data.append({"tokens": full_tokens, "ner_tags": full_tags})
    else:
        # Fallback Faker sentences
        for _ in range(neg_count):
            sent = fake.sentence()
            full_tokens = smart_tokenize(sent)
            full_tags = [label2id["O"]] * len(full_tokens)
            data.append({"tokens": full_tokens, "ner_tags": full_tags})

    random.shuffle(data)
    return data

if __name__ == "__main__":
    label_list = ["O", "B-NAME", "I-NAME", "B-ADDRESS", "I-ADDRESS", "B-PHONE", "I-PHONE", "B-ID", "I-ID", "B-ACCOUNT", "I-ACCOUNT", "B-LICENSE_PLATE", "I-LICENSE_PLATE", "B-ORG", "I-ORG"]
    label2id = {l: i for i, l in enumerate(label_list)}

    # 1. è¼‰å…¥å¤–éƒ¨æ•¸æ“š (è«‹ç¢ºä¿é€™äº›è·¯å¾‘ä¸‹çš„æ–‡ä»¶å­˜åœ¨)
    # å»ºè­°ï¼šå¯ä»¥ä½¿ç”¨ Config å­—å…¸ç®¡ç†è·¯å¾‘ï¼Œä½†ç‚ºäº†ä¿æŒä»£ç¢¼ç°¡å–®ï¼Œé€™è£¡ç¶­æŒåŸæ¨£
    try:
        names_pool = load_names("./Chinese-Names-Corpus-master") 
        addr_pool = load_addresses("./geojson_files")
        negative_pool = load_negative_samples("./negative_corpus", max_samples=10000)
    except FileNotFoundError as e:
        print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°æ•¸æ“šæ–‡ä»¶ ({e})ï¼Œè«‹æª¢æŸ¥è·¯å¾‘ã€‚å°‡ä½¿ç”¨ç©ºæ•¸æ“šç¹¼çºŒ...")
        names_pool, addr_pool, negative_pool = [], [], []

    # 2. ç”Ÿæˆåˆæˆæ•¸æ“š
    # æ³¨æ„ï¼šå¦‚æœ addr_pool ç‚ºç©ºï¼Œé€™è£¡å¯èƒ½æœƒå ±éŒ¯æˆ–ç”Ÿæˆç©ºæ•¸æ“š
    training_data = create_dataset_safe(
        names_pool, 
        addr_pool, 
        label2id, 
        negative_texts=negative_pool,
        target_count=50000 
    )

    # 3. åˆä½µé è™•ç†æ•¸æ“š (é€²è¡Œ Upsampling / å€å¢)
    # æˆ‘å€‘å°‡çœŸå¯¦æ•¸æ“šé‡è¤‡å¤šæ¬¡ï¼Œç¢ºä¿æ¨¡å‹åœ¨è¨“ç·´æ™‚ã€Œå¤šçœ‹å¹¾çœ¼ã€
    novel_data = load_pre_annotated_data("novel_data.json")
    news_data = load_pre_annotated_data("news_data.json")
    mtr_data = load_pre_annotated_data("mtr_news_data.json")

    # å°èªªæ•¸æ“šé‡å°šå¯ï¼Œé‡è¤‡ 5 æ¬¡
    if novel_data:
        print(f"ğŸ“ˆ å°‡å°èªªæ•¸æ“šå€å¢ 5 å€ (ç¸½æ•¸: {len(novel_data) * 5})")
        training_data.extend(novel_data * 5)

    # æ–°èæ•¸æ“šæ¥µå°‘ä½†æ¥µé‡è¦ (æ•™å°å¿½ç•¥æ•¸å­—)ï¼Œé‡è¤‡ 50 æ¬¡ï¼
    if news_data:
        print(f"ğŸ“ˆ å°‡éµè·¯æ–°èæ•¸æ“šå€å¢ 50 å€ (ç¸½æ•¸: {len(news_data) * 50})")
        training_data.extend(news_data * 50)

    # æ¸¯éµæ•¸æ“šæ˜¯æ··åˆæ¨£æœ¬ï¼Œé‡è¤‡ 50 æ¬¡ï¼
    if mtr_data:
        print(f"ğŸ“ˆ å°‡æ¸¯éµæ–°èæ•¸æ“šå€å¢ 50 å€ (ç¸½æ•¸: {len(mtr_data) * 50})")
        training_data.extend(mtr_data * 50)
        
    random.shuffle(training_data)
    print(f"ğŸš€ æœ€çµ‚æ•¸æ“šé›†ç¸½é‡: {len(training_data)} æ¢")

    output_data = {
        "data": training_data, 
        "label2id": label2id, 
        "id2label": {str(v): k for k, v in label2id.items()}
    }
    
    with open("train_data_lora.json", "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False)
        
    print("âœ… æ•¸æ“šæº–å‚™å®Œæˆï¼train_data_lora.json å·²æ›´æ–°ã€‚")