from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from peft import PeftModel
import re
from collections import defaultdict

# ==========================================
# 1. è¨­å®šèˆ‡è¼‰å…¥æ¨¡å‹
# ==========================================
base_model_name = "Davlan/xlm-roberta-large-ner-hrl"
lora_model_path = "./final_lora_model" 

# å®šç¾©æ¨™ç±¤
label_list = ["O", "B-NAME", "I-NAME", "B-ADDRESS", "I-ADDRESS", "B-PHONE", "I-PHONE", "B-ID", "I-ID", "B-ACCOUNT", "I-ACCOUNT"]
label2id = {l: i for i, l in enumerate(label_list)}
id2label = {i: l for l, i in label2id.items()}

print("æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
try:
    tokenizer = AutoTokenizer.from_pretrained(lora_model_path)
    base_model = AutoModelForTokenClassification.from_pretrained(
        base_model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True
    )
    model = PeftModel.from_pretrained(base_model, lora_model_path)
    model.eval()
except Exception as e:
    print(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    exit()

nlp = pipeline("token-classification", model=model, tokenizer=tokenizer, aggregation_strategy="simple", device=0)

# ==========================================
# 2. è¼”åŠ©å‡½æ•¸ï¼šæ•¸æ“šåˆ†ä½ˆåˆ†æ (Data Percentage)
# ==========================================
def analyze_text_composition(text):
    """è¨ˆç®—å­—ä¸²ä¸­çš„ä¸­è‹±æ–‡æ¯”ä¾‹"""
    if not text: return "N/A"
    
    chi_chars = len(re.findall(r'[\u4e00-\u9fff]', text)) # ä¸­æ–‡
    eng_chars = len(re.findall(r'[a-zA-Z]', text))       # è‹±æ–‡
    total_valid = chi_chars + eng_chars
    
    if total_valid == 0: return "No alphanumeric content"
    
    chi_pct = (chi_chars / total_valid) * 100
    eng_pct = (eng_chars / total_valid) * 100
    
    return f"ä¸­æ–‡: {chi_pct:.1f}% | è‹±æ–‡: {eng_pct:.1f}% (Total: {total_valid} chars)"

# ==========================================
# 3. æ ¸å¿ƒå¾Œè™•ç†é‚è¼¯
# ==========================================
def clean_and_process_entities(results, text):
    """
    1. åˆä½µæ–·è£‚å¯¦é«”
    2. éæ¿¾é›œè¨Š
    3. æ™ºèƒ½ç·¨è™Ÿ (ç›¸åŒå…§å®¹å…±ç”¨ ID)
    """
    # --- æ­¥é©Ÿ A: åˆæ­¥æ¸…ç†èˆ‡åˆä½µ ---
    cleaned = []
    skip_next = False
    
    for i in range(len(results)):
        if skip_next:
            skip_next = False
            continue
            
        curr = results[i]
        word = text[curr['start']:curr['end']].strip() 
        
        # 1. éæ¿¾ç´”æ¨™é»
        if re.match(r'^[\W_]+$', word):
            continue
            
        # 2. éæ¿¾æ¥µçŸ­èª¤åˆ¤
        if len(word) <= 1 and curr['score'] < 0.9:
            continue

        # 3. åˆä½µ Phone + ID
        if i < len(results) - 1:
            next_e = results[i+1]
            if curr['entity_group'] == 'PHONE' and next_e['entity_group'] == 'ID':
                gap = next_e['start'] - curr['end']
                if gap <= 2:
                    new_entity = curr.copy()
                    new_entity['end'] = next_e['end']
                    new_entity['word'] = text[curr['start']:next_e['end']]
                    new_entity['score'] = (curr['score'] + next_e['score']) / 2
                    cleaned.append(new_entity)
                    skip_next = True
                    continue

        cleaned.append(curr)

    # --- æ­¥é©Ÿ B: åˆ†é…ç·¨è™Ÿ ---
    cleaned.sort(key=lambda x: x['start'])
    
    final_output = []
    counters = defaultdict(int)
    entity_registry = {} 
    
    for entity in cleaned:
        label = entity['entity_group']
        word_content = text[entity['start']:entity['end']].strip()
        
        dict_key = (label, word_content)
        
        if dict_key in entity_registry:
            seq_num = entity_registry[dict_key]
        else:
            counters[label] += 1
            seq_num = counters[label]
            entity_registry[dict_key] = seq_num
        
        tag_with_num = f"{label}-{seq_num}"
        entity['numbered_tag'] = tag_with_num
        final_output.append(entity)
        
    return final_output

def mask_text(text, entities):
    masked_text = text
    # å€’åºæ›¿æ›
    for entity in sorted(entities, key=lambda x: x['start'], reverse=True):
        start = entity['start']
        end = entity['end']
        tag = f"[{entity['numbered_tag']}]"
        masked_text = masked_text[:start] + tag + masked_text[end:]
    return masked_text

# ==========================================
# 4. æ¸¬è©¦åŸ·è¡Œ
# ==========================================
test_inputs = [
    "æå˜‰èª å¥½æœ‰éŒ¢ï¼Œä»²è¦ä½ä¿‚é¦™æ¸¯ä¸­ç’°çš‡åå¤§é“ä¸­ 33 è™Ÿè¬å®œå¤§å»ˆ 12 æ¨“ï¼Œå¹´é½¡ 82 æ­²ã€‚å…¶å‡ºç”Ÿåœ°æœªæœ‰è¨˜éŒ„ï¼Œçˆ¶æ¯ä¸å¥åœ¨ï¼Œç›®å‰å¾äº‹å…¼è·å·¥ä½œã€‚è¯çµ¡é›»è©±ç‚º +852 9123 4567ã€‚æ›¾ä»»è·æ–¼é•·å’Œä¸»å¸­ã€‚",
    "Li Ka-shing is widely recognized as one of the wealthiest individuals in Hong Kong, with a reputation that extends far beyond the city itself. He currently resides at 12/F, Man Yee Building, 33 Queenâ€™s Road Central, located in the heart of Hong Kongâ€™s Central district â€” an area known for its financial institutions, luxury offices, and bustling commercial activity. At the age of 82, he has lived through decades of change in Hong Kongâ€™s economic and social landscape.",
    "Li Ka-shing is very wealthy and resides at 12/F, Man Yee Building, 33 Queenâ€™s Road Central, Hong Kong. He is 82 years old. His place of birth is not recorded, and his parents are deceased. He is currently engaged in part-time work. His contact number is +852 9123 4567. He previously served as Chairman of Cheung Kong Holdings.",
    "å·²çŸ¥æå˜‰èª å±…ä½æ–¼ Hong Kong Kwun Tong 99 è™Ÿ AIA Tower 8/Fï¼Œä»–ä»Šå¹´ 31 æ­²ï¼Œå‡ºç”Ÿåœ°æœªçŸ¥ï¼Œçˆ¶æ¯é›¢ç•°ï¼Œç¾æ™‚ç„¡æ¥­ï¼Œé›»è©±è™Ÿç¢¼ç‚º +852 9167 8920ï¼Œæ›¾ç¶“æ“”ä»»é Deliveroo å¤–è³£å“¡ä¸€è·ï¼Œæˆ‘æƒ³çŸ¥åŒä½¢æœ‰é—œå˜…äººå˜…è³‡æ–™",
    "å·²çŸ¥ A å›ç¾å±…æ–¼é¦™æ¸¯è§€å¡˜é“ 99 è™Ÿ AIA Tower å…«æ¨“ï¼Œå¹´é½¡ 31 æ­²ã€‚å…¶å‡ºç”Ÿåœ°ä¸è©³ï¼Œçˆ¶æ¯å·²é›¢ç•°ï¼Œç›®å‰è™•æ–¼å¤±æ¥­ç‹€æ…‹ã€‚è¯çµ¡é›»è©±ç‚º +852 9167 8920ã€‚éå¾€æ›¾ä»»è· Deliveroo å¤–è³£å“¡ï¼Œå…·å‚™ç›¸é—œå·¥ä½œç¶“é©—ã€‚è«‹ç‚ºæˆ‘æœå°‹ç›¸é—œè³‡æ–™"
]

print("=" * 60)
for text in test_inputs:
    print(f"\nåŸå§‹è¼¸å…¥: {text}")
    
    # 1. é¡¯ç¤ºæ•¸æ“šåˆ†ä½ˆ (Data Percentage)
    composition = analyze_text_composition(text)
    print(f"ğŸ“Š æ•¸æ“šåˆ†ä½ˆ: {composition}")
    
    # 2. æ¨è«–èˆ‡è™•ç†
    raw_results = nlp(text)
    processed_entities = clean_and_process_entities(raw_results, text)
    masked_result = mask_text(text, processed_entities)
    
    print("-" * 30)
    print(f"é®è”½çµæœ: {masked_result}")
    print("åµæ¸¬å¯¦é«” (Confidence %):")
    for e in processed_entities:
        word = text[e['start']:e['end']].strip()
        # é€™è£¡å°‡åˆ†æ•¸è½‰æ›ç‚ºç™¾åˆ†æ¯”é¡¯ç¤º
        print(f" - {word} | {e['numbered_tag']} (ä¿¡å¿ƒ: {e['score']:.1%})")
    print("=" * 60)