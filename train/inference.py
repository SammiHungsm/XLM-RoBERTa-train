import json
import os
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from peft import PeftModel
import re
from collections import defaultdict
import torch # è¨˜å¾—è¦ import torch

# ==========================================
# 1. Model Setup and Loading
# ==========================================
base_model_name = "Davlan/xlm-roberta-large-ner-hrl"
lora_model_path = "./final_lora_model" 

label_list = [
    "O", "B-NAME", "I-NAME", "B-ADDRESS", "I-ADDRESS", "B-PHONE", "I-PHONE", 
    "B-ID", "I-ID", "B-ACCOUNT", "I-ACCOUNT", "B-LICENSE_PLATE", "I-LICENSE_PLATE",
    "B-ORG", "I-ORG"
]

label2id = {l: i for i, l in enumerate(label_list)}
id2label = {i: l for l, i in label2id.items()}

print("Loading model...")
try:
    tokenizer = AutoTokenizer.from_pretrained(lora_model_path)
    base_model = AutoModelForTokenClassification.from_pretrained(
        base_model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True
    )
    model = PeftModel.from_pretrained(base_model, lora_model_path)
    model.eval()
except Exception as e:
    print(f"Model loading failed: {e}")
    exit()

#nlp = pipeline("token-classification", model=model, tokenizer=tokenizer, aggregation_strategy="simple", device=0)
if torch.cuda.is_available():
    device_id = 0
    print("ğŸš€ Using Device: GPU (CUDA)")
elif torch.backends.mps.is_available():
    # å°æ–¼ Mac M1/M2 ç”¨æˆ¶
    device_id = "mps" 
    print("ğŸ Using Device: Mac GPU (MPS)")
else:
    device_id = -1
    print("ğŸ¢ Using Device: CPU")

# 2. åˆå§‹åŒ– Pipeline æ™‚å‚³å…¥ device_id
nlp = pipeline(
    "token-classification", 
    model=model, 
    tokenizer=tokenizer, 
    aggregation_strategy="simple", 
    device=device_id  # æ”¹ç”¨è®Šæ•¸
)
# ==========================================
# 2. Debugging Helper Functions
# ==========================================
def clean_and_process_entities(results, text):
    merged_entities = []
    merged_entities.sort(key=lambda x: (x['start'], -x['end'])) # æŒ‰é–‹å§‹ä½ç½®æ’åº
    
    no_overlap_entities = []
    if merged_entities:
        last_ent = merged_entities[0]
        for curr_ent in merged_entities[1:]:
            # å¦‚æœç•¶å‰å¯¦é«”å˜…é–‹å§‹ä½ç½®ï¼Œå°æ–¼ä¸Šä¸€å€‹å¯¦é«”å˜…çµæŸä½ç½® -> é‡ç–Š
            if curr_ent['start'] < last_ent['end']:
                # ç°¡å–®ç­–ç•¥ï¼šä¿ç•™é•·åº¦è¼ƒé•·çš„
                if (curr_ent['end'] - curr_ent['start']) > (last_ent['end'] - last_ent['start']):
                    last_ent = curr_ent
                # å¦å‰‡å¿½ç•¥ current
            else:
                no_overlap_entities.append(last_ent)
                last_ent = curr_ent
        no_overlap_entities.append(last_ent)
    
    merged_entities = no_overlap_entities
    # --- Phase 1: Initial Filter & Merge ---
    current_entity = None
    for res in results:
        # [Rule 1] Confidence Threshold
        if res['score'] < 0.60: 
            # DEBUG PRINT
            # print(f"âŒ [Low Score] {text[res['start']:res['end']]} ({res['score']:.2f})")
            continue
        
        entity_group = res['entity_group']
        word = text[res['start']:res['end']]
        
        if current_entity and res['start'] == current_entity['end'] and current_entity['entity_group'] == entity_group:
            current_entity['word'] += word
            current_entity['end'] = res['end']
            current_entity['score_sum'] += res['score']
            current_entity['token_count'] += 1
        else:
            if current_entity:
                current_entity['score'] = current_entity['score_sum'] / current_entity['token_count']
                merged_entities.append(current_entity)
            current_entity = {
                "entity_group": entity_group, "word": word, "start": res['start'],
                "end": res['end'], "score": res['score'], "score_sum": res['score'], "token_count": 1
            }
    if current_entity:
        current_entity['score'] = current_entity['score_sum'] / current_entity['token_count']
        merged_entities.append(current_entity)

    # ================= NEW CODE START =================
    # [æ–°å¢] è§£æ±ºé‡ç–Šå•é¡Œ (De-overlap Logic)
    # ç­–ç•¥ï¼šå¦‚æœä½ç½®é‡ç–Šï¼Œä¿ç•™ã€Œé•·åº¦è¼ƒé•·ã€çš„é‚£å€‹ (ä¾‹å¦‚ä¿ç•™ "å¾®è»Ÿ" æ£„ "å¾®")
    if merged_entities:
        # æŒ‰é–‹å§‹ä½ç½®æ’åºï¼Œå¦‚æœé–‹å§‹ä½ç½®ç›¸åŒï¼Œå‰‡æŒ‰é•·åº¦(é•·åˆ°çŸ­)æ’åº
        merged_entities.sort(key=lambda x: (x['start'], -x['end']))
        
        no_overlap_entities = []
        last_ent = merged_entities[0]
        
        for curr_ent in merged_entities[1:]:
            # æª¢æŸ¥é‡ç–Šï¼šå¦‚æœç•¶å‰å¯¦é«”çš„ Start < ä¸Šä¸€å€‹å¯¦é«”çš„ End
            if curr_ent['start'] < last_ent['end']:
                # é‡ç–Šäº†ï¼æ¯”è¼ƒé•·åº¦
                last_len = last_ent['end'] - last_ent['start']
                curr_len = curr_ent['end'] - curr_ent['start']
                
                if curr_len > last_len:
                    last_ent = curr_ent  # å–ä»£ç‚ºæ›´é•·çš„
                elif curr_len == last_len:
                    if curr_ent['score'] > last_ent['score']: # é•·åº¦ä¸€æ¨£ï¼Œæ¯”åˆ†æ•¸
                        last_ent = curr_ent
                # å¦å‰‡ä¿æŒ last_ent (å¿½ç•¥è¼ƒçŸ­çš„ current)
            else:
                # ç„¡é‡ç–Šï¼Œå°‡ä¸Šä¸€å€‹å­˜å…¥çµæœï¼Œä¸¦æ›´æ–° last ç‚ºç•¶å‰
                no_overlap_entities.append(last_ent)
                last_ent = curr_ent
        
        no_overlap_entities.append(last_ent) # åˆ¥å¿˜äº†æœ€å¾Œä¸€å€‹
        merged_entities = no_overlap_entities
    # ================= NEW CODE END =================

    # ... (æ¥ Phase 2: Advanced Cleaning Rules)
    # --- Phase 2: Advanced Cleaning Rules ---
    final_cleaned = []
    blacklist_words = ["å¥åœ¨", "ä¸è©³", "æœªçŸ¥", "ç„¡æ¥­", "é›¢ç•°", "å–®èº«", "ä¸ä¾¿", "æ•´åˆ", "è™•ç†", "éŒ¯èª¤", "é«˜åº¦", "é—Šåº¦"]
    cantonese_noise = ["é»", "ä¿‚", "æ‰“", "ä¹‹å‰", "ä¸»å¸­", "è·", "ä»²è¦"] 

    for ent in merged_entities:
        original_word = ent['word']
        # [Pre-clean]: Strip punctuation
        word = ent['word'].strip().strip("ã€Œã€ã€ã€ã€Šã€‹()ï¼ˆï¼‰ã€‚ï¼Œã€ï¼ï¼Ÿï¼šï¼›")
        ent['word'] = word
        
        label = ent['entity_group']
        start = ent['start']
        end = ent['end']
        
        # 1. Basic Filters
        if not word: 
            print(f"âŒ [Empty] {original_word}")
            continue
        if word in blacklist_words or word in cantonese_noise: 
            print(f"âŒ [Blacklist] {word}")
            continue
        if re.match(r'^[\W_]+$', word): 
            print(f"âŒ [Symbol Only] {word}")
            continue

        # 2. [Rule A: URL/Path Filter] (Fixed)
        is_url = False
        if any(x in word.lower() for x in ['http', 'www.', '.com', '.org', '.net', '.html', '.php']):
            is_url = True
        if '%' in word and re.search(r'%[0-9A-Fa-f]{2}', word):
            is_url = True
        if is_url: 
            print(f"âŒ [Rule A: URL] {word}")
            continue

        # 3. [Rule B: Measurement Filter]
        if label in ['ID', 'ACCOUNT', 'PHONE', 'LICENSE_PLATE']:
            if re.search(r'\d+(cm|kg|km|m|g|ml|L|Hz|GB|MB|KB|ft|in)$', word, re.IGNORECASE): 
                print(f"âŒ [Rule B: Unit] {word}")
                continue

        # 4. [Rule C: Account/Phone Strict Clean]
        if label in ['ACCOUNT', 'PHONE']:
            cleaned_word = re.sub(r'[^\d\+\-\(\)\s]', '', word).strip()
            if len(cleaned_word) < 3: 
                print(f"âŒ [Rule C: Short Acc/Phone] {word} -> {cleaned_word}")
                continue
            ent['word'] = cleaned_word

        # 5. [Rule D: ID/Plate Clean Chinese]
        if label in ['ID', 'LICENSE_PLATE']:
            cleaned_word = re.sub(r'[\u4e00-\u9fff]+', '', word).strip()
            if len(cleaned_word) < 2: 
                print(f"âŒ [Rule D: Short ID/Plate] {word} -> {cleaned_word}")
                continue
            ent['word'] = cleaned_word

        # 6. [Rule E: License Plate Left-Extension]
        if label == 'LICENSE_PLATE' and re.match(r'^\d+$', ent['word']):
            pre_start = start - 3 if start >=3 else 0
            prefix_text = text[pre_start:start]
            prefix_match = re.search(r'([A-Z]{1,2})\s?$', prefix_text)
            if prefix_match:
                prefix = prefix_match.group(1)
                ent['start'] = start - len(prefix_match.group(0))
                ent['word'] = prefix + ent['word']

        # 7. [Rule F: Smart Name Length & Logic]
        if label == 'NAME':
            has_chinese = bool(re.search(r'[\u4e00-\u9fff]', word))
            has_english = bool(re.search(r'[a-zA-Z]', word))
            has_separator = bool(re.search(r'[Â·ï¼]', word))

            if has_chinese:
                if has_english and len(word) > 12: 
                    print(f"âŒ [Rule F: Mixed Name Too Long] {word}")
                    continue
                if not has_english:
                    if has_separator:
                        if len(word) > 15: 
                            print(f"âŒ [Rule F: Transliterated Name Too Long] {word}")
                            continue
                    elif len(word) > 5: 
                        print(f"âŒ [Rule F: Chinese Name Too Long] {word}")
                        continue
                if len(word) == 1 and ent['score'] < 0.995: 
                    print(f"âŒ [Rule F: Single Char Name] {word}")
                    continue
            else:
                if len(word) > 25: 
                    print(f"âŒ [Rule F: English Name Too Long] {word}")
                    continue
                if len(word) < 2: 
                    print(f"âŒ [Rule F: English Name Too Short] {word}")
                    continue

        # 8. [Rule G: ID Bracket Fix]
        if label == 'ID' and end < len(text) and text[end] == ')':
             ent['end'] += 1
             ent['word'] += ')'

        # 9. [Rule H: English Name Right-Completion]
        if label == 'NAME':
            if re.search(r'[a-zA-Z]$', ent['word']):
                remaining_text = text[ent['end']:]
                suffix_match = re.match(r'^([a-z]+)', remaining_text)
                if suffix_match:
                    suffix = suffix_match.group(1)
                    ent['end'] += len(suffix)
                    ent['word'] += suffix

        # 10. [Rule I: Address Cleaning]
        if label == 'ADDRESS':
            if len(word) < 2 and re.search(r'[\u4e00-\u9fff]', word):
                print(f"âŒ [Rule I: Single Char Address] {word}")
                continue
            geo_terms = ["å¹³åŸ", "å±±è„ˆ", "ç›†åœ°", "æ²™æ¼ ", "åœ°è²Œ", "é«˜åœ°", "é«˜åŸ", "æ²³æµ", "æ°´åº«", "å°å¡¬"]
            if any(term in word for term in geo_terms):
                print(f"âŒ [Rule I: Geo Term] {word}")
                continue
# [æ–°å¢] Rule J: éæ¿¾å–®å­—ä¸­æ–‡æ©Ÿæ§‹ (é™¤éæ˜¯éå¸¸å¸¸è¦‹çš„ç°¡ç¨±å¦‚ "ä¸­é›»", "æ¸¯éµ" - é€™è£¡åªéæ¿¾å–®å­—)
        if label == 'ORG':
            # å¦‚æœæ˜¯ç´”ä¸­æ–‡ï¼Œä¸”åªæœ‰ 1 å€‹å­— (ä¾‹å¦‚ "å¾®", "åœ‹") -> è¸¢èµ°
            if len(word) == 1 and re.search(r'[\u4e00-\u9fff]', word):
                continue
            
            # éæ¿¾å¸¸è¦‹èª¤åˆ¤è©å½™
            if word in ["åœ‹éš›", "æœ‰é™å…¬å¸", "é›†åœ˜", "åˆ†è¡Œ"]:
                continue
        final_cleaned.append(ent)
# ... (åœ¨ Phase 2 loop å®Œçµå¾Œï¼ŒPhase 3 ä¹‹å‰åŠ å…¥)
# ... (Phase 2 loop å®Œçµå¾Œï¼ŒPhase 3 ä¹‹å‰) ...

    # ================= NEW CODE START =================
    # [æ–°å¢/æ›´æ–°] ä¸»å‹•æƒææ¼ç¶²çš„ã€ŒéŠ€è¡Œæˆ¶å£ã€åŒã€Œé›»è©±è™Ÿç¢¼ã€
    # åŸç†ï¼šå»ºç«‹ä¸€å€‹ set è¨˜éŒ„å·²è¢« AI æ‰åˆ°çš„ä½ç½®ï¼Œç„¶å¾Œç”¨ Regex æƒæå‰©ä¸‹çš„æ•¸å­—
    
    existing_ranges = set()
    for ent in final_cleaned:
        for i in range(ent['start'], ent['end']):
            existing_ranges.add(i)

    # å®šç¾©è£œæ¼è¦å‰‡ (Regex Patterns)
    fallback_patterns = [
        # 1. é¦™æ¸¯é›»è©± (8ä½æ•¸å­—ï¼Œ2-9é–‹é ­)
        {"name": "PHONE", "pattern": r'\b[23569]\d{7}\b'},
        
        # 2. éŠ€è¡Œæˆ¶å£ (10-12ä½æ•¸å­—)
        {"name": "ACCOUNT", "pattern": r'\b\d{10,12}\b'}
    ]

    for rule in fallback_patterns:
        for match in re.finditer(rule["pattern"], text):
            start, end = match.span()
            
            # æª¢æŸ¥é‡ç–Šï¼šå¦‚æœé€™ä¸²æ•¸å­—å·²ç¶“è¢« AI (æˆ–ä¹‹å‰çš„ Regex) æ‰éï¼Œå°±è·³é
            # ä¾‹å¦‚ AI æ‰å’— ID "R98272829"ï¼ŒRegex å°±å””å¥½ç•¶ä½¢ä¿‚é›»è©±
            if any(i in existing_ranges for i in range(start, end)):
                continue
            
            # æˆåŠŸè£œæ¼ï¼åŠ å…¥çµæœ
            final_cleaned.append({
                "entity_group": rule["name"],
                "word": match.group(),
                "start": start,
                "end": end,
                "score": 1.0,      # Regex ä¿¡å¿ƒä¿‚ 100%
                "score_sum": 1.0,
                "token_count": 1
            })
            
            # æ›´æ–° existing_rangesï¼Œé˜²æ­¢åŒä¸€å€‹æ•¸å­—è¢«æ‰å…©æ¬¡
            for i in range(start, end):
                existing_ranges.add(i)
    # ================= NEW CODE END =================

    # --- Phase 3: Numbering & Formatting ---
    # ... (æ¥ä½åŸæœ¬å˜…ä»£ç¢¼)
    # ... (æ¥ Phase 3: Numbering)
    # --- Phase 3: Numbering & Formatting ---
    final_output = []
    counters = defaultdict(int)
    entity_registry = {} 
    final_cleaned.sort(key=lambda x: x['start'])

    for entity in final_cleaned:
        label = entity['entity_group']
        dict_key = (label, entity['word'])
        if dict_key in entity_registry:
            seq_num = entity_registry[dict_key]
        else:
            counters[label] += 1
            seq_num = counters[label]
            entity_registry[dict_key] = seq_num
        entity['numbered_tag'] = f"{label}-{seq_num}"
        final_output.append(entity)
        
    return final_output

def mask_text(text, entities):
    masked_text = text
    for entity in sorted(entities, key=lambda x: x['start'], reverse=True):
        start, end = entity['start'], entity['end']
        tag = f"[{entity['numbered_tag']}]"
        masked_text = masked_text[:start] + tag + masked_text[end:]
    return masked_text

def load_test_inputs(filepath="train/test_data.json"):
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                print(f"ğŸ“‚ Loaded test data from {filepath}")
                return json.load(f)
        except Exception: pass
    return ["ä½ å¥½ï¼Œæˆ‘ä¿‚ Sammiã€‚æˆ‘çš„é›»è©±ä¿‚ 9123 4567ã€‚"]

# Run
test_inputs = load_test_inputs("train/test_data.json")

print("=" * 60)
for text in test_inputs:
    print(f"\nOriginal Input: {text}")
    
    raw_results = nlp(text)
    processed_entities = clean_and_process_entities(raw_results, text)
    masked_result = mask_text(text, processed_entities)
    
    print("-" * 30)
    print(f"Masked Result: {masked_result}")
    print("Detected Entities:")
    for e in processed_entities:
        print(f" - {e['word']} | {e['numbered_tag']} ({e['score']:.1%})")
    print("=" * 60)