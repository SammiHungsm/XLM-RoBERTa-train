import json
import re
from data_utils.tokenizer import smart_tokenize 

# ==========================================
# 1. å®šç¾©åå–® (ç¢ºä¿é€™è£¡æœ‰ä½ è¦æ‰çš„å­—)
# ==========================================
target_orgs = [
    "ä¸­åœ‹åœ‹å®¶éµè·¯é›†åœ˜æœ‰é™å…¬å¸",
    "åœ‹éµé›†åœ˜"
]

target_addrs = [
    "å¤§å¶¼å±±", "å±¯é–€", "åŒ—éƒ½", 
    "é•·è´›", "ç€‹ç™½", 
    "é¦™æ¸¯", "ä¸­åœ‹"
]

# ==========================================
# 2. åŸå§‹æ–‡æœ¬
# ==========================================
raw_content = """æ“šä¸­åœ‹åœ‹å®¶éµè·¯é›†åœ˜æœ‰é™å…¬å¸ä»Šï¼ˆ4æ—¥ï¼‰æŠ«éœ²ï¼Œéµè·¯ã€Œåå››äº”ã€å¯¦ç¾åœ“æ»¿æ”¶å®˜ã€‚ã€Œåå››äº”ã€æœŸé–“ï¼Œå…¨åœ‹éµè·¯ç‡Ÿæ¥­é‡Œç¨‹ç”±14.63è¬å…¬é‡Œå¢è‡³16.5è¬å…¬é‡Œã€å¢é•·12.8%ï¼Œé«˜éµç”±3.79è¬å…¬é‡Œå¢è‡³5.04è¬å…¬é‡Œã€å¢é•·32.98%ï¼Œä¸­åœ‹å»ºæˆä¸–ç•Œè¦æ¨¡æœ€å¤§ã€å…ˆé€²ç™¼é”çš„é«˜é€Ÿéµè·¯ç¶²ã€‚

2025å¹´ï¼Œåœ‹éµé›†åœ˜åŠ å¿«å»ºè¨­ç¾ä»£åŒ–éµè·¯åŸºç¤è¨­æ–½é«”ç³»ï¼Œåœ“æ»¿å®Œæˆéµè·¯å»ºè¨­ä»»å‹™ï¼Œå…¨åœ‹éµè·¯å®Œæˆå›ºå®šè³‡ç”¢æŠ•è³‡9015å„„å…ƒäººæ°‘å¹£ã€åŒæ¯”å¢é•·6%ï¼ŒæŠ•ç”¢æ–°ç·š3109å…¬é‡Œï¼Œå…¶ä¸­é«˜éµ2862å…¬é‡Œï¼Œéµè·¯æŠ•è³‡æ‹‰å‹•ä½œç”¨å……åˆ†é¡¯ç¾ã€‚

2025å¹´ï¼Œåœ‹éµé›†åœ˜ä»¥åœ‹å®¶ã€Œåå››äº”ã€è¦åŠƒç¶±è¦ç¢ºå®šçš„102é …é‡å¤§å·¥ç¨‹éµè·¯é …ç›®å’Œã€Œå…©é‡ã€é …ç›®ç‚ºé‡é»ï¼ŒåŠ å¤§å¯¦æ–½åŠ›åº¦ï¼Œé•·è´›é«˜éµç­‰8å€‹é …ç›®é–‹å·¥å»ºè¨­ï¼Œç€‹ç™½é«˜éµç­‰25å€‹é …ç›®é–‹é€šé‹ç‡Ÿã€‚åŠ å¿«ç‰©æµåŸºç¤è¨­æ–½å»ºè¨­ï¼Œå»ºæˆéµè·¯å°ˆç”¨ç·š52æ¢ã€‚

ã€Œåäº”äº”ã€æœŸé–“ï¼Œåœ‹éµé›†åœ˜å°‡é€²ä¸€æ­¥æ¨é€²éµè·¯ç¶²å»ºè¨­ã€‚åˆ°2030å¹´ï¼Œå…¨åœ‹éµè·¯ç‡Ÿæ¥­é‡Œç¨‹é”åˆ°18è¬å…¬é‡Œå·¦å³ï¼Œå…¶ä¸­é«˜éµ6è¬å…¬é‡Œå·¦å³ï¼Œå¾©ç·šç‡å’Œé›»æ°£åŒ–ç‡åˆ†åˆ¥é”åˆ°64%å’Œ78%ï¼Œæˆ°ç•¥éª¨å¹¹é€šé“å…¨é¢åŠ å¼·ï¼Œã€Œå…«ç¸±å…«æ©«ã€é«˜éµç³»çµ±æˆç¶²ï¼Œå€åŸŸäº’è¯äº’é€šæ°´å¹³é¡¯è‘—æå‡ï¼Œè²¨é‹ç¶²çµ¡èƒ½åŠ›å¤§å¹…å¢å¼·ï¼ŒåŸºæœ¬å»ºæˆä¸–ç•Œä¸€æµçš„ç¾ä»£åŒ–éµè·¯ç¶²ã€‚

åŸæ–‡ç¶²å€ï¼šhttps://news.mingpao.com/ins/%E5%85%A9%E5%B2%B8/article/20260104/s00004/1767518315152"""

# ==========================================
# 3. è™•ç†é‚è¼¯ (å¸¶ Debug)
# ==========================================
def process_news(text):
    print("ğŸš€ é–‹å§‹è™•ç†æ–°èæ•¸æ“š...")
    
    if "åŸæ–‡ç¶²å€ï¼š" in text: text = text.split("åŸæ–‡ç¶²å€ï¼š")[0]
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n])', text)
    segments = [s.strip() for s in sentences if s.strip()]
    
    label2id = {"O": 0, "B-NAME": 1, "I-NAME": 2, "B-ADDRESS": 3, "I-ADDRESS": 4, "B-ORG": 13, "I-ORG": 14}
    final_data = []
    
    match_count = 0

    for sent in segments:
        tokens = smart_tokenize(sent)
        tags = [0] * len(tokens) # é è¨­å…¨ç‚º O (ID=0)
        
        # --- 1. å»ºç«‹ Alignment æ˜ å°„ (Token -> Char) ---
        token_spans = []
        search_start = 0
        for token in tokens:
            start = sent.find(token, search_start)
            if start == -1:
                token_spans.append(None)
                continue
            end = start + len(token)
            token_spans.append((start, end))
            search_start = end
            
        # --- 2. æ¨™è¨»å‡½æ•¸ (å®‰å…¨æ¨¡å¼) ---
        def apply_labels(targets, label_b, label_i, type_name):
            nonlocal match_count
            for target in targets:
                for match in re.finditer(re.escape(target), sent):
                    match_start, match_end = match.span()
                    
                    # æ‰¾åˆ°äº†æ–‡å­—ï¼Œç¾åœ¨è¦æ‰¾å°æ‡‰çš„ Token
                    found_tokens = False
                    for idx, span in enumerate(token_spans):
                        if span is None: continue
                        t_start, t_end = span
                        
                        # æª¢æŸ¥ Token æ˜¯å¦åœ¨ç¯„åœå…§
                        if t_start >= match_start and t_end <= match_end:
                            # ğŸ”¥ é˜²æ­¢è¦†è“‹ï¼šåªæœ‰ç•¶å®ƒæ˜¯ 0 çš„æ™‚å€™æ‰æ¨™è¨˜
                            if tags[idx] == 0:
                                if t_start == match_start:
                                    tags[idx] = label2id[label_b]
                                    print(f"  âœ… [æ‰åˆ°] {type_name}: {target} (Token: {tokens[idx]})")
                                    match_count += 1
                                else:
                                    tags[idx] = label2id[label_i]
                                found_tokens = True
        
        # åŸ·è¡Œæ¨™è¨» (å„ªå…ˆæ¨™ ORGï¼Œå†æ¨™ Address)
        apply_labels(target_orgs, "B-ORG", "I-ORG", "ORG")
        apply_labels(target_addrs, "B-ADDRESS", "I-ADDRESS", "ADDR")
        
        if len(tokens) > 0:
            final_data.append({"tokens": tokens, "ner_tags": tags})
            
    print(f"ğŸ“Š ç¸½çµï¼šå…±æ‰åˆ° {match_count} å€‹å¯¦é«”ã€‚")
    return final_data

# ==========================================
# 4. åŸ·è¡Œèˆ‡å„²å­˜
# ==========================================
news_json_data = process_news(raw_content)

output_file = "news_data.json"
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(news_json_data, f, ensure_ascii=False, indent=2)

print(f"ğŸ“ news_data.json å·²æ›´æ–° (å…± {len(news_json_data)} æ¢)ã€‚")