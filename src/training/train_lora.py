import json
import numpy as np
import torch
import os
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorForTokenClassification,
    EarlyStoppingCallback
)
from peft import get_peft_model, LoraConfig, TaskType
import evaluate

# ==========================================
# 1. è¼‰å…¥æ•¸æ“š
# ==========================================
print("ğŸ“‚ æ­£åœ¨è¼‰å…¥æ•¸æ“š...")
try:
    with open("train_data_lora.json", "r", encoding="utf-8") as f:
        raw = json.load(f)
        data = raw["data"]
        label2id = raw["label2id"]
        id2label = {int(k): v for k, v in raw["id2label"].items()}
    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(data)} æ¢è¨“ç·´æ•¸æ“š")
except FileNotFoundError:
    print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° train_data_lora.jsonã€‚è«‹å…ˆåŸ·è¡Œ prepare_data.pyï¼")
    exit()

dataset = Dataset.from_list(data)
# åˆ‡åˆ† 10% ä½œç‚ºé©—è­‰é›†
dataset = dataset.train_test_split(test_size=0.1)

# ==========================================
# 2. æ¨¡å‹èˆ‡åˆ†è©å™¨ (XLM-R Large)
# ==========================================
model_name = "Davlan/xlm-roberta-large-ner-hrl" 
print(f"ğŸ¤– æ­£åœ¨è¼‰å…¥æ¨¡å‹åŠåˆ†è©å™¨: {model_name}")
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ==========================================
# 3. Tokenization & Alignment (å„ªåŒ– Max Length)
# ==========================================
def tokenize_and_align_labels(examples):
    # å¢åŠ è‡³ 384 ä»¥é˜²æ–°èé•·å¥è¢«æˆªæ–·
    tokenized_inputs = tokenizer(
        examples["tokens"], 
        is_split_into_words=True, 
        truncation=True, 
        padding="max_length", 
        max_length=384 
    )

    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100) 
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx]) 
            else:
                label_ids.append(-100) 
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

print("âš™ï¸ æ­£åœ¨è™•ç† Tokenization åŠ Label Alignment (Max Length: 384)...")
tokenized_datasets = dataset.map(
    tokenize_and_align_labels, 
    batched=True,
    remove_columns=dataset["train"].column_names
)

# ==========================================
# 4. è¼‰å…¥æ¨¡å‹ä¸¦é…ç½®é€²éš LoRA
# ==========================================
model = AutoModelForTokenClassification.from_pretrained(
    model_name, 
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True 
)

# é‡å° Large æ¨¡å‹å„ªåŒ– target_modulesï¼Œè¦†è“‹æ‰€æœ‰ Dense å±¤ä»¥æå‡æ•ˆæœ
peft_config = LoraConfig(
    task_type=TaskType.TOKEN_CLS, 
    inference_mode=False, 
    r=16,           
    lora_alpha=32,  
    lora_dropout=0.1,
    bias="none",
    target_modules=["query", "key", "value", "output.dense", "intermediate.dense"]
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ==========================================
# 5. è¨“ç·´åƒæ•¸ (VRAM å„ªåŒ–çµ„åˆ)
# ==========================================
use_fp16 = torch.cuda.is_available()
print(f"âš¡ GPU åŠ é€Ÿæ¨¡å¼: {'FP16 (CUDA)' if use_fp16 else 'FP32'}")

args = TrainingArguments(
    output_dir="./lora_xlm_roberta_ner",
    eval_strategy="steps",        # æ”¹ç‚ºæŒ‰æ­¥æ•¸è©•ä¼°ï¼Œé…åˆ Early Stopping æ›´éˆæ´»
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    learning_rate=2e-4,
    per_device_train_batch_size=4,   # é™ä½ Batch Size ä»¥é˜² OOM
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=2,  # ç´¯ç©æ¢¯åº¦ï¼Œç¶­æŒ Effective Batch Size = 8
    num_train_epochs=5,
    weight_decay=0.01,
    logging_steps=20,
    save_total_limit=2,           
    load_best_model_at_end=True,  
    metric_for_best_model="f1",
    fp16=use_fp16,
    # ğŸ”¥ VRAM æ ¸å¿ƒå„ªåŒ–ï¼šé–‹å•Ÿæ¢¯åº¦æª¢æŸ¥é»
    gradient_checkpointing=True,
    # Windows ç³»çµ±å»ºè­°
    dataloader_num_workers=0,
    report_to="none" 
)

# ==========================================
# 6. Metrics è©•ä¼°èˆ‡ Collator
# ==========================================
data_collator = DataCollatorForTokenClassification(
    tokenizer, 
    pad_to_multiple_of=8 if use_fp16 else None
)

metric = evaluate.load("seqeval")

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

# ==========================================
# 7. é–‹å§‹è¨“ç·´ (åŠ å…¥ Early Stopping)
# ==========================================
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # 3æ¬¡è©•ä¼°å†‡é€²æ­¥å°±åœ
)

# ğŸ” è¨“ç·´å‰æœ€å¾Œæª¢æŸ¥
sample = dataset['train'][0]
if len(sample['tokens']) != len(sample['ner_tags']):
    print("âŒ è‡´å‘½éŒ¯èª¤ï¼šTokens èˆ‡ Tags é•·åº¦ä¸ä¸€ï¼")
    exit()

print("ğŸš€ å•Ÿå‹•å¾®èª¿è¨“ç·´...")
trainer.train()

# ==========================================
# 8. å„²å­˜
# ==========================================
final_output = "./final_lora_model"
model.save_pretrained(final_output)
tokenizer.save_pretrained(final_output)
print(f"âœ… è¨“ç·´å®Œæˆï¼æ¨¡å‹å·²å­˜è‡³ {final_output}")