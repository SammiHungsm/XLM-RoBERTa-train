import json
import numpy as np
import torch
import os
import sys
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorForTokenClassification,
    EarlyStoppingCallback,
    TrainerCallback
)
from peft import get_peft_model, LoraConfig, TaskType
import evaluate
# ğŸ”¥ [æ–°å¢] å°å…¥è©³ç´°å ±å‘Šå·¥å…·
from seqeval.metrics import classification_report

# ===========================
# ğŸ”¥ 1. è·¯å¾‘ä¿®å¾© (Critical Path Fix)
# ===========================
# ç¢ºä¿ç„¡è«–åœ¨å“ªè£¡åŸ·è¡Œè…³æœ¬ï¼Œéƒ½èƒ½æ‰¾åˆ° 'src' æ¨¡çµ„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.append(project_root)

from src.config import BASE_MODEL_NAME, LORA_MODEL_PATH, LABEL2ID, ID2LABEL

# ===========================
# ğŸ”¥ 2. è‡ªå®šç¾©æ—¥èªŒ (Log Callback)
# ===========================
class LogCallback(TrainerCallback):
    def __init__(self, log_path="training_history.json"):
        self.log_path = log_path
        self.history = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            log_entry = {
                "step": state.global_step,
                "epoch": round(state.epoch, 2) if state.epoch else 0,
                **logs
            }
            self.history.append(log_entry)
            with open(self.log_path, "w", encoding="utf-8") as f:
                json.dump(self.history, f, ensure_ascii=False, indent=2)

def train():
    # 3. è¼‰å…¥æ•¸æ“š
    print("ğŸ“‚ è¼‰å…¥è¨“ç·´æ•¸æ“š...")
    input_file = "train_data_lora_cleaned.json"
    
    if not os.path.exists(input_file):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° {input_file}ã€‚è«‹å…ˆåŸ·è¡Œ clean_and_augment.pyï¼")
        return

    with open(input_file, "r", encoding="utf-8") as f:
        raw = json.load(f)
        data = raw["data"] if "data" in raw else raw # å…¼å®¹ä¸åŒæ ¼å¼
    
    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(data)} æ¢æ¸…æ´—å¾Œçš„æ•¸æ“š")
    
    # è½‰æ›ç‚º HuggingFace Dataset
    dataset = Dataset.from_list(data).train_test_split(test_size=0.1)

    # 4. è¼‰å…¥ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)

    # 5. æ•¸æ“šé è™•ç† (Tokenization & Alignment)
    def tokenize_and_align_labels(examples):
        tokenized_inputs = tokenizer(
            examples["tokens"], 
            is_split_into_words=True, 
            truncation=True, 
            max_length=384, 
            padding="max_length"
        )
        labels = []
        for i, label in enumerate(examples["ner_tags"]):
            word_ids = tokenized_inputs.word_ids(batch_index=i)
            previous_word_idx = None
            label_ids = []
            
            for word_idx in word_ids:
                if word_idx is None:
                    label_ids.append(-100) # å¿½ç•¥ç‰¹æ®Š token
                elif word_idx != previous_word_idx:
                    label_ids.append(label[word_idx]) # æ¯å€‹å­—çš„ç¬¬ä¸€å€‹ token
                else:
                    label_ids.append(label[word_idx]) # åŒä¸€å€‹å­—çš„å¾ŒçºŒ token (Subword)
                
                previous_word_idx = word_idx
            labels.append(label_ids)

        tokenized_inputs["labels"] = labels
        return tokenized_inputs

    print("âš™ï¸ æ­£åœ¨åŸ·è¡Œå…¨æ¨™ç±¤å°é½Šè™•ç†...")
    tokenized_datasets = dataset.map(
        tokenize_and_align_labels, 
        batched=True,
        remove_columns=dataset["train"].column_names
    )

    # 6. è¼‰å…¥æ¨¡å‹ä¸¦é…ç½® LoRA
    model = AutoModelForTokenClassification.from_pretrained(
        BASE_MODEL_NAME, 
        num_labels=len(LABEL2ID),
        id2label=ID2LABEL,
        label2id=LABEL2ID,
        ignore_mismatched_sizes=True 
    )

    peft_config = LoraConfig(
        task_type=TaskType.TOKEN_CLS, 
        r=8,              # ç§© (Rank): æ§åˆ¶åƒæ•¸é‡
        lora_alpha=16,    # Alpha: ç¸®æ”¾å› å­
        lora_dropout=0.1,
        target_modules=["query", "key", "value", "output.dense", "intermediate.dense"]
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # 7. è¨­å®šè©•ä¼°æŒ‡æ¨™ (Metrics)
    metric = evaluate.load("seqeval")
    
    def compute_metrics(p):
        predictions, labels = p
        predictions = np.argmax(predictions, axis=2)

        # ç§»é™¤ -100 çš„æ¨™ç±¤ï¼Œåªè¨ˆç®—çœŸå¯¦ Token
        true_predictions = [
            [ID2LABEL[p] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        true_labels = [
            [ID2LABEL[l] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]

        results = metric.compute(predictions=true_predictions, references=true_labels)
        
        # ğŸ”¥ [é—œéµæ–°å¢] ç”Ÿæˆä¸¦æ‰“å°è©³ç´°åˆ†é¡å ±å‘Š
        # é€™èƒ½è®“ä½ åœ¨ Console ä¸­ç›´æ¥çœ‹åˆ°æ¯å€‹é¡åˆ¥ (ADDRESS, NAME...) çš„åˆ†æ•¸
        try:
            report = classification_report(true_labels, true_predictions)
            print("\n" + "="*40)
            print("ğŸ“Š è©³ç´°åˆ†é¡æ•ˆèƒ½å ±å‘Š (Per-Entity Report):")
            print(report)
            print("="*40 + "\n")
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•ç”Ÿæˆè©³ç´°å ±å‘Š: {e}")

        return {
            "f1": results["overall_f1"],
            "precision": results["overall_precision"],
            "recall": results["overall_recall"]
        }

    # 8. è¨“ç·´åƒæ•¸ (Training Arguments)
    args = TrainingArguments(
        output_dir="./lora_out",
        eval_strategy="steps",
        
        # å„ªåŒ–è¨­ç½®ï¼šæ¸›å°‘è©•ä¼°é »ç‡ä»¥åŠ å¿«è¨“ç·´
        eval_steps=500,        
        save_strategy="steps",
        save_steps=500,        
        
        save_total_limit=2,    
        
        learning_rate=2e-5,
        num_train_epochs=5,
        lr_scheduler_type="cosine",
        warmup_ratio=0.1,
        weight_decay=0.05,
        label_smoothing_factor=0.1,
        
        per_device_train_batch_size=4,
        gradient_accumulation_steps=2,
        logging_steps=10,
        logging_dir='./logs',
        fp16=torch.cuda.is_available(),
        gradient_checkpointing=True,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        report_to="tensorboard"
    )

    # 9. å•Ÿå‹• Trainer
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        tokenizer=tokenizer,
        data_collator=DataCollatorForTokenClassification(tokenizer),
        compute_metrics=compute_metrics,
        callbacks=[
            # å„ªåŒ–è¨­ç½®ï¼šçµ¦äºˆæ›´å¤šè€å¿ƒ (Patience 10)
            EarlyStoppingCallback(early_stopping_patience=10), 
            LogCallback(log_path="training_history.json")
        ]
    )

    print("ğŸš€ å•Ÿå‹•å¼·åŒ–ç‰ˆæ¨™ç±¤å°é½ŠåŠå•†ç”¨ç²¾èª¿è¨“ç·´...")
    trainer.train()

    # 10. å„²å­˜æœ€çµ‚æ¨¡å‹
    print(f"ğŸ’¾ æ­£åœ¨å„²å­˜æ¨¡å‹è‡³ {LORA_MODEL_PATH}...")
    model.save_pretrained(LORA_MODEL_PATH)
    tokenizer.save_pretrained(LORA_MODEL_PATH)
    print(f"âœ… è¨“ç·´å®Œæˆï¼")

if __name__ == "__main__":
    train()