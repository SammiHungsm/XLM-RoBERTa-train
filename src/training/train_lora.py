import json
import numpy as np
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorForTokenClassification
)
from peft import get_peft_model, LoraConfig, TaskType
import evaluate

# ==========================================
# 1. è¼‰å…¥æ•¸æ“š
# ==========================================
print("ğŸ“‚ æ­£åœ¨è¼‰å…¥æ•¸æ“š...")
try:
    with open("train_data_lora.json", "r", encoding="utf-8") as f:
        raw = json.load(f)
        data = raw["data"]
        label2id = raw["label2id"]
        # ç¢ºä¿ id2label çš„ key æ˜¯æ•´æ•¸
        id2label = {int(k): v for k, v in raw["id2label"].items()}
    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(data)} æ¢è¨“ç·´æ•¸æ“š")
except FileNotFoundError:
    print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° train_data_lora.jsonã€‚è«‹å…ˆåŸ·è¡Œ prepare_data.pyï¼")
    exit()

dataset = Dataset.from_list(data)
# åˆ‡åˆ† 10% ä½œç‚ºé©—è­‰é›† (Test/Validation Set)
dataset = dataset.train_test_split(test_size=0.1)

# ==========================================
# 1.5 æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥ (Sanity Check)
# ==========================================
# è®“æˆ‘å€‘çœ‹çœ‹ smart_tokenize çš„æ•ˆæœï¼
print("\nğŸ” æ•¸æ“šæ¨£æœ¬æª¢æŸ¥ (Example 0):")
print(f"Tokens: {dataset['train'][0]['tokens']}")
print(f"Tags:   {dataset['train'][0]['ner_tags']}")
print("(è«‹ç¢ºèªä¸Šæ–¹çš„ Tokens åŒ…å«å®Œæ•´çš„è‹±æ–‡å–®è©ï¼Œä¾‹å¦‚ 'Block' è€Œä¸æ˜¯ 'B','l'...) \n")

# ==========================================
# 2. æ¨¡å‹èˆ‡åˆ†è©å™¨
# ==========================================
model_name = "Davlan/xlm-roberta-large-ner-hrl" 
print(f"ğŸ¤– æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ==========================================
# 3. Tokenization & Alignment
# ==========================================
def tokenize_and_align_labels(examples):
    # é€™è£¡çš„ is_split_into_words=True éå¸¸é‡è¦
    # å› ç‚ºæˆ‘å€‘çš„è¼¸å…¥å·²ç¶“æ˜¯åˆ‡åˆ†å¥½çš„ List (smart_tokenize çš„çµæœ)
    tokenized_inputs = tokenizer(
        examples["tokens"], 
        is_split_into_words=True, 
        truncation=True, 
        padding="max_length", 
        max_length=256 
    )

    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                # ç‰¹æ®Š token (<s>, </s>) è¨­ç‚º -100 (ä¸è¨ˆç®— Loss)
                label_ids.append(-100) 
            elif word_idx != previous_word_idx:
                # é€™æ˜¯å–®è©çš„ç¬¬ä¸€å€‹ Subtoken -> è³¦äºˆçœŸå¯¦ Label
                # å› ç‚ºæˆ‘å€‘ç¾åœ¨ç”¨ smart_tokenizeï¼Œé€™è£¡èƒ½ç¢ºä¿ "Complex" é€™å€‹è©
                # åªæœ‰å®ƒçš„ç¬¬ä¸€å€‹ subtoken ç²å¾— B-TAGï¼Œé€™å°æ¨¡å‹å­¸ç¿’å¾ˆæœ‰å¹«åŠ©
                label_ids.append(label[word_idx]) 
            else:
                # åŒä¸€å€‹å–®è©çš„å¾ŒçºŒ Subtokens -> è¨­ç‚º -100
                # ä¾‹å¦‚ "Structure" è¢«åˆ‡æˆ "Struc" + "ture"
                # "ture" æœƒè¢«æ¨™è¨˜ç‚º -100ï¼Œé¿å…æ¨¡å‹éåº¦é—œæ³¨å¾Œç¶´
                label_ids.append(-100) 
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

print("âš™ï¸ æ­£åœ¨è™•ç† Tokenization åŠ Label Alignment...")
tokenized_datasets = dataset.map(
    tokenize_and_align_labels, 
    batched=True,
    remove_columns=dataset["train"].column_names # ç§»é™¤åŸå§‹æ–‡å­—æ¬„ä½
)

# ==========================================
# 4. è¼‰å…¥æ¨¡å‹ä¸¦é…ç½® LoRA
# ==========================================
model = AutoModelForTokenClassification.from_pretrained(
    model_name, 
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True 
)

# é‡å° NER ä»»å‹™çš„ LoRA é…ç½®
peft_config = LoraConfig(
    task_type=TaskType.TOKEN_CLS, 
    inference_mode=False, 
    r=16,           
    lora_alpha=32,  
    lora_dropout=0.1,
    bias="none",
    target_modules=["query", "key", "value", "output.dense", "intermediate.dense"]
)

model = get_peft_model(model, peft_config)
print("--- LoRA åƒæ•¸åˆ†ä½ˆ ---")
model.print_trainable_parameters()

# ==========================================
# 5. è¨“ç·´åƒæ•¸
# ==========================================
# è‡ªå‹•æª¢æ¸¬æ˜¯å¦å¯ä»¥ä½¿ç”¨ fp16 (CUDA)
use_fp16 = torch.cuda.is_available()
print(f"âš¡ GPU åŠ é€Ÿæ¨¡å¼: {'FP16 (CUDA)' if use_fp16 else 'FP32 (CPU/MPS)'}")

args = TrainingArguments(
    output_dir="./lora_xlm_roberta_ner",
    eval_strategy="epoch",        
    save_strategy="epoch",        
    learning_rate=2e-4,
    per_device_train_batch_size=8, # 8G VRAM å»ºè­° 8; 4G VRAM æ”¹ 4
    gradient_accumulation_steps=1, 
    num_train_epochs=5,
    weight_decay=0.01,
    logging_steps=50,
    save_total_limit=2,           
    remove_unused_columns=False,
    load_best_model_at_end=True,  
    metric_for_best_model="f1",   
    
    # è¨­å‚™ç›¸é—œè¨­ç½®
    fp16=use_fp16,                # åªæœ‰ NVIDIA GPU æ‰é–‹ FP16
    dataloader_num_workers=0      # Windows å¿…é ˆè¨­ç‚º 0
)

# åŠ å…¥ pad_to_multiple_of=8 å¯ä»¥è®“ Tensor Core é‹ç®—æ›´æœ‰æ•ˆç‡
data_collator = DataCollatorForTokenClassification(
    tokenizer, 
    pad_to_multiple_of=8 if use_fp16 else None
)

# ==========================================
# 6. Metrics è©•ä¼°å‡½æ•¸
# ==========================================
print("ğŸ“Š è¼‰å…¥è©•ä¼°æŒ‡æ¨™...")
metric = evaluate.load("seqeval")

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # å°‡é æ¸¬çµæœé‚„åŸç‚ºæ¨™ç±¤åç¨± (éæ¿¾æ‰ -100)
    true_predictions = [
        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

# ==========================================
# 7. é–‹å§‹è¨“ç·´
# ==========================================
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

print("ğŸš€ é–‹å§‹è¨“ç·´...")
trainer.train()

# ==========================================
# 8. å„²å­˜æ¨¡å‹
# ==========================================
print("ğŸ’¾ æ­£åœ¨å„²å­˜æœ€ä½³æ¨¡å‹...")
model.save_pretrained("./final_lora_model")
tokenizer.save_pretrained("./final_lora_model")
print("âœ… è¨“ç·´å®Œæˆï¼æœ€ä½³æ¨¡å‹å·²å­˜è‡³ ./final_lora_model")