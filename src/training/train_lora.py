import json
import numpy as np
import torch
import os
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorForTokenClassification,
    EarlyStoppingCallback,
    TrainerCallback
)
from peft import get_peft_model, LoraConfig, TaskType
import evaluate
from src.config import BASE_MODEL_NAME, LORA_MODEL_PATH, LABEL2ID, ID2LABEL

# ğŸ”¥ è‡ªå®šç¾©æ—¥èªŒè¨˜éŒ„å™¨
class LogCallback(TrainerCallback):
    def __init__(self, log_path="training_history.json"):
        self.log_path = log_path
        self.history = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            log_entry = {
                "step": state.global_step,
                "epoch": round(state.epoch, 2) if state.epoch else 0,
                **logs
            }
            self.history.append(log_entry)
            with open(self.log_path, "w", encoding="utf-8") as f:
                json.dump(self.history, f, ensure_ascii=False, indent=2)

def train():
    # 1. è¼‰å…¥æ•¸æ“š
    print("ğŸ“‚ è¼‰å…¥è¨“ç·´æ•¸æ“š...")
    with open("train_data_lora.json", "r", encoding="utf-8") as f:
        raw = json.load(f)
        data = raw["data"]
    
    dataset = Dataset.from_list(data).train_test_split(test_size=0.1)

    # 2. è¼‰å…¥ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)

    # 3. âš™ï¸ è™•ç†æ•¸æ“š (Point 3ï¼šå¼·åŒ–æ¨™ç±¤å°é½Šé‚è¼¯)
    def tokenize_and_align_labels(examples):
        tokenized_inputs = tokenizer(
            examples["tokens"], 
            is_split_into_words=True, 
            truncation=True, 
            max_length=384, 
            padding="max_length"
        )
        labels = []
        for i, label in enumerate(examples["ner_tags"]):
            word_ids = tokenized_inputs.word_ids(batch_index=i)
            previous_word_idx = None
            label_ids = []
            
            for word_idx in word_ids:
                if word_idx is None:
                    label_ids.append(-100)
                elif word_idx != previous_word_idx:
                    label_ids.append(label[word_idx])
                else:
                    # ğŸ”¥ Point 3 æ ¸å¿ƒï¼šæ¨™ç±¤å‚³æ’­ï¼Œç¢ºä¿æ¨¡å‹å­¸æœƒé•·å¯¦é«”çš„é€£çºŒæ€§
                    label_ids.append(label[word_idx])
                
                previous_word_idx = word_idx
            labels.append(label_ids)

        tokenized_inputs["labels"] = labels
        return tokenized_inputs

    

    print("âš™ï¸ æ­£åœ¨åŸ·è¡Œå…¨æ¨™ç±¤å°é½Šè™•ç† (Label Propagation)...")
    tokenized_datasets = dataset.map(
        tokenize_and_align_labels, 
        batched=True,
        remove_columns=dataset["train"].column_names
    )

    # 4. è¼‰å…¥æ¨¡å‹ä¸¦é…ç½® LoRA
    model = AutoModelForTokenClassification.from_pretrained(
        BASE_MODEL_NAME, 
        num_labels=len(LABEL2ID),
        id2label=ID2LABEL,
        label2id=LABEL2ID,
        ignore_mismatched_sizes=True 
    )

    peft_config = LoraConfig(
        task_type=TaskType.TOKEN_CLS, 
        r=16, lora_alpha=32, lora_dropout=0.1,
        target_modules=["query", "key", "value", "output.dense", "intermediate.dense"]
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # 5. è¨­å®šè©•ä¼°æŒ‡æ¨™
    metric = evaluate.load("seqeval")
    def compute_metrics(p):
        predictions, labels = p
        predictions = np.argmax(predictions, axis=2)
        true_predictions = [[ID2LABEL[p] for (p, l) in zip(pr, la) if l != -100] for pr, la in zip(predictions, labels)]
        true_labels = [[ID2LABEL[l] for (p, l) in zip(pr, la) if l != -100] for pr, la in zip(predictions, labels)]
        results = metric.compute(predictions=true_predictions, references=true_labels)
        return {
            "f1": results["overall_f1"],
            "precision": results["overall_precision"],
            "recall": results["overall_recall"]
        }

    # 6. è¨“ç·´åƒæ•¸ (Point 5 å¼·åŒ–å•†ç”¨ç²¾èª¿é…ç½®)
    args = TrainingArguments(
        output_dir="./lora_out",
        eval_strategy="steps",
        eval_steps=100,
        save_strategy="steps",
        save_steps=100,
        
        # ğŸ”¥ Point 5 å„ªåŒ–çµ„åˆ
        learning_rate=2e-5,           # ä½å­¸ç¿’ç‡ç¢ºä¿ç²¾ç´°å¾®èª¿
        num_train_epochs=5,           # é…åˆä½å­¸ç¿’ç‡å¢åŠ  Epoch ç¢ºä¿æ”¶æ–‚
        lr_scheduler_type="cosine",    # ä½¿ç”¨é¤˜å¼¦é€€ç«ï¼Œæ”¶å°¾å­¸å¾—æ›´éš
        warmup_ratio=0.1,             # åŠ å…¥ç†±èº«é˜²æ­¢åˆæœŸæ¢¯åº¦ä¸ç©©
        weight_decay=0.01,            # å¢åŠ æ¬Šé‡è¡°æ¸›é˜²æ­¢éæ“¬åˆ
        
        per_device_train_batch_size=4,
        gradient_accumulation_steps=2, # Effective Batch Size = 8
        logging_steps=10,
        logging_dir='./logs',
        fp16=torch.cuda.is_available(),
        gradient_checkpointing=True,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        report_to="tensorboard"
    )

    # 7. å•Ÿå‹• Trainer
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        tokenizer=tokenizer,
        data_collator=DataCollatorForTokenClassification(tokenizer),
        compute_metrics=compute_metrics,
        callbacks=[
            EarlyStoppingCallback(early_stopping_patience=3),
            LogCallback(log_path="training_history.json")
        ]
    )

    print("ğŸš€ å•Ÿå‹•å¼·åŒ–ç‰ˆæ¨™ç±¤å°é½ŠåŠå•†ç”¨ç²¾èª¿è¨“ç·´...")
    trainer.train()

    # 8. å„²å­˜
    model.save_pretrained(LORA_MODEL_PATH)
    tokenizer.save_pretrained(LORA_MODEL_PATH)
    print(f"âœ… è¨“ç·´å®Œæˆï¼æ¨¡å‹å·²å­˜è‡³ {LORA_MODEL_PATH}")

if __name__ == "__main__":
    train()