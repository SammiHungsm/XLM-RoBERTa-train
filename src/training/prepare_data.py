# src/training/prepare_data.py
import json
import os
import random
import sys

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

from src.config import LABEL2ID, ID2LABEL

# O çš„ ID (é€šå¸¸æ˜¯ 0ï¼Œä½†å¾ config æ‹¿æœ€ç©©é™£)
O_ID = LABEL2ID.get("O", 0)

# 1. éœæ…‹ç¦æ­¢åå–® (é€™äº›è©å¦‚æœåœ¨åˆæˆæ•¸æ“šä¸­æ¨™ç‚º Oï¼Œ99% æ˜¯éŒ¯çš„ï¼Œå¿…é ˆåˆªé™¤)
STRICT_FORBIDDEN = {
    "ä¸­åœ‹", "åœ‹éµ", "æ¸¯éµ", "MTR", "éµè·¯", "é›†åœ˜", "æœ‰é™å…¬å¸", "åå››äº”", "åäº”äº”", "å»ºè¨­", "ç™¼å±•", "é«˜éµ",
    "éŠ€è¡Œ", "HSBC", "åŒ¯è±", "æ¸£æ‰“", "ä¸­éŠ€", "æ’ç”Ÿ", "æ”¯ä»˜å¯¶", "Alipay", "PayMe", "FPS", "è½‰æ•¸å¿«",
    "é †è±", "SF Express", "DHL", "æ·˜å¯¶", "Foodpanda", "Deliveroo",
    "é¦™æ¸¯", "ä¹é¾", "æ–°ç•Œ", "ä¸­å¿ƒ", "å¤§å»ˆ", "å»£å ´", "è¡—é“", "Road", "Street", "Building", "Tower",
    "http", "https", ".com", ".org", ".net", "www", "åŸæ–‡ç¶²å€"
}

def load_json(path):
    if not os.path.exists(path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {path}ï¼Œå°‡è·³éã€‚")
        return []
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
        # å…¼å®¹æ€§è™•ç†ï¼šå¦‚æœ json çµæ§‹æ˜¯ {"data": [...]}
        if isinstance(data, dict) and "data" in data:
            return data["data"]
        return data

def extract_gold_entities(data_list):
    """å‹•æ…‹æå–ï¼šå¾æ–°èåŒ MTR æ•¸æ“šä¸­æå–å¯¦é«”ï¼Œä½†éæ¿¾æ‰å¤ªçŸ­çš„è©"""
    gold_words = set()
    for item in data_list:
        tokens = item.get("tokens", [])
        tags = item.get("ner_tags", [])
        
        for i, tag in enumerate(tags):
            # å¦‚æœæ˜¯å¯¦é«” (ä¸æ˜¯ O)
            if tag != O_ID: 
                word = tokens[i].lower()
                # å•†æ¥­é‚è¼¯ï¼šåªæå–é•·åº¦ >= 2 çš„è©é€²å…¥é»‘åå–®
                # å› ç‚ºå–®å­— (å¦‚ "ä¸­", "ç¾", "è¡Œ") åœ¨æ™®é€šèªå¢ƒå¤ªå¸¸è¦‹ï¼Œä¸æ‡‰ç¦æ­¢å…¶ä½œç‚º O å‡ºç¾
                if len(word) >= 2:
                    gold_words.add(word)
    return gold_words

def is_clean(item, forbidden_set):
    """
    ä¿®æ­£ç‰ˆéæ¿¾é‚è¼¯ï¼š
    1. ä½¿ç”¨ Set é€²è¡Œ O(1) å¿«é€ŸæŸ¥æ‰¾
    2. ä½¿ç”¨ token_low in forbidden_set (ç²¾ç¢ºåŒ¹é…)ï¼Œè€Œé substring
    """
    tokens = item.get("tokens", [])
    tags = item.get("ner_tags", [])
    
    # å¦‚æœé•·åº¦ä¸ä¸€è‡´ï¼Œç›´æ¥ä¸Ÿæ£„ (å£æ•¸æ“š)
    if len(tokens) != len(tags):
        return False

    for i, t in enumerate(tokens):
        # é‚è¼¯ï¼šå¦‚æœé€™å€‹ Token è¢«æ¨™è¨˜ç‚º O (å³éå¯¦é«”)
        # ä½†å®ƒå‡ºç¾åœ¨æˆ‘å€‘çš„ã€Œé«˜å±åå–®ã€ä¸­ -> ä»£è¡¨åˆæˆæ•¸æ“šå¯èƒ½æ¨™æ¼äº† (False Negative)
        # æ‰€ä»¥æˆ‘å€‘è¦ä¸Ÿæ£„é€™æ¢æ•¸æ“šï¼Œä»¥å…èª¤å°æ¨¡å‹
        if tags[i] == O_ID:
            token_low = t.lower()
            if token_low in forbidden_set:
                # é€™è£¡åŸæœ¬æ˜¯ return False
                # ä½†ç‚ºäº†ä¿ç•™ã€Œè² é¢æ¨£æœ¬ã€(ä¾‹å¦‚ï¼šæˆ‘ä½åœ¨[ä¸­ç’°]) vs (ä¸­ç’°ä¿‚ä¸€å€‹åœ°æ–¹)ï¼Œæˆ‘å€‘åªå° STRICT åå–®åš´æ ¼
                # å°æ–¼å‹•æ…‹åå–®ï¼Œæˆ‘å€‘æ”¾å¯¬ä¸€é»ï¼Œæˆ–è€…ä½ å¯ä»¥é¸æ“‡ return False (è¦–ä¹ä½ å°åˆæˆæ•¸æ“šè³ªé‡çš„ä¿¡å¿ƒ)
                return False 
                
    return True

if __name__ == "__main__":
    # 1. è®€å–æ•¸æ“š
    print("ğŸ“‚ è®€å–åŸå§‹æ•¸æ“š...")
    news = load_json("./data/raw/news_data.json")
    novel = load_json("./data/raw/novel_data.json")
    mtr = load_json("./data/raw/mtr_news_data.json")
    # é€™è£¡å‡è¨­ä½ æœ‰ç”Ÿæˆå¥½çš„åˆæˆæ•¸æ“š (å¦‚ç„¡ï¼Œå‰‡ç‚ºç©º list)
    synthetic_raw = load_json("./data/raw/synthetic_data.json")

    # 2. âš¡ åŸ·è¡Œå‰ç½®å‹•æ…‹æå–
    print("ğŸ” æ­£åœ¨å¾é‡‘æ¨™æ•¸æ“šä¸­æå–å‹•æ…‹ç¦æ­¢åå–®...")
    dynamic_forbidden = extract_gold_entities(news + mtr)
    
    # åˆä½µéœæ…‹èˆ‡å‹•æ…‹åå–®ï¼Œä¸¦è½‰ç‚ºå°å¯« set ä»¥åŠ é€Ÿ
    full_forbidden_set = set(w.lower() for w in STRICT_FORBIDDEN) | dynamic_forbidden
    print(f"âœ… ç¦æ­¢åå–®æ§‹å»ºå®Œæˆ (éœæ…‹: {len(STRICT_FORBIDDEN)} + å‹•æ…‹: {len(dynamic_forbidden)})")

    # 3. ğŸ›¡ï¸ éæ¿¾åˆæˆæ•¸æ“š
    if synthetic_raw:
        print(f"ğŸ›¡ï¸ æ­£åœ¨åŸ·è¡Œåˆæˆæ•¸æ“šæœ€çµ‚æ¸…æ´— (åŸå§‹: {len(synthetic_raw)})...")
        synthetic_cleaned = [d for d in synthetic_raw if is_clean(d, full_forbidden_set)]
        
        removed_count = len(synthetic_raw) - len(synthetic_cleaned)
        if removed_count > 0:
            print(f"ğŸš« å·²å‰”é™¤ {removed_count} æ¢æ½›åœ¨æ¨™ç±¤è¡çªçš„åˆæˆæ¨£æœ¬ã€‚")
    else:
        synthetic_cleaned = []
        print("âš ï¸ ç„¡åˆæˆæ•¸æ“šï¼Œè·³ééæ¿¾æ­¥é©Ÿã€‚")

    # 4. æŒ‰æ¬Šé‡åˆä½µ (æ•¸æ“šå¢å¼·ç­–ç•¥)
    all_training_data = []
    
    # åˆæˆæ•¸æ“š (x1) - ç”¨ä½œåŸºç¤æ³›åŒ–
    all_training_data.extend(synthetic_cleaned)
    
    # æ–°èæ•¸æ“š (x10) - æ¥µå…¶é‡è¦ï¼ŒåŒ…å«é«˜é » PIIï¼ŒåŠ é‡æ¬Šé‡
    if news:
        print(f"ğŸ“ˆ æ–°èæ•¸æ“šå€å¢ x10")
        all_training_data.extend(news * 10)
    
    # å°èªªæ•¸æ“š (x3) - å¢åŠ ä¸Šä¸‹æ–‡å¤šæ¨£æ€§ï¼Œé˜²æ­¢ Overfitting
    if novel:
        print(f"ğŸ“ˆ å°èªªæ•¸æ“šå€å¢ x3")
        all_training_data.extend(novel * 3)
    
    # æ¸¯éµæ•¸æ“š (x10) - é‡å°æ€§é ˜åŸŸçŸ¥è­˜
    if mtr:
        print(f"ğŸ“ˆ æ¸¯éµæ•¸æ“šå€å¢ x10")
        all_training_data.extend(mtr * 10)

    # 5. æ´—ç‰Œ
    random.shuffle(all_training_data)

    # 6. æ•¸æ“šé›†æˆåˆ†åˆ†æ
    pos_count = sum(1 for d in all_training_data if any(t > 0 for t in d['ner_tags']))
    neg_count = len(all_training_data) - pos_count

    print(f"ğŸ“Š æœ€çµ‚æ•¸æ“šé›†æ‘˜è¦ï¼š")
    print(f"   - ç¸½æ•¸: {len(all_training_data)}")
    print(f"   - å«å¯¦é«”æ¨£æœ¬: {pos_count}")
    print(f"   - ç´”è² æ¨£æœ¬ (Negative Samples): {neg_count} (ä½” {neg_count/len(all_training_data):.1%})")

    # 7. è¼¸å‡º
    output = {"data": all_training_data, "label2id": LABEL2ID, "id2label": ID2LABEL}
    with open("train_data_lora.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"ğŸš€ train_data_lora.json å·²ç”Ÿæˆï¼")