# src/training/prepare_data.py
import json
import os
import random
import sys

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

from src.config import LABEL2ID, ID2LABEL

# 1. éœæ…‹ç¦æ­¢åå–® (é‡å°å¸¸ç”¨è©åŒ URL)
STRICT_FORBIDDEN = [
    "ä¸­åœ‹", "åœ‹éµ", "æ¸¯éµ", "MTR", "éµè·¯", "é›†åœ˜", "æœ‰é™å…¬å¸", "åå››äº”", "åäº”äº”", "å»ºè¨­", "ç™¼å±•", "é«˜éµ",
    "éŠ€è¡Œ", "HSBC", "åŒ¯è±", "æ¸£æ‰“", "ä¸­éŠ€", "æ’ç”Ÿ", "æ”¯ä»˜å¯¶", "Alipay", "PayMe", "FPS", "è½‰æ•¸å¿«",
    "é †è±", "SF Express", "DHL", "æ·˜å¯¶", "Foodpanda", "Deliveroo",
    "é¦™æ¸¯", "ä¹é¾", "æ–°ç•Œ", "ä¸­å¿ƒ", "å¤§å»ˆ", "å»£å ´", "è¡—é“", "Road", "Street", "Building", "Tower",
    "http", "https", ".com", ".org", ".net", "www", "åŸæ–‡ç¶²å€"
]

def load_json(path):
    if not os.path.exists(path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {path}")
        return []
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def extract_gold_entities(data_list):
    """å‹•æ…‹æå–ï¼šå¾æ–°èåŒ MTR æ•¸æ“šä¸­æå–æ‰€æœ‰çœŸå¯¦æ¨™è¨»éçš„å¯¦é«”å­—"""
    gold_words = set()
    for item in data_list:
        tokens = item["tokens"]
        tags = item["ner_tags"]
        for i, tag in enumerate(tags):
            if tag != 0: 
                gold_words.add(tokens[i].lower())
    return list(gold_words)

def is_clean(item, dynamic_forbidden):
    """æœ€å¼·éæ¿¾é‚è¼¯ï¼šéœæ…‹åå–® + å‹•æ…‹é‡‘æ¨™"""
    tokens = item.get("tokens", [])
    tags = item.get("ner_tags", [])
    
    # å»ºç«‹ä¸€å€‹å¤§åå–®
    full_forbidden_list = STRICT_FORBIDDEN + dynamic_forbidden

    for i, t in enumerate(tokens):
        # å¦‚æœ Token æ¨™è¨˜ä¿‚ Oï¼Œä½†ä½¢å–ºç¦æ­¢åå–®å…¥é¢
        if tags[i] == 0:
            token_low = t.lower()
            if any(forbidden_word.lower() in token_low for forbidden_word in full_forbidden_list):
                return False
    return True

if __name__ == "__main__":
    # 1. è®€å–æ•¸æ“š
    news = load_json("./data/raw/news_data.json")
    novel = load_json("./data/raw/novel_data.json")
    mtr = load_json("./data/raw/mtr_news_data.json")
    synthetic_raw = load_json("./data/raw/synthetic_data.json")

    # 2. âš¡ åŸ·è¡Œå‰ç½®å‹•æ…‹æå–
    print("ğŸ” æ­£åœ¨å¾é‡‘æ¨™æ•¸æ“šä¸­æå–å‹•æ…‹ç¦æ­¢åå–®...")
    dynamic_forbidden = extract_gold_entities(news + mtr)
    print(f"âœ… æå–å®Œæˆï¼Œæ–°å¢ {len(dynamic_forbidden)} å€‹å‹•æ…‹ä¿è­·è©ã€‚")

    # 3. ğŸ›¡ï¸ éæ¿¾åˆæˆæ•¸æ“š
    print(f"ğŸ›¡ï¸ æ­£åœ¨åŸ·è¡Œåˆæˆæ•¸æ“šæœ€çµ‚æ¸…æ´— (åŸå§‹æ•¸é‡: {len(synthetic_raw)})...")
    # å‚³å…¥å‹•æ…‹åå–®é€²è¡Œéæ¿¾
    synthetic_cleaned = [d for d in synthetic_raw if is_clean(d, dynamic_forbidden)]
    
    removed_count = len(synthetic_raw) - len(synthetic_cleaned)
    if removed_count > 0:
        print(f"ğŸš« å·²è‡ªå‹•å‰”é™¤ {removed_count} æ¢å¯èƒ½å°è‡´æ¨™ç±¤ç«¶çˆ­çš„åˆæˆæ¨£æœ¬ã€‚")

    # 4. æŒ‰æ¬Šé‡åˆä½µ (å•†ç”¨å„ªåŒ–ç‰ˆæ¬Šé‡)
    all_training_data = []
    all_training_data.extend(synthetic_cleaned)   # x1
    all_training_data.extend(news * 10)           # x10
    all_training_data.extend(novel * 2)            # x2
    all_training_data.extend(mtr * 10)            # x10

    # 5. æ´—ç‰Œ
    random.shuffle(all_training_data)

    # 6. æ•¸æ“šé›†æˆåˆ†åˆ†æ
    pos_count = sum(1 for d in all_training_data if any(t > 0 for t in d['ner_tags']))
    neg_count = len(all_training_data) - pos_count

    print(f"ğŸ“Š æ•¸æ“šåˆ†ä½ˆæ‘˜è¦ï¼š")
    print(f"   - æ­£æ¨£æœ¬: {pos_count}")
    print(f"   - è² æ¨£æœ¬: {neg_count} (ç´„ {neg_count/len(all_training_data):.1%})")

    # 7. è¼¸å‡º
    output = {"data": all_training_data, "label2id": LABEL2ID, "id2label": ID2LABEL}
    with open("train_data_lora.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"ğŸš€ æœ€çµ‚è¨“ç·´é›†æ‰“åŒ…å®Œæˆï¼ç¸½æ¨£æœ¬æ•¸: {len(all_training_data)}")