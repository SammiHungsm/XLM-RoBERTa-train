# src/training/prepare_data.py
import json
import os
import random
import sys

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

# âœ… 1. å¾ Config å°å…¥ BASE_FORBIDDENï¼Œä¿æŒä»£ç¢¼æ•´æ½”
from src.config import LABEL2ID, ID2LABEL, BASE_FORBIDDEN
# âœ… 2. å°å…¥æ©Ÿæ§‹åå–®ï¼Œè‡ªå‹•åŒæ­¥
from src.utils.templates import ALL_HK_ORGS 

# O çš„ ID
O_ID = LABEL2ID.get("O", 0)

# ğŸ”¥ æ ¸å¿ƒé‚è¼¯ï¼šå‹•æ…‹åˆä½µã€ŒåŸºç¤ç¦æ­¢è©ã€èˆ‡ã€Œæ‰€æœ‰å·²çŸ¥æ©Ÿæ§‹åã€
STRICT_FORBIDDEN = set(BASE_FORBIDDEN) | set(ALL_HK_ORGS)

def load_json(path):
    if not os.path.exists(path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {path}ï¼Œå°‡è·³éã€‚")
        return []
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
        if isinstance(data, dict) and "data" in data:
            return data["data"]
        return data

def extract_gold_entities(data_list):
    """
    ğŸ”¥ [å·²å‡ç´š] å‹•æ…‹æå–å¯¦é«”
    è§£æ BIO æ¨™ç±¤ï¼Œé‚„åŸå‡ºå®Œæ•´çš„å¯¦é«”è©ï¼ˆä¾‹å¦‚å¾ "B-ORG, I-ORG" é‚„åŸå‡º "æ¸¯éµ"ï¼‰ã€‚
    """
    gold_words = set()
    for item in data_list:
        tokens = item.get("tokens", [])
        tags = item.get("ner_tags", [])
        
        current_word = ""
        for i, tag in enumerate(tags):
            if tag != O_ID:
                current_word += tokens[i]
            else:
                if len(current_word) >= 2:
                    gold_words.add(current_word.lower())
                current_word = ""
        
        if len(current_word) >= 2:
            gold_words.add(current_word.lower())
            
    return gold_words

def is_clean(item, forbidden_set):
    """
    ğŸ”¥ [å·²å‡ç´š] éæ¿¾é‚è¼¯
    å°‡ O-tag çš„éƒ¨åˆ†é‡çµ„ç‚ºå­—ä¸²å¾Œå†æª¢æŸ¥ï¼Œè§£æ±º Tokenizer å°‡ã€Œæ¸¯éµã€åˆ‡åˆ†å¾Œç„¡æ³•éæ¿¾çš„å•é¡Œã€‚
    """
    tokens = item.get("tokens", [])
    tags = item.get("ner_tags", [])
    
    if len(tokens) != len(tags): return False

    # 1. æ§‹å»ºã€Œç´” O å…§å®¹å­—ä¸²ã€
    o_content_segments = []
    current_segment = ""
    
    for i, t in enumerate(tokens):
        if tags[i] == O_ID:
            current_segment += t
        else:
            if current_segment:
                o_content_segments.append(current_segment)
                current_segment = ""
    
    if current_segment:
        o_content_segments.append(current_segment)
    
    # 2. æª¢æŸ¥æ¯å€‹ O ç‰‡æ®µæ˜¯å¦å«æœ‰ç¦æ­¢è©
    for segment in o_content_segments:
        seg_lower = segment.lower()
        for forbidden in forbidden_set:
            if forbidden.lower() in seg_lower:
                return False
                
    return True

if __name__ == "__main__":
    # 1. è®€å–æ•¸æ“š
    print("ğŸ“‚ è®€å–åŸå§‹æ•¸æ“š...")
    news = load_json("./data/raw/news_data.json")
    novel = load_json("./data/raw/novel_data.json")
    mtr = load_json("./data/raw/mtr_news_data.json")
    synthetic_raw = load_json("./data/raw/synthetic_data.json")

    # 2. åŸ·è¡Œå‰ç½®å‹•æ…‹æå–
    dynamic_forbidden = extract_gold_entities(news + mtr)
    full_forbidden_set = STRICT_FORBIDDEN | dynamic_forbidden
    print(f"âœ… ç¦æ­¢åå–®æ§‹å»ºå®Œæˆ (éœæ…‹: {len(STRICT_FORBIDDEN)} + å‹•æ…‹: {len(dynamic_forbidden)})")
    
    # 3. éæ¿¾åˆæˆæ•¸æ“š
    if synthetic_raw:
        print(f"ğŸ›¡ï¸ æ­£åœ¨åŸ·è¡Œåˆæˆæ•¸æ“šæœ€çµ‚æ¸…æ´— (åŸå§‹: {len(synthetic_raw)})...")
        synthetic_cleaned = [d for d in synthetic_raw if is_clean(d, full_forbidden_set)]
        removed_count = len(synthetic_raw) - len(synthetic_cleaned)
        if removed_count > 0:
            print(f"ğŸš« å·²å‰”é™¤ {removed_count} æ¢æ½›åœ¨æ¨™ç±¤è¡çªçš„åˆæˆæ¨£æœ¬ã€‚")
    else:
        synthetic_cleaned = []

    # 4. æŒ‰æ¬Šé‡åˆä½µ
    all_training_data = []
    
    # åˆæˆæ•¸æ“š (x1)
    all_training_data.extend(synthetic_cleaned)
    
    # æ–°èæ•¸æ“š (x10)
    if news: all_training_data.extend(news * 10)
    
    # å°èªªæ•¸æ“š (x1)
    if novel: 
        print(f"ğŸ“‰ å°èªªæ•¸æ“šæ¬Šé‡é™è‡³ x1 (é˜²æ­¢å¼•å…¥éå¤šè² æ¨£æœ¬)")
        all_training_data.extend(novel * 1)
    
    # æ¸¯éµæ•¸æ“š (x10)
    if mtr: all_training_data.extend(mtr * 10)

    # 5. å¼·åˆ¶å¹³è¡¡æ©Ÿåˆ¶ (Balancing)
    print("âš–ï¸ æ­£åœ¨åŸ·è¡Œæ•¸æ“šå¹³è¡¡ (Target: è² æ¨£æœ¬ä½”ç¸½æ•¸ ~25%)...")
    
    pos_samples = [d for d in all_training_data if any(t != O_ID for t in d['ner_tags'])]
    neg_samples = [d for d in all_training_data if all(t == O_ID for t in d['ner_tags'])]
    
    print(f"   - åŸå§‹åˆ†ä½ˆ -> æ­£æ¨£æœ¬: {len(pos_samples)} | è² æ¨£æœ¬: {len(neg_samples)}")

    target_neg_count = int(len(pos_samples) * 0.35) 
    
    if len(neg_samples) > target_neg_count:
        print(f"   - âœ‚ï¸ å‰Šæ¸›è² æ¨£æœ¬: {len(neg_samples)} -> {target_neg_count}")
        neg_samples = random.sample(neg_samples, target_neg_count)
    else:
        print(f"   - âœ… è² æ¨£æœ¬æ•¸é‡å¥åº·ï¼Œç„¡éœ€å‰Šæ¸›ã€‚")
        
    final_data = pos_samples + neg_samples
    random.shuffle(final_data)

    # 6. æœ€çµ‚çµ±è¨ˆ
    final_pos = len(pos_samples)
    final_neg = len(neg_samples)
    total = len(final_data)

    print(f"ğŸ“Š æœ€çµ‚æ•¸æ“šé›†æ‘˜è¦ï¼š")
    print(f"   - ç¸½æ•¸: {total}")
    print(f"   - å«å¯¦é«”æ¨£æœ¬: {final_pos} ({final_pos/total:.1%})")
    print(f"   - ç´”è² æ¨£æœ¬:   {final_neg} ({final_neg/total:.1%})")

    # 7. è¼¸å‡º
    output = {"data": final_data, "label2id": LABEL2ID, "id2label": ID2LABEL}
    with open("train_data_lora.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"ğŸš€ train_data_lora.json å·²ç”Ÿæˆï¼")