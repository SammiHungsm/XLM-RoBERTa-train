# src/training/prepare_data.py
import json
import os
import random
import sys

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

from src.config import LABEL2ID, ID2LABEL

# O çš„ ID
O_ID = LABEL2ID.get("O", 0)

# 1. éœæ…‹ç¦æ­¢åå–®
STRICT_FORBIDDEN = {
    "ä¸­åœ‹", "åœ‹éµ", "æ¸¯éµ", "MTR", "éµè·¯", "é›†åœ˜", "æœ‰é™å…¬å¸", "åå››äº”", "åäº”äº”", "å»ºè¨­", "ç™¼å±•", "é«˜éµ",
    "éŠ€è¡Œ", "HSBC", "åŒ¯è±", "æ¸£æ‰“", "ä¸­éŠ€", "æ’ç”Ÿ", "æ”¯ä»˜å¯¶", "Alipay", "PayMe", "FPS", "è½‰æ•¸å¿«",
    "é †è±", "SF Express", "DHL", "æ·˜å¯¶", "Foodpanda", "Deliveroo",
    "é¦™æ¸¯", "ä¹é¾", "æ–°ç•Œ", "ä¸­å¿ƒ", "å¤§å»ˆ", "å»£å ´", "è¡—é“", "Road", "Street", "Building", "Tower",
    "http", "https", ".com", ".org", ".net", "www", "åŸæ–‡ç¶²å€"
}

def load_json(path):
    if not os.path.exists(path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {path}ï¼Œå°‡è·³éã€‚")
        return []
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
        if isinstance(data, dict) and "data" in data:
            return data["data"]
        return data

def extract_gold_entities(data_list):
    """å‹•æ…‹æå–å¯¦é«”"""
    gold_words = set()
    for item in data_list:
        tokens = item.get("tokens", [])
        tags = item.get("ner_tags", [])
        for i, tag in enumerate(tags):
            if tag != O_ID: 
                word = tokens[i].lower()
                if len(word) >= 2:
                    gold_words.add(word)
    return gold_words

def is_clean(item, forbidden_set):
    """éæ¿¾é‚è¼¯"""
    tokens = item.get("tokens", [])
    tags = item.get("ner_tags", [])
    if len(tokens) != len(tags): return False
    for i, t in enumerate(tokens):
        if tags[i] == O_ID:
            token_low = t.lower()
            if token_low in forbidden_set:
                return False 
    return True

if __name__ == "__main__":
    # 1. è®€å–æ•¸æ“š
    print("ğŸ“‚ è®€å–åŸå§‹æ•¸æ“š...")
    news = load_json("./data/raw/news_data.json")
    novel = load_json("./data/raw/novel_data.json")
    mtr = load_json("./data/raw/mtr_news_data.json")
    synthetic_raw = load_json("./data/raw/synthetic_data.json")

    # 2. åŸ·è¡Œå‰ç½®å‹•æ…‹æå–
    # æ³¨æ„ï¼šå¦‚æœé€™è£¡çµæœæ˜¯ 0ï¼Œä»£è¡¨ news/mtr å¯èƒ½æœ‰å•é¡Œï¼Œä½†æˆ‘å€‘ä¸‹é¢çš„å¹³è¡¡æ©Ÿåˆ¶æœƒé˜²æ­¢å®ƒç ´å£æ•¸æ“šé›†
    dynamic_forbidden = extract_gold_entities(news + mtr)
    full_forbidden_set = set(w.lower() for w in STRICT_FORBIDDEN) | dynamic_forbidden
    print(f"âœ… ç¦æ­¢åå–®æ§‹å»ºå®Œæˆ (éœæ…‹: {len(STRICT_FORBIDDEN)} + å‹•æ…‹: {len(dynamic_forbidden)})")

    # 3. éæ¿¾åˆæˆæ•¸æ“š
    if synthetic_raw:
        print(f"ğŸ›¡ï¸ æ­£åœ¨åŸ·è¡Œåˆæˆæ•¸æ“šæœ€çµ‚æ¸…æ´— (åŸå§‹: {len(synthetic_raw)})...")
        synthetic_cleaned = [d for d in synthetic_raw if is_clean(d, full_forbidden_set)]
        removed_count = len(synthetic_raw) - len(synthetic_cleaned)
        if removed_count > 0:
            print(f"ğŸš« å·²å‰”é™¤ {removed_count} æ¢æ½›åœ¨æ¨™ç±¤è¡çªçš„åˆæˆæ¨£æœ¬ã€‚")
    else:
        synthetic_cleaned = []

    # 4. æŒ‰æ¬Šé‡åˆä½µ
    all_training_data = []
    
    # åˆæˆæ•¸æ“š (x1) - é€™æ˜¯æˆ‘å€‘çš„ä¸»åŠ›
    all_training_data.extend(synthetic_cleaned)
    
    # æ–°èæ•¸æ“š (x10) - å¦‚æœè£¡é¢å…¨æ˜¯è² æ¨£æœ¬ï¼Œé€™ä¸€æ­¥æœƒå¼•å…¥å¤§é‡è² æ¨£æœ¬
    if news: all_training_data.extend(news * 10)
    
    # å°èªªæ•¸æ“š (æ”¹ç‚º x1) - é™ä½æ¬Šé‡ï¼Œå› ç‚ºå°èªªé€šå¸¸è² æ¨£æœ¬å¾ˆå¤š
    if novel: 
        print(f"ğŸ“‰ å°èªªæ•¸æ“šæ¬Šé‡é™è‡³ x1 (é˜²æ­¢å¼•å…¥éå¤šè² æ¨£æœ¬)")
        all_training_data.extend(novel * 1)
    
    # æ¸¯éµæ•¸æ“š (x10)
    if mtr: all_training_data.extend(mtr * 10)

    # 5. ğŸ”¥ [æ ¸å¿ƒä¿®æ”¹] å¼·åˆ¶å¹³è¡¡æ©Ÿåˆ¶ (Balancing)
    print("âš–ï¸ æ­£åœ¨åŸ·è¡Œæ•¸æ“šå¹³è¡¡ (Target: è² æ¨£æœ¬ä½”ç¸½æ•¸ ~25%)...")
    
    # åˆ†é›¢æ­£è² æ¨£æœ¬
    pos_samples = [d for d in all_training_data if any(t != O_ID for t in d['ner_tags'])]
    neg_samples = [d for d in all_training_data if all(t == O_ID for t in d['ner_tags'])]
    
    print(f"   - åŸå§‹åˆ†ä½ˆ -> æ­£æ¨£æœ¬: {len(pos_samples)} | è² æ¨£æœ¬: {len(neg_samples)}")

    # è¨ˆç®—ç›®æ¨™è² æ¨£æœ¬æ•¸é‡ (æ­£æ¨£æœ¬çš„ 1/3ï¼Œå³ç¸½æ•¸çš„ 25% å·¦å³)
    target_neg_count = int(len(pos_samples) * 0.35) 
    
    if len(neg_samples) > target_neg_count:
        print(f"   - âœ‚ï¸ å‰Šæ¸›è² æ¨£æœ¬: {len(neg_samples)} -> {target_neg_count}")
        neg_samples = random.sample(neg_samples, target_neg_count)
    else:
        print(f"   - âœ… è² æ¨£æœ¬æ•¸é‡å¥åº·ï¼Œç„¡éœ€å‰Šæ¸›ã€‚")
        
    # åˆä½µä¸¦æ´—ç‰Œ
    final_data = pos_samples + neg_samples
    random.shuffle(final_data)

    # 6. æœ€çµ‚çµ±è¨ˆ
    final_pos = len(pos_samples)
    final_neg = len(neg_samples)
    total = len(final_data)

    print(f"ğŸ“Š æœ€çµ‚æ•¸æ“šé›†æ‘˜è¦ï¼š")
    print(f"   - ç¸½æ•¸: {total}")
    print(f"   - å«å¯¦é«”æ¨£æœ¬: {final_pos} ({final_pos/total:.1%})")
    print(f"   - ç´”è² æ¨£æœ¬:   {final_neg} ({final_neg/total:.1%})")

    # 7. è¼¸å‡º
    output = {"data": final_data, "label2id": LABEL2ID, "id2label": ID2LABEL}
    with open("train_data_lora.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"ğŸš€ train_data_lora.json å·²ç”Ÿæˆï¼")