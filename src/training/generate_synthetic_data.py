import json
import os
import random
import sys
from faker import Faker

# è¨­å®šè·¯å¾‘ä»¥ä¾¿è®€å– src æ¨¡çµ„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.append(project_root)

from src.utils.tokenizer import smart_tokenize
from src.utils.generators import get_random_fillers
from src.utils.loaders import load_names, load_addresses, load_negative_samples
from src.utils.templates import get_all_templates
from src.config import LABEL2ID

fake = Faker(['zh_TW', 'en_US'])

# ===========================
# ğŸ›¡ï¸ ç¦æ­¢åå–®è¨­å®š
# ===========================
# CRITICAL: é€™äº›è©çµ•å°ä¸èƒ½å‡ºç¾åœ¨ O (éå¯¦é«”) æ¨™ç±¤ä¸­ï¼Œå¦å‰‡æœƒèª¤å°æ¨¡å‹
# (ä¾‹å¦‚ï¼šå¦‚æœ "MTR" è¢«æ¨™ç‚º Oï¼Œæ¨¡å‹å°±æœƒå­¸æœƒ "MTR" ä¸ç”¨é®)
CRITICAL_FORBIDDEN = [
    "ä¸­åœ‹", "åœ‹éµ", "æ¸¯éµ", "MTR", "éµè·¯", "é›†åœ˜", "æœ‰é™å…¬å¸", 
    "åå››äº”", "åäº”äº”", "å»ºè¨­", "ç™¼å±•", "é«˜éµ"
]

# NAME_FORBIDDEN: é€™äº›è©ä¸èƒ½å‡ºç¾åœ¨ NAME (äººå) å¯¦é«”å…§éƒ¨ï¼Œä½†å¯ä»¥å‡ºç¾åœ¨ä¸Šä¸‹æ–‡ (O) ä¸­
# (ä¾‹å¦‚ï¼šæˆ‘å€‘å…è¨± "é™³å…ˆç”Ÿ" (Name + O)ï¼Œä½†ä¸å…è¨± "é™³å…ˆç”Ÿ" æ•´å€‹è¢«æ¨™ç‚º Name)
NAME_FORBIDDEN = [
    "å…ˆç”Ÿ", "è€é—†", "å°å§", "å¥³å£«"
]

def generate_synthetic(target_count=20000):
    # ===========================
    # 1. è¼‰å…¥åŸå§‹è³‡æº
    # ===========================
    print("ğŸ“‚ æ­£åœ¨è¼‰å…¥åŸºç¤èªæ–™åº«...")
    
    # è¼‰å…¥åå­— (å›å‚³ Dict: {'standard': [...], 'transliterated': [...]})
    raw_names_data = load_names("./data/raw/Chinese-Names-Corpus-master")
    
    # è¼‰å…¥åœ°å€ (GeoJSON è§£æå¾Œçš„çµ„åˆ)
    raw_addresses = load_addresses("./data/raw/geojson_files")
    
    # ğŸ”¥ è¼‰å…¥çœŸå¯¦è² æ¨£æœ¬ (å¾ç¾æœ‰çš„ JSON æ•¸æ“šä¸­æå–ç´” O å¥å­)
    existing_jsons = [
        "./data/raw/news_data.json",
        "./data/raw/novel_data.json",
        "./data/raw/mtr_news_data.json"
    ]
    real_negative_samples = load_negative_samples(existing_jsons, max_samples=5000)
    
    # ===========================
    # 2. éæ¿¾èˆ‡æ¸…æ´— (Sanitization)
    # ===========================
    print("ğŸ§¹ æ­£åœ¨éæ¿¾æ•æ„Ÿè©...")

    # éæ¿¾æ¨™æº–å (Chinese/Ancient...)
    standard_clean = [
        n for n in raw_names_data["standard"] 
        if not any(word in n for word in CRITICAL_FORBIDDEN + NAME_FORBIDDEN)
    ]
    
    # éæ¿¾è­¯å (English_Cn...)
    transliterated_clean = [
        n for n in raw_names_data["transliterated"] 
        if not any(word in n for word in CRITICAL_FORBIDDEN + NAME_FORBIDDEN)
    ]
    
    # é‡çµ„ä¹¾æ·¨çš„åå­—æ•¸æ“šåŒ…
    names_data = {
        "standard": standard_clean,
        "transliterated": transliterated_clean
    }
    
    # éæ¿¾åœ°å€ (åœ°å€åªéœ€éæ¿¾ CRITICALï¼Œå› ç‚ºåœ°å€åŒ…å« "å…ˆç”Ÿ" æ˜¯åˆæ³•çš„ï¼Œå¦‚ "å…ˆç”Ÿé‡Œ")
    addresses = [a for a in raw_addresses if not any(word in a for word in CRITICAL_FORBIDDEN)]
    
    # ===========================
    # 3. é–‹å§‹ç”Ÿæˆå¾ªç’°
    # ===========================
    templates = get_all_templates()
    data = []
    print(f"ğŸš€ æ­£åœ¨ç”Ÿæˆã€Œåˆ†æºè™•ç†ã€åˆæˆæ•¸æ“š... ç›®æ¨™: {target_count}")

    while len(data) < target_count:
        # 85% æ­£æ¨£æœ¬ (æœ‰å¯¦é«”)ï¼Œ15% è² æ¨£æœ¬ (å…¨ O)
        is_positive = random.random() < 0.85
        tokens_list = []
        tags_list = []
        is_contaminated = False

        if is_positive:
            # --- æ­£æ¨£æœ¬ç”Ÿæˆ (Template Based) ---
            template_parts = random.choice(templates)
            
            # ğŸ”¥ å‚³å…¥åˆ†é¡å¥½çš„æ•¸æ“šåŒ… (names_data)
            fillers = get_random_fillers(names_data, addresses)
            
            for part in template_parts:
                entity_type = "O"
                
                # æª¢æŸ¥ Template Part æ˜¯å¦éœ€è¦å¡«å……
                if part in fillers:
                    text_segment = str(fillers[part])
                    
                    # ğŸ·ï¸ å¯¦é«”æ¨™ç±¤æ˜ å°„ (å¿…é ˆåŒ…å«æ‰€æœ‰ Template ç”¨åˆ°çš„ Key)
                    if part == "{name}": entity_type = "NAME"
                    elif part == "{addr}": entity_type = "ADDRESS"
                    elif part == "{phone}": entity_type = "PHONE"
                    elif part == "{id_num}": entity_type = "ID"
                    elif part == "{account}": entity_type = "ACCOUNT"
                    elif part == "{plate}": entity_type = "LICENSE_PLATE"
                    elif part == "{org}": entity_type = "ORG"
                    
                    # ğŸ”¥ è£œæ¼ Keys (é˜²æ­¢æ¼æ¨™)
                    elif part == "{bank}": entity_type = "ORG"       # éŠ€è¡Œ -> ORG
                    elif part == "{company}": entity_type = "ORG"    # å…¬å¸ -> ORG
                    elif part == "{station}": entity_type = "ORG"    # èœé³¥é©›ç«™ -> ORG
                    elif part == "{pickup_code}": entity_type = "ID" # å–ä»¶ç¢¼ -> ID
                    elif part == "{code}": entity_type = "ID"        # é©—è­‰ç¢¼ -> ID
                    elif part == "{order_id}": entity_type = "ID"    # è¨‚å–®è™Ÿ -> ID
                    elif part == "{email}": entity_type = "O"        # Email æš«ä¸é®è”½
                    
                else:
                    text_segment = part

                tokens = smart_tokenize(text_segment)
                
                # ğŸ›¡ï¸ æ ¸å¿ƒå®‰å…¨æª¢æŸ¥
                # å¦‚æœé€™ä¸€æ®µæ˜¯ O (éå¯¦é«”)ï¼Œå®ƒçµ•å°ä¸èƒ½åŒ…å« MTR ç­‰ç¦æ­¢è©
                if entity_type == "O":
                    if any(word in text_segment for word in CRITICAL_FORBIDDEN):
                        is_contaminated = True
                        break

                tokens_list.extend(tokens)
                
                # ç”Ÿæˆ BIO æ¨™ç±¤
                if entity_type != "O":
                    try:
                        # B-TYPE
                        tags_list.append(LABEL2ID[f"B-{entity_type}"])
                        # I-TYPE
                        tags_list.extend([LABEL2ID[f"I-{entity_type}"]] * (len(tokens) - 1))
                    except KeyError:
                        print(f"âš ï¸ Warning: Label {entity_type} not found in config. Marking as O.")
                        tags_list.extend([LABEL2ID["O"]] * len(tokens))
                else:
                    tags_list.extend([LABEL2ID["O"]] * len(tokens))
        else:
            # --- è² æ¨£æœ¬ç”Ÿæˆ (Negative Samples) ---
            # å„ªå…ˆä½¿ç”¨çœŸå¯¦èªæ–™ï¼Œå¦‚æœæ²’æœ‰æˆ–éš¨æ©Ÿè½é¸ï¼Œæ‰ç”¨ Faker
            if real_negative_samples and random.random() < 0.8:
                raw_sent = random.choice(real_negative_samples)
            else:
                raw_sent = fake.sentence()

            # è² æ¨£æœ¬çµ•å°ä¸èƒ½åŒ…å«ç¦æ­¢è©
            if any(word in raw_sent for word in CRITICAL_FORBIDDEN):
                continue
                
            tokens = smart_tokenize(raw_sent)
            tokens_list = tokens
            tags_list = [LABEL2ID["O"]] * len(tokens)

        # ===========================
        # 4. æœ€çµ‚æ ¡å°èˆ‡å„²å­˜
        # ===========================
        # æ¢ä»¶ï¼šç„¡æ±¡æŸ“ + é•·åº¦ä¸€è‡´ + éç©º
        if not is_contaminated and len(tokens_list) == len(tags_list) and len(tokens_list) > 0:
            data.append({"tokens": tokens_list, "ner_tags": tags_list})

    return data

if __name__ == "__main__":
    # ç”Ÿæˆç›®æ¨™æ•¸é‡
    results = generate_synthetic(target_count=20000)
    
    output_path = "./data/raw/synthetic_data.json"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… åˆæˆæ•¸æ“šç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“ æª”æ¡ˆå·²å„²å­˜è‡³: {output_path}")