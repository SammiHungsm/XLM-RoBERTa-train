import json
import os
import random
import sys
from tqdm import tqdm # åŠ å€‹é€²åº¦æ¢æ¯”è¼ƒå¥½çœ‹
from faker import Faker

# è¨­å®šè·¯å¾‘ä»¥ä¾¿è®€å– src æ¨¡çµ„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.append(project_root)

from src.utils.tokenizer import smart_tokenize
from src.utils.generators import get_random_fillers
from src.utils.loaders import load_names, load_addresses, load_negative_samples
from src.utils.templates import get_all_templates
from src.config import LABEL2ID

fake = Faker(['zh_TW', 'en_US'])

# ===========================
# ğŸ›¡ï¸ ç¦æ­¢åå–®è¨­å®š (ä¿ç•™æ‚¨çš„è¨­å®š)
# ===========================
CRITICAL_FORBIDDEN = [
    "ä¸­åœ‹", "åœ‹éµ", "æ¸¯éµ", "MTR", "éµè·¯", "é›†åœ˜", "æœ‰é™å…¬å¸", 
    "åå››äº”", "åäº”äº”", "å»ºè¨­", "ç™¼å±•", "é«˜éµ", "éŠ€è¡Œ", "åˆ†è¡Œ" # åŠ å…¥éŠ€è¡Œç›¸é—œ
]

NAME_FORBIDDEN = [
    "å…ˆç”Ÿ", "è€é—†", "å°å§", "å¥³å£«"
]

def generate_synthetic(target_count=20000):
    # ===========================
    # 1. è¼‰å…¥åŸå§‹è³‡æº
    # ===========================
    print("ğŸ“‚ æ­£åœ¨è¼‰å…¥åŸºç¤èªæ–™åº«...")
    
    # è¼‰å…¥åå­—
    raw_names_data = load_names("./data/raw/Chinese-Names-Corpus-master")
    
    # è¼‰å…¥åœ°å€ (GeoJSON)
    # ğŸ’¡ è¨»ï¼šéŠ€è¡Œåœ°å€ (Excel) å·²ç¶“æ•´åˆåœ¨ generators.py å…§éƒ¨ï¼Œé€™è£¡è¼‰å…¥çš„æ˜¯éš¨æ©Ÿè·¯å
    raw_addresses = load_addresses("./data/raw/geojson_files")
    
    # è¼‰å…¥çœŸå¯¦è² æ¨£æœ¬
    existing_jsons = [
        "./data/raw/news_data.json",
        "./data/raw/novel_data.json",
        "./data/raw/mtr_news_data.json"
    ]
    real_negative_samples = load_negative_samples(existing_jsons, max_samples=5000)
    
    # ===========================
    # 2. éæ¿¾èˆ‡æ¸…æ´— (Sanitization)
    # ===========================
    print("ğŸ§¹ æ­£åœ¨éæ¿¾æ•æ„Ÿè©...")

    standard_clean = [
        n for n in raw_names_data["standard"] 
        if not any(word in n for word in CRITICAL_FORBIDDEN + NAME_FORBIDDEN)
    ]
    
    transliterated_clean = [
        n for n in raw_names_data["transliterated"] 
        if not any(word in n for word in CRITICAL_FORBIDDEN + NAME_FORBIDDEN)
    ]
    
    names_data = {
        "standard": standard_clean,
        "transliterated": transliterated_clean
    }
    
    addresses = [a for a in raw_addresses if not any(word in a for word in CRITICAL_FORBIDDEN)]
    
    # ===========================
    # 3. é–‹å§‹ç”Ÿæˆå¾ªç’°
    # ===========================
    # é€™è£¡æœƒè‡ªå‹•åŒ…å« Excel è¼‰å…¥çš„éŠ€è¡Œç›¸é—œ Templates
    templates = get_all_templates() 
    
    data = []
    print(f"ğŸš€ æ­£åœ¨ç”Ÿæˆã€Œåˆ†æºè™•ç†ã€åˆæˆæ•¸æ“š... ç›®æ¨™: {target_count}")

    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦
    with tqdm(total=target_count) as pbar:
        while len(data) < target_count:
            # 85% æ­£æ¨£æœ¬ (æœ‰å¯¦é«”)ï¼Œ15% è² æ¨£æœ¬ (å…¨ O)
            is_positive = random.random() < 0.85
            tokens_list = []
            tags_list = []
            is_contaminated = False

            if is_positive:
                # --- æ­£æ¨£æœ¬ç”Ÿæˆ (Template Based) ---
                template_parts = random.choice(templates)
                
                # ğŸ”¥ é€™è£¡æœƒè‡ªå‹•æ··åˆ GeoJSON åœ°å€ å’Œ Excel éŠ€è¡Œåœ°å€
                fillers = get_random_fillers(names_data, addresses)
                
                for part in template_parts:
                    entity_type = "O"
                    
                    # æª¢æŸ¥ Template Part æ˜¯å¦éœ€è¦å¡«å……
                    if part in fillers:
                        text_segment = str(fillers[part])
                        
                        # ğŸ·ï¸ å¯¦é«”æ¨™ç±¤æ˜ å°„
                        if part == "{name}": entity_type = "NAME"
                        elif part == "{addr}": entity_type = "ADDRESS"
                        elif part == "{phone}": entity_type = "PHONE"
                        elif part == "{id_num}": entity_type = "ID"
                        elif part == "{account}": entity_type = "ACCOUNT"
                        elif part == "{plate}": entity_type = "LICENSE_PLATE"
                        elif part == "{org}": entity_type = "ORG"
                        
                        # ğŸ”¥ è£œæ¼ Keys (åŒ…å«éŠ€è¡Œç›¸é—œ)
                        elif part == "{bank}": entity_type = "ORG"       # éŠ€è¡Œ -> ORG
                        elif part == "{company}": entity_type = "ORG"    # å…¬å¸ -> ORG
                        elif part == "{station}": entity_type = "ORG"    # èœé³¥é©›ç«™ -> ORG
                        elif part == "{pickup_code}": entity_type = "O" 
                        elif part == "{code}": entity_type = "O"        
                        elif part == "{order_id}": entity_type = "O"    
                        elif part == "{email}": entity_type = "O"        
                        
                    else:
                        text_segment = part

                    tokens = smart_tokenize(text_segment)
                    
                    # ğŸ›¡ï¸ æ ¸å¿ƒå®‰å…¨æª¢æŸ¥
                    if entity_type == "O":
                        if any(word in text_segment for word in CRITICAL_FORBIDDEN):
                            is_contaminated = True
                            break

                    tokens_list.extend(tokens)
                    
                    # ç”Ÿæˆ BIO æ¨™ç±¤
                    if entity_type != "O":
                        try:
                            # B-TYPE
                            tags_list.append(LABEL2ID[f"B-{entity_type}"])
                            # I-TYPE
                            tags_list.extend([LABEL2ID[f"I-{entity_type}"]] * (len(tokens) - 1))
                        except KeyError:
                            # Fallback
                            tags_list.extend([LABEL2ID["O"]] * len(tokens))
                    else:
                        tags_list.extend([LABEL2ID["O"]] * len(tokens))
            else:
                # --- è² æ¨£æœ¬ç”Ÿæˆ ---
                if real_negative_samples and random.random() < 0.8:
                    raw_sent = random.choice(real_negative_samples)
                else:
                    raw_sent = fake.sentence()

                if any(word in raw_sent for word in CRITICAL_FORBIDDEN):
                    continue
                    
                tokens = smart_tokenize(raw_sent)
                tokens_list = tokens
                tags_list = [LABEL2ID["O"]] * len(tokens)

            # ===========================
            # 4. æœ€çµ‚æ ¡å°èˆ‡å„²å­˜
            # ===========================
            if not is_contaminated and len(tokens_list) == len(tags_list) and len(tokens_list) > 0:
                data.append({"tokens": tokens_list, "ner_tags": tags_list})
                pbar.update(1) # æ›´æ–°é€²åº¦æ¢

    return data

if __name__ == "__main__":
    # ç”Ÿæˆç›®æ¨™æ•¸é‡
    # å»ºè­°ç”Ÿæˆå¤šä¸€é»ï¼Œå› ç‚ºä½¿ç”¨äº†çœŸå¯¦éŠ€è¡Œæ•¸æ“šï¼Œå¤šæ¨£æ€§å¾ˆé«˜
    target = 20000 
    results = generate_synthetic(target_count=target)
    
    # æ³¨æ„ï¼šé€™è£¡ç›´æ¥è¼¸å‡º train_data_lora.jsonï¼Œå› ç‚ºæˆ‘å€‘å·²ç¶“åŒ…å«äº† tags
    # é€™æ¨£å°±å¯ä»¥è·³é prepare_data.py çš„æ¨™è¨»æ­¥é©Ÿï¼Œç›´æ¥é€²å…¥ clean/train
    output_path = "train_data_lora.json"
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… åˆæˆæ•¸æ“šç”Ÿæˆå®Œæˆï¼(å·²åŒ…å« BIO æ¨™ç±¤)")
    print(f"ğŸ“ æª”æ¡ˆå·²å„²å­˜è‡³: {output_path}")
    print("ğŸš€ æ‚¨å¯ä»¥è·³é prepare_data.pyï¼Œç›´æ¥åŸ·è¡Œ 'clean_and_augment.py'ï¼")