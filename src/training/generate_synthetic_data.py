# src/training/generate_synthetic_data.py
import json
import os
import random
import sys
from faker import Faker

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

from src.utils.tokenizer import smart_tokenize
from src.utils.generators import get_random_fillers
from src.utils.loaders import load_names, load_addresses, load_negative_samples
from src.utils.templates import get_all_templates
from src.config import LABEL2ID

fake = Faker(['zh_TW', 'en_US'])

# ğŸ”¥ çµ‚æ¥µã€Œé›¶éŒ¯èª¤ã€ç¦æ­¢åå–®ï¼šä»»ä½•æ¨™è¨˜ç‚º "O" çš„ Token å¦‚æœåŒ…å«é€™äº›å­—ï¼Œè©²æ¢æ•¸æ“šå³åˆ»å»¢æ£„
STRICT_FORBIDDEN = [
    "ä¸­åœ‹", "åœ‹éµ", "æ¸¯éµ", "MTR", "éµè·¯", "é›†åœ˜", "æœ‰é™å…¬å¸", 
    "åå››äº”", "åäº”äº”", "å»ºè¨­", "ç™¼å±•", "é«˜éµ", "å…ˆç”Ÿ", "è€é—†", "å°å§", "å¥³å£«"
]

def generate_synthetic(target_count=20000):
    # 1. è¼‰å…¥åŸå§‹è³‡æº
    raw_names = load_names("./Chinese-Names-Corpus-master")
    raw_addresses = load_addresses("./geojson_files")
    
    # ğŸ›¡ï¸ æª¢æŸ¥ get_random_fillers çš„è¼¸å…¥ï¼šç¢ºä¿å‚³å…¥çš„åå–®ä¸åŒ…å«ç¦æ­¢è©
    # é€™æ¨£å¯ä»¥é˜²æ­¢ä¾‹å¦‚ã€Œä¸­åœ‹äººã€è¢«ç•¶æˆ NAME æ¨™è¨˜ï¼Œæˆ–è€…ã€Œä¸­åœ‹ã€è¢«ç•¶æˆåœ°å€å‚³å…¥å¾Œå› è¦å‰‡è¡çªè¢«å»¢æ£„
    names = [n for n in raw_names if not any(word in n for word in STRICT_FORBIDDEN)]
    addresses = [a for a in raw_addresses if not any(word in a for word in STRICT_FORBIDDEN)]
    
    # ğŸ”¥ æ–·çµ•å±éšªä¾†æºï¼šå¼·åˆ¶è®Šç©ºï¼Œä¸è®€å–ä»»ä½•å¯èƒ½å«æœ‰å¯¦é«”ä½†æ¨™è¨˜ç‚º 0 çš„åŸæ–‡æª”æ¡ˆ
    neg_texts = [] 
    
    templates = get_all_templates()

    data = []
    print(f"ğŸš€ æ­£åœ¨ç”Ÿæˆã€Œé›¶æ±¡æŸ“ã€åˆæˆæ•¸æ“š... ç›®æ¨™: {target_count}")

    while len(data) < target_count:
        is_positive = random.random() < 0.85
        tokens_list = []
        tags_list = []
        is_contaminated = False

        if is_positive:
            template_parts = random.choice(templates)
            fillers = get_random_fillers(names, addresses)
            
            for part in template_parts:
                entity_type = "O"
                if part in fillers:
                    text_segment = str(fillers[part])
                    if part == "{name}": entity_type = "NAME"
                    elif part == "{addr}": entity_type = "ADDRESS"
                    elif part == "{phone}": entity_type = "PHONE"
                    elif part == "{id_num}": entity_type = "ID"
                    elif part == "{account}": entity_type = "ACCOUNT"
                    elif part == "{plate}": entity_type = "LICENSE_PLATE"
                    elif part == "{org}": entity_type = "ORG"
                else:
                    text_segment = part

                tokens = smart_tokenize(text_segment)
                
                # ğŸ”¥ æ ¸å¿ƒå®‰å…¨æª¢æŸ¥ï¼šå¦‚æœæ˜¯ O (å›ºå®šæ–‡æœ¬)ï¼Œå…§å®¹çµ•å°ä¸èƒ½åŒ…å«ç¦æ­¢è©
                if entity_type == "O":
                    if any(word in text_segment for word in STRICT_FORBIDDEN):
                        is_contaminated = True
                        break

                tokens_list.extend(tokens)
                if entity_type != "O":
                    tags_list.append(LABEL2ID[f"B-{entity_type}"])
                    tags_list.extend([LABEL2ID[f"I-{entity_type}"]] * (len(tokens) - 1))
                else:
                    tags_list.extend([LABEL2ID["O"]] * len(tokens))
        else:
            # ğŸ›¡ï¸ è² æ¨£æœ¬ç”Ÿæˆï¼šç”±æ–¼ neg_texts ç‚ºç©ºï¼Œé€™è£¡åªæœƒè¡Œ fake.sentence()
            # Faker ç”Ÿæˆçš„è‹±æ–‡/ä¸­æ–‡å¥å­åŒæ¨£è¦éæ¿¾ç¦æ­¢è©ï¼Œç¢ºä¿ 100% å®‰å…¨
            raw_sent = fake.sentence()
            if any(word in raw_sent for word in STRICT_FORBIDDEN):
                continue
                
            tokens = smart_tokenize(raw_sent)
            tokens_list = tokens
            tags_list = [LABEL2ID["O"]] * len(tokens)

        # æœ€çµ‚æ ¡å°ï¼šåªæœ‰å®Œå…¨æ²’è¢«æ±¡æŸ“ä¸”é•·åº¦åŒ¹é…çš„æ•¸æ“šæ‰è¢«æ¡ç´
        if not is_contaminated and len(tokens_list) == len(tags_list) and len(tokens_list) > 0:
            data.append({"tokens": tokens_list, "ner_tags": tags_list})

    return data

if __name__ == "__main__":
    # ç”Ÿæˆ 2 è¬æ¢ç´”æ·¨æ•¸æ“š
    results = generate_synthetic(target_count=20000)
    
    output_path = "./data/raw/synthetic_data.json"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… ã€Œé›¶éŒ¯èª¤ã€åˆæˆæ•¸æ“šç”Ÿæˆå®Œæˆã€‚")
    print(f"ğŸ“ æª”æ¡ˆå·²å„²å­˜è‡³: {output_path}")