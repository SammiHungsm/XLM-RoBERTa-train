# src/training/generate_synthetic_data.py
import json
import os
import random
import sys
from faker import Faker

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

from src.utils.tokenizer import smart_tokenize
from src.utils.generators import get_random_fillers
from src.utils.loaders import load_names, load_addresses
from src.utils.templates import get_all_templates
from src.config import LABEL2ID

fake = Faker(['zh_TW', 'en_US'])

# ç¦æ­¢åå–®
CRITICAL_FORBIDDEN = ["ä¸­åœ‹", "åœ‹éµ", "æ¸¯éµ", "MTR", "éµè·¯", "é›†åœ˜", "æœ‰é™å…¬å¸", "åå››äº”", "åäº”äº”", "å»ºè¨­", "ç™¼å±•", "é«˜éµ"]
NAME_FORBIDDEN = ["å…ˆç”Ÿ", "è€é—†", "å°å§", "å¥³å£«"]

def generate_synthetic(target_count=20000):
    # 1. è¼‰å…¥åŸå§‹è³‡æº (ç¾åœ¨ raw_names æ˜¯ä¸€å€‹ Dict)
    raw_names_data = load_names("./data/raw/Chinese-Names-Corpus-master")
    raw_addresses = load_addresses("./data/raw/geojson_files") # æ³¨æ„è·¯å¾‘å¯èƒ½è¦èª¿æ•´ï¼Œè¦–ä¹ä½ åŸ·è¡Œä½ç½®
    
    # ğŸ›¡ï¸ åˆ†åˆ¥éæ¿¾å…©å€‹åå–®
    # éæ¿¾æ¨™æº–å (Chinese/Ancient...)
    standard_clean = [
        n for n in raw_names_data["standard"] 
        if not any(word in n for word in CRITICAL_FORBIDDEN + NAME_FORBIDDEN)
    ]
    # éæ¿¾è­¯å (English_Cn...)
    transliterated_clean = [
        n for n in raw_names_data["transliterated"] 
        if not any(word in n for word in CRITICAL_FORBIDDEN + NAME_FORBIDDEN)
    ]
    
    # é‡çµ„ä¹¾æ·¨çš„æ•¸æ“šåŒ…
    names_data = {
        "standard": standard_clean,
        "transliterated": transliterated_clean
    }
    
    # éæ¿¾åœ°å€
    addresses = [a for a in raw_addresses if not any(word in a for word in CRITICAL_FORBIDDEN)]
    
    templates = get_all_templates()
    data = []
    print(f"ğŸš€ æ­£åœ¨ç”Ÿæˆã€Œåˆ†æºè™•ç†ã€åˆæˆæ•¸æ“š... ç›®æ¨™: {target_count}")

    while len(data) < target_count:
        is_positive = random.random() < 0.85
        tokens_list = []
        tags_list = []
        is_contaminated = False

        if is_positive:
            template_parts = random.choice(templates)
            
            # ğŸ”¥ å‚³å…¥åˆ†é¡å¥½çš„æ•¸æ“šåŒ…
            fillers = get_random_fillers(names_data, addresses)
            
            for part in template_parts:
                entity_type = "O"
                if part in fillers:
                    text_segment = str(fillers[part])
                    
                    if part == "{name}": entity_type = "NAME"
                    elif part == "{addr}": entity_type = "ADDRESS"
                    elif part == "{phone}": entity_type = "PHONE"
                    elif part == "{id_num}": entity_type = "ID"
                    elif part == "{account}": entity_type = "ACCOUNT"
                    elif part == "{plate}": entity_type = "LICENSE_PLATE"
                    elif part == "{org}": entity_type = "ORG"
                    # è£œæ¼ keys
                    elif part == "{bank}": entity_type = "ORG"
                    elif part == "{company}": entity_type = "ORG"
                    elif part == "{station}": entity_type = "ORG"
                    elif part == "{pickup_code}": entity_type = "ID"
                    elif part == "{code}": entity_type = "ID"
                    elif part == "{order_id}": entity_type = "ID"
                    elif part == "{email}": entity_type = "O"
                else:
                    text_segment = part

                tokens = smart_tokenize(text_segment)
                
                # æ ¸å¿ƒå®‰å…¨æª¢æŸ¥
                if entity_type == "O":
                    if any(word in text_segment for word in CRITICAL_FORBIDDEN):
                        is_contaminated = True
                        break

                tokens_list.extend(tokens)
                if entity_type != "O":
                    try:
                        tags_list.append(LABEL2ID[f"B-{entity_type}"])
                        tags_list.extend([LABEL2ID[f"I-{entity_type}"]] * (len(tokens) - 1))
                    except KeyError:
                        tags_list.extend([LABEL2ID["O"]] * len(tokens))
                else:
                    tags_list.extend([LABEL2ID["O"]] * len(tokens))
        else:
            # è² æ¨£æœ¬
            raw_sent = fake.sentence()
            if any(word in raw_sent for word in CRITICAL_FORBIDDEN):
                continue
            tokens = smart_tokenize(raw_sent)
            tokens_list = tokens
            tags_list = [LABEL2ID["O"]] * len(tokens)

        if not is_contaminated and len(tokens_list) == len(tags_list) and len(tokens_list) > 0:
            data.append({"tokens": tokens_list, "ner_tags": tags_list})

    return data

if __name__ == "__main__":
    results = generate_synthetic(target_count=20000)
    output_path = "./data/raw/synthetic_data.json"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"âœ… å®Œæˆï¼æª”æ¡ˆå·²å„²å­˜è‡³: {output_path}")