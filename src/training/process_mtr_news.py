import json
import re
# âœ… å¼•å…¥å…±ç”¨çš„ tokenizer
from data_utils.tokenizer import smart_tokenize 

# ==========================================
# 1. å®šç¾©è¦æ¨™è¨»çš„ã€Œå¯¦é«”åå–®ã€
# ==========================================
target_names = ["æ¥Šç¾ç", "é‡‘æ¾¤åŸ¹"]
target_orgs = ["æ¸¯éµ"]

# ==========================================
# 2. åŸå§‹æ–‡æœ¬
# ==========================================
raw_content = """æ¸¯éµæ–°ä»»è¡Œæ”¿ç¸½è£æ¥Šç¾çæ˜¨å±¥æ–°ï¼Œå¥¹ä»Šæ—©ï¼ˆ2æ—¥ï¼‰æ–¼æ¸¯éµç¸½éƒ¨æœƒè¦‹å‚³åª’ï¼Œç¨±ä¸Šä»»å¾Œå·¥ä½œé‡é»åŒ…æ‹¬æ¨é€²6å€‹æ–°éµè·¯é …ç›®ï¼Œå°ˆæ³¨å·¥ç¨‹å’Œè²¡å‹™ç®¡ç†åŠå»ºè¨­é«˜å³°æœŸçš„ç¾é‡‘æµï¼Œäº¦æœƒç©æ¥µç”¨å¥½ç§‘æŠ€åŠäººå·¥æ™ºèƒ½å»é¢å°æŒ‘æˆ°ï¼ŒåŒæ™‚ç™¼å±•é–‹æ‹“ä¸åŒæ¥­å‹™ã€‚æ¥Šç¾çåˆè¡¨ç¤ºï¼Œè‘—é‡èˆ‡å“¡å·¥æºé€šï¼Œç›¼ç· é€ è‰¯å¥½å·¥ä½œç’°å¢ƒè®“å“¡å·¥ç™¼æ®æ‰€é•·ã€‚

æ¥Šç¾çåŸæ˜¯æ¸¯éµå¸¸å‹™ç¸½ç›£ï¼ˆé¦™æ¸¯å®¢é‹æœå‹™ï¼‰ï¼Œæ˜¨æ—¥èµ·æ¥æ›¿é‡‘æ¾¤åŸ¹å‡ä»»è¡Œæ”¿ç¸½è£ã€‚å¥¹ç¨±åŠ å…¥æ¸¯éµå·²26å¹´ï¼Œæ˜ç™½æ–°å´—ä½è²¬ä»»é‡å¤§ï¼Œæœƒé—œå¿ƒæ˜ç™½ä¹˜å®¢éœ€è¦ä¸¦ä¸æ–·æå‡éµè·¯æœå‹™ï¼Œèˆ‡æ™‚ä¸¦é€²ï¼ŒåŒæ™‚éå›ºéµè·¯è³‡ç”¢ã€è³ªç´ åŠéŸŒæ€§ã€‚
å¥¹åˆèªªï¼Œæ¸¯éµç›®å‰å¦ä¸€é‡ä»»æ˜¯æ¨é€²6å€‹æ¶‰åŠå¤§å¶¼å±±ã€å±¯é–€åŠåŒ—éƒ½çš„éµè·¯æ–°é …ç›®ï¼Œæœªè¨ˆåŠåŒ—ç’°ç·šéƒ¨åˆ†çš„æŠ•è³‡å·²æ¶‰1400å„„å…ƒï¼Œå°‡åœ¨2027å¹´è‡³2034å¹´å¸¶ä¾†é€¾20å€‹æ–°è»Šç«™ï¼Œå·¥ç¨‹åŠè²¡å‹™ç®¡ç†å°‡å±¬å·¥ä½œé‡é»ã€‚æ¥Šæåˆ°ï¼Œæ¸¯éµå»å¹´æœ‰ä¸åŒçµ„åˆæ‡‰ä»˜ç¾é‡‘æµéœ€è¦ï¼Œå°‡æŒçºŒåˆ©ç”¨ç›¸é—œåšæ³•ï¼Œèªç‚ºç¾æ™‚ç®¡ç†ç¾é‡‘æµæ–¹é¢åšå¾—ä¸éŒ¯ã€‚
æ¥Šç¾ççºŒç¨±ï¼Œæ¸¯éµéµè·¯æœå‹™æ˜¯é¦™æ¸¯æ°‘ç”ŸåŠç¶“æ¿Ÿçš„é‡è¦åŸºå»ºè¨­æ–½ï¼Œå¿…é ˆè®“å…¶èƒ½æŒçºŒç™¼å±•ï¼Œé¢å°ä¹˜å®¢éœ€æ±‚è½‰è®Šã€å¸‚å ´åŠç’°å¢ƒè®ŠåŒ–ç­‰æŒ‘æˆ°ï¼ŒæœƒåŠªåŠ›ç©æ¥µåˆ©ç”¨äººå·¥æ™ºèƒ½ï¼Œä»¥å‰µæ–°ã€æå‡æœå‹™å’Œç‡Ÿé‹æ•ˆç‡ï¼ŒåŒæ™‚ä¿æŒç«¶çˆ­åŠ›ï¼Œé€éœ²ç¾æ­£æœ‰ä¸åŒAIæ–¹é¢çš„æ¸¬è©¦åŠå¯¦é©—ï¼Œç›®æ¨™æ˜¯å°‡æ›´å¤šç›¸é—œè¨ˆåŠƒè½å¯¦åŠæ“´å¤§ã€‚
æ¥Šç¾çäº¦è¡¨ç¤ºï¼Œæ¸¯éµæœ‰å„ªç§€çš„åœ˜éšŠï¼Œæƒ³ç‚ºå“¡å·¥ç· é€ è‰¯å¥½å·¥ä½œå¹³å°ï¼ŒæœŸæœ›åŠ å¼·å·¥ä½œæ–‡åŒ–åŠæºé€šã€‚

åŸæ–‡ç¶²å€ï¼šhttps://news.mingpao.com/ins/%E6%B8%AF%E8%81%9E/article/20260102/s00001/1767321394120"""

# ==========================================
# 3. è™•ç†é‚è¼¯ (å·²ä¿®æ­£ç‚º Word-level å…¼å®¹)
# ==========================================
def process_data(text):
    # A. ç§»é™¤ URL
    if "åŸæ–‡ç¶²å€ï¼š" in text:
        text = text.split("åŸæ–‡ç¶²å€ï¼š")[0]

    # B. åˆ†å¥
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n])', text)
    segments = []
    current_sent = ""
    for s in sentences:
        current_sent += s
        if re.match(r'[ã€‚ï¼ï¼Ÿ\n]', s):
            if current_sent.strip():
                segments.append(current_sent.strip())
            current_sent = ""
    if current_sent.strip(): segments.append(current_sent.strip())

    # C. è‡ªå‹•æ¨™è¨»
    label2id = {
        "O": 0, "B-NAME": 1, "I-NAME": 2, 
        "B-ADDRESS": 3, "I-ADDRESS": 4, 
        "B-PHONE": 5, "I-PHONE": 6, 
        "B-ID": 7, "I-ID": 8, 
        "B-ACCOUNT": 9, "I-ACCOUNT": 10,
        "B-LICENSE_PLATE": 11, "I-LICENSE_PLATE": 12,
        "B-ORG": 13, "I-ORG": 14
    }

    final_data = []

    for sent in segments:
        # [ä¿®æ­£ 1] ä½¿ç”¨ smart_tokenize (Word-level)
        tokens = smart_tokenize(sent) 
        tags = [label2id["O"]] * len(tokens)
        
        # [ä¿®æ­£ 2] å»ºç«‹ Token èˆ‡ å­—ç¬¦ä½ç½® çš„æ˜ å°„ (Token Spans)
        # é€™æ˜¯æœ€é—œéµçš„ä¸€æ­¥ï¼šç¢ºä¿ä¸ç®¡æ˜¯ "MTR" (len 3) é‚„æ˜¯ "æ¥Š" (len 1) éƒ½èƒ½å°æº–ä½ç½®
        token_spans = []
        search_start = 0
        for token in tokens:
            # åœ¨å¥å­ä¸­å°‹æ‰¾é€™å€‹ token çš„çœŸå¯¦ä½ç½®
            start = sent.find(token, search_start)
            if start == -1: 
                token_spans.append(None)
                continue
            end = start + len(token)
            token_spans.append((start, end)) # è¨˜éŒ„ (é–‹å§‹, çµæŸ)
            search_start = end

        # å®šç¾©ä¸€å€‹é€šç”¨çš„æ¨™è¨»å‡½æ•¸
        def apply_labels(targets, label_b, label_i):
            for target in targets:
                # ä½¿ç”¨ re.escape é¿å…åå­—ä¸­æœ‰ç‰¹æ®Šç¬¦è™Ÿå°è‡´ regex å ±éŒ¯
                for match in re.finditer(re.escape(target), sent):
                    match_start, match_end = match.span()
                    
                    # æª¢æŸ¥æ¯ä¸€å€‹ Token æ˜¯å¦è½åœ¨é€™å€‹ match çš„ç¯„åœå…§
                    for idx, span in enumerate(token_spans):
                        if span is None: continue
                        t_start, t_end = span
                        
                        # å¦‚æœ Token çš„ç¯„åœå®Œå…¨åœ¨ match ç¯„åœå…§
                        if t_start >= match_start and t_end <= match_end:
                            # å¦‚æœæ˜¯è©²å¯¦é«”çš„é–‹é ­
                            if t_start == match_start:
                                if tags[idx] == label2id["O"]: # é¿å…è¦†è“‹
                                    tags[idx] = label2id[label_b]
                            else:
                                if tags[idx] == label2id["O"]:
                                    tags[idx] = label2id[label_i]

        # åŸ·è¡Œæ¨™è¨»
        apply_labels(target_names, "B-NAME", "I-NAME")
        apply_labels(target_orgs, "B-ORG", "I-ORG")
        
        if len(tokens) > 0:
            final_data.append({
                "tokens": tokens,
                "ner_tags": tags
            })
            
    return final_data

# ==========================================
# 4. åŸ·è¡Œèˆ‡å„²å­˜
# ==========================================
# æ³¨æ„ï¼šé€™è£¡è¼¸å‡ºæª”åæ”¹ç‚º news_data.json ä»¥å€åˆ† mtr_news
mtr_json_data = process_data(raw_content)

output_file = "news_data.json" 
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(mtr_json_data, f, ensure_ascii=False, indent=2)

print(f"âœ… è™•ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(mtr_json_data)} æ¢æ··åˆæ•¸æ“šã€‚")
print(f"   - å·²æ¨™è¨» NAME: {target_names}")
print(f"   - å·²æ¨™è¨» ORG:  {target_orgs}")
print(f"ğŸ“ æª”æ¡ˆå·²å„²å­˜ç‚º: {output_file}")