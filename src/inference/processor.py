import re
from collections import defaultdict

class PIIProcessor:
    # =========================================================================
    # ğŸ”§ 1. Configuration & Rules (é…ç½®ä¸­å¿ƒ - æ¥­å‹™é‚è¼¯é›†ä¸­ç®¡ç†)
    # =========================================================================
    
    # ä¿¡å¿ƒé–€æª»èˆ‡çª—å£å¤§å° (Magic Numbers)
    DEFAULT_CONFIDENCE = 0.30
    CONTEXT_WINDOW_SIZE = 20

    # å…è¨±æ“´å¼µé‚Šç•Œçš„æ¨™ç±¤
    EXPANDABLE_LABELS = {"ID", "ACCOUNT", "PHONE", "LICENSE_PLATE"}
    
    # åŸºå»ºå¾Œç¶´è¦å‰‡ (é€šç”¨èªè¨€åº«)
    INFRA_SUFFIXES = [
        "é«˜éµ", "éµè·¯", "å¤§æ©‹", "éš§é“", "å¹¹ç·š", "å…¬è·¯", "é€šé“", "ç·š", "ç«™",
        "High Speed Rail", "Bridge", "Tunnel", "Line", "Station", "Rail"
    ]
    
    # å¹´é½¡é—œéµè©
    AGE_KEYWORDS = {'æ­²', 'years', 'yrs', 'age', 'old', 'ä»Šå¹´', 'å¹´ç´€', 'at'}

    # ç²µèªè¦å‰‡é…ç½®
    CANTONESE_PARTICLES = {'é»', 'åšŸ', 'å·¨', 'å’—', 'åº¦'}
    CANTONESE_VERBS = {'é', 'æ‰“', 'è¿”', 'åšŸ', 'å»', 'å·¦'} # è§¸ç™¼ç²’è©æª¢æŸ¥çš„å‰ç½®å‹•è©

    # åˆä½µç­–ç•¥é…ç½®ï¼šå®šç¾©ä¸åŒå¯¦é«”å…è¨±çš„æœ€å¤§æ–·è£‚è·é›¢ (Token Gap)
    MERGE_GAP_TOLERANCE = {
        "ORG": 1, 
        "ADDRESS": 1, 
        "NAME": 1, 
        "PHONE": 2, 
        "ACCOUNT": 2, 
        "ID": 1
    }

    # å„ªå…ˆç´šé…ç½®ï¼šè§£æ±ºé‡ç–Šæ™‚èª°è´ (æ•¸å€¼è¶Šå¤§è¶Šå„ªå…ˆ)
    # Regex æŠ“åˆ°çš„é€šå¸¸ç”±æ­¤é‚è¼¯ä¿è­·
    LABEL_PRIORITY = {
        "LICENSE_PLATE": 50, 
        "ID": 50, 
        "EMAIL": 50, 
        "PHONE": 40, 
        "NAME": 30, 
        "ORG": 20, 
        "ADDRESS": 20, 
        "ACCOUNT": 10
    }

    # Regex è¦å‰‡åº«
    REGEX_PATTERNS = {
        "ID": r'(?<![A-Za-z0-9])[A-Z]{1,2}\s?[0-9]{6}\(?[0-9A]\)?(?![A-Za-z0-9])',
        "LICENSE_PLATE": r'(?<!\bof\s)(?<!\bage\s)(?<!\bat\s)(?<![a-z])[A-Z]{2}\s?[0-9]{1,4}(?![0-9])',
        "EMAIL": r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
        "PHONE": r'(?<!\d)(?:\+852\s?)?[23569]\d{3}\s?\d{4}(?!\d)',
        "ACCOUNT": r'(?<!\d)\d{3}[-\s]?\d{3,6}[-\s]?\d{3,}(?!\d)'
    }

    # =========================================================================
    # âš™ï¸ 2. Initialization & Helpers
    # =========================================================================

    def __init__(self, text, raw_entities):
        self.text = text
        self.entities = raw_entities
        self.url_ranges = self._get_url_ranges()

    def _get_url_ranges(self):
        url_pattern = r'https?://[^\s,]+'
        return [match.span() for match in re.finditer(url_pattern, self.text)]

    def _is_in_forbidden_range(self, start, end):
        for r_start, r_end in self.url_ranges:
            if max(start, r_start) < min(end, r_end):
                return True
        return False

    def _is_valid_char_for_expansion(self, char, label):
        # è¼”åŠ©å‡½æ•¸ï¼šæª¢æŸ¥æ˜¯å¦ç‚º ASCII å­—æ¯æˆ–æ•¸å­— (æ’é™¤ä¸­æ–‡)
        def is_ascii_alnum(c):
            return c.isascii() and c.isalnum()

        if label == "ID": 
            # ID åªå…è¨± ASCII å­—æ¯ã€æ•¸å­—å’Œæ‹¬è™Ÿ
            return is_ascii_alnum(char) or char in "()"
            
        if label == "LICENSE_PLATE": 
            # è»Šç‰Œåªå…è¨± ASCII å­—æ¯ã€æ•¸å­—å’Œç©ºæ ¼
            return is_ascii_alnum(char) or char == " "
            
        if label in ["PHONE", "ACCOUNT"]: 
            return char.isdigit() or char in "-+ "
            
        return False

    # =========================================================================
    # ğŸš€ 3. Core Logic (Logic is now pure, referencing Configs)
    # =========================================================================

    def filter_low_confidence(self, threshold=None):
        # Use config default if not provided
        if threshold is None:
            threshold = self.DEFAULT_CONFIDENCE
            
        valid = []
        for r in self.entities:
            r['score'] = float(r['score'])
            if r['score'] > threshold and not self._is_in_forbidden_range(r['start'], r['end']):
                valid.append(r)
        self.entities = valid

    def normalize_infrastructure_labels(self):
        """åˆ©ç”¨å¾Œç¶´è¦å‰‡ (Suffix Rule) æ ¡æ­£åœ°é»æ¨™ç±¤"""
        if not self.entities: return
        
        self.entities.sort(key=lambda x: x['start'])
        is_infra_chain = [False] * len(self.entities)

        for i in range(len(self.entities) - 1, -1, -1):
            ent = self.entities[i]
            next_text = self.text[ent['end']:].lstrip()
            
            touches_suffix = False
            for suffix in self.INFRA_SUFFIXES:
                if next_text.startswith(suffix):
                    touches_suffix = True
                    break
            
            touches_next_infra = False
            if i < len(self.entities) - 1:
                next_ent = self.entities[i+1]
                # æª¢æŸ¥æ˜¯å¦æ¥è§¸ä¸‹ä¸€å€‹å·²ç¢ºèªçš„åŸºå»ºå¯¦é«”
                if next_ent['start'] - ent['end'] == 0 and is_infra_chain[i+1]:
                    touches_next_infra = True

            if touches_suffix or touches_next_infra:
                ent['entity_group'] = "ADDRESS"
                is_infra_chain[i] = True

    def merge_fragments(self):
        if not self.entities: return
        self.entities.sort(key=lambda x: x['start'])
        
        merged = []
        curr = self.entities[0]
        
        for next_ent in self.entities[1:]:
            # âœ… å¾é…ç½®è®€å– Gap Tolerance
            max_gap = self.MERGE_GAP_TOLERANCE.get(curr['entity_group'], 2)
            gap = next_ent['start'] - curr['end']
            
            if next_ent['entity_group'] == curr['entity_group'] and gap <= max_gap:
                curr['end'] = next_ent['end']
                curr['word'] = self.text[curr['start']:curr['end']]
                curr['score'] = max(float(curr['score']), float(next_ent['score']))
            else:
                merged.append(curr)
                curr = next_ent
        merged.append(curr)
        self.entities = merged

    def filter_cantonese_particles(self):
        """éæ¿¾ç²µèªåŠ©è© (Kill Rule)"""
        valid_entities = []
        for ent in self.entities:
            keep = True
            word = ent['word'].strip()
            
            if ent['entity_group'] == "NAME" and len(word) == 1 and word in self.CANTONESE_PARTICLES:
                prev_char_idx = ent['start'] - 1
                if prev_char_idx >= 0:
                    prev_char = self.text[prev_char_idx]
                    # âœ… å¾é…ç½®è®€å–å‹•è©è¡¨
                    if prev_char in self.CANTONESE_VERBS:
                        keep = False
            
            if keep:
                valid_entities.append(ent)
        self.entities = valid_entities

    def cut_infrastructure_suffix(self):
        processed = []
        for ent in self.entities:
            word = self.text[ent['start']:ent['end']]
            suffix_found = False
            for suffix in self.INFRA_SUFFIXES:
                if word.endswith(suffix):
                    suffix_len = len(suffix)
                    if len(word) > suffix_len:
                        ent['end'] -= suffix_len
                        ent['word'] = self.text[ent['start']:ent['end']]
                        ent['entity_group'] = "ADDRESS"
                        processed.append(ent)
                    suffix_found = True
                    break
            if not suffix_found:
                processed.append(ent)
        self.entities = processed

    def refine_address_age(self):
        valid_entities = []
        for ent in self.entities:
            keep_entity = True
            clean_word = ent['word'].strip()
            
            if clean_word.lower() in self.AGE_KEYWORDS:
                keep_entity = False
            elif re.match(r'^[,ï¼Œ\.\sã€‚ï¼Ÿï¼!?-]+$', ent['word']):
                keep_entity = False
            elif re.match(r'^\d+$', clean_word):
                keep_entity = False

            if keep_entity and ent['entity_group'] == "ADDRESS":
                current_word = self.text[ent['start']:ent['end']]
                next_text = self.text[ent['end']:].lstrip().lower()
                
                # âœ… ä½¿ç”¨é…ç½®çš„çª—å£å¤§å°
                prev_start = max(0, ent['start'] - self.CONTEXT_WINDOW_SIZE)
                prev_text = self.text[prev_start:ent['start']].lower()
                
                is_age_context = False
                for kw in self.AGE_KEYWORDS:
                    if next_text.startswith(kw):
                        is_age_context = True
                        break
                if not is_age_context:
                    if "age" in prev_text or "ä»Šå¹´" in prev_text or "æ­²" in prev_text:
                        is_age_context = True
                    if "of" in prev_text and "age" in prev_text:
                         is_age_context = True

                if is_age_context:
                    match = re.search(r'([,ï¼Œ\s]*\d+)$', current_word)
                    if match:
                        cut_len = len(match.group(1))
                        ent['end'] -= cut_len
                        ent['word'] = self.text[ent['start']:ent['end']]

            if ent['end'] <= ent['start'] or not ent['word'].strip():
                keep_entity = False
            
            if keep_entity:
                valid_entities.append(ent)
        self.entities = valid_entities

    def expand_boundaries(self):
        for ent in self.entities:
            label = ent['entity_group']
            if label not in self.EXPANDABLE_LABELS:
                continue
            new_start = ent['start']
            while new_start > 0:
                char = self.text[new_start - 1]
                if self._is_valid_char_for_expansion(char, label):
                    new_start -= 1
                else:
                    break
            new_end = ent['end']
            while new_end < len(self.text):
                char = self.text[new_end]
                if self._is_valid_char_for_expansion(char, label):
                    new_end += 1
                else:
                    break
            ent['start'] = new_start
            ent['end'] = new_end
            ent['word'] = self.text[new_start:new_end]

    def apply_regex_fallback(self):
        existing_ranges = [(e['start'], e['end']) for e in self.entities]
        new_entities = []
        for label, pattern in self.REGEX_PATTERNS.items():
            for match in re.finditer(pattern, self.text):
                start, end = match.span()
                if self._is_in_forbidden_range(start, end):
                    continue
                is_overlap = False
                for e_start, e_end in existing_ranges:
                    if max(start, e_start) < min(end, e_end):
                        is_overlap = True
                        break
                if not is_overlap:
                    new_entities.append({
                        "entity_group": label, "score": 1.0, 
                        "word": self.text[start:end], "start": start, "end": end
                    })
                    existing_ranges.append((start, end))
        self.entities.extend(new_entities)

    def resolve_overlaps(self):
        if not self.entities: return
        
        # âœ… å¾é…ç½®è®€å–å„ªå…ˆç´š
        self.entities.sort(key=lambda x: (
            self.LABEL_PRIORITY.get(x['entity_group'], 0), 
            x['end'] - x['start'], 
            x['score']
        ), reverse=True)
        
        final = []
        for ent in self.entities:
            is_overlapping = False
            for kept in final:
                if max(ent['start'], kept['start']) < min(ent['end'], kept['end']):
                    is_overlapping = True
                    break
            if not is_overlapping:
                final.append(ent)
        final.sort(key=lambda x: x['start'])
        self.entities = final

    def assign_numbered_tags(self):
        """
        Assigns consistent numbered tags.
        """
        type_counts = defaultdict(int)
        entity_value_map = {}

        for ent in self.entities:
            label = ent['entity_group']
            clean_word = ent['word'].strip().lower()
            key = (label, clean_word)

            if key not in entity_value_map:
                type_counts[label] += 1
                entity_value_map[key] = type_counts[label]
            
            ent['numbered_tag'] = f"{label}-{entity_value_map[key]}"

    # =========================================================================
    # ğŸš€ 4. Execution Pipeline
    # =========================================================================
    
    def process(self):
        self.filter_low_confidence()
        self.normalize_infrastructure_labels()
        self.merge_fragments()
        
        # Kill Phase
        self.cut_infrastructure_suffix()
        self.refine_address_age()
        self.filter_cantonese_particles()
        
        # Fill Phase
        self.expand_boundaries()
        self.apply_regex_fallback()
        
        # Finalize
        self.resolve_overlaps()
        self.assign_numbered_tags()
        return self.entities

    def get_masked_text(self):
        masked = self.text
        for ent in sorted(self.entities, key=lambda x: x['start'], reverse=True):
            if ent['end'] <= ent['start']: continue
            original_word = self.text[ent['start']:ent['end']]
            prefix = " " if original_word.startswith(" ") else ""
            suffix = " " if original_word.endswith(" ") else ""
            tag = f"{prefix}[{ent['numbered_tag']}]{suffix}"
            masked = masked[:ent['start']] + tag + masked[ent['end']:]
        return masked