import torch
import os
import sys
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from peft import PeftModel

# ===========================
# ğŸ”¥ 1. è·¯å¾‘è¨­å®š
# ===========================
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.append(project_root)

# ğŸ”¥ é—œéµï¼šå¿…é ˆåŒ¯å…¥ LABEL2ID ç­‰è¨­å®šï¼Œå‘Šè¨´æ¨¡å‹æœ‰å¹¾å€‹æ¨™ç±¤
from src.config import LORA_MODEL_PATH, BASE_MODEL_NAME, LABEL2ID, ID2LABEL
from src.inference.processor import PIIProcessor

class PIIPipeline:
    def __init__(self, model_path=LORA_MODEL_PATH, device=None):
        """
        åˆå§‹åŒ– PII Pipelineï¼šè² è²¬æ­£ç¢ºè¼‰å…¥ Base Model + LoRA Adapter
        """
        if device is None:
            device = 0 if torch.cuda.is_available() else -1
            
        print(f"ğŸ“‚ æ­£åœ¨å¾ {model_path} è¼‰å…¥æ¨¡å‹...")
        
        try:
            # 1. è¼‰å…¥ Tokenizer
            # å„ªå…ˆå˜—è©¦å¾ LoRA è³‡æ–™å¤¾è¼‰å…¥ï¼Œå¤±æ•—å‰‡å¾ Base Model è¼‰å…¥
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            except:
                print("âš ï¸ LoRA è³‡æ–™å¤¾æ‰¾ä¸åˆ° Tokenizerï¼Œæ”¹ç”¨ Base Model çš„ Tokenizerã€‚")
                self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)

            # ğŸ”¥ 2. é—œéµä¿®æ­£ï¼šå…ˆè¼‰å…¥ Base Modelï¼Œä¸¦å¼·åˆ¶æŒ‡å®šæ¨™ç±¤æ•¸é‡ (è§£æ±º Size Mismatch)
            print(f"âš™ï¸ æ­£åœ¨åˆå§‹åŒ– Base Model ({BASE_MODEL_NAME}) ä¸¦è¨­å®š {len(LABEL2ID)} å€‹æ¨™ç±¤...")
            base_model = AutoModelForTokenClassification.from_pretrained(
                BASE_MODEL_NAME,
                num_labels=len(LABEL2ID),  # å‘Šè¨´æ¨¡å‹ï¼šæˆ‘å€‘æœ‰ 15 å€‹æ¨™ç±¤ï¼Œä¸æ˜¯ 2 å€‹
                id2label=ID2LABEL,
                label2id=LABEL2ID,
                ignore_mismatched_sizes=True
            )

            # ğŸ”¥ 3. è¼‰å…¥ LoRA Adapter ä¸¦èˆ‡ Base Model åˆä½µ
            print("ğŸ”— æ­£åœ¨ç–ŠåŠ  LoRA æ¬Šé‡...")
            self.model = PeftModel.from_pretrained(base_model, model_path)
            self.model = self.model.merge_and_unload() # åˆä½µæ¬Šé‡ï¼Œæå‡æ¨è«–é€Ÿåº¦

        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            print("ğŸ’¡ è«‹ç¢ºèª src/config.py è£¡çš„ LABEL2ID æ˜¯å¦èˆ‡è¨“ç·´æ™‚ä¸€è‡´ã€‚")
            raise e
        
        # å»ºç«‹ HuggingFace Pipeline
        self.nlp_pipeline = pipeline(
            "token-classification", 
            model=self.model, 
            tokenizer=self.tokenizer, 
            aggregation_strategy="simple",
            device=device
        )
        print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼(Device: {'GPU' if device==0 else 'CPU'})")

    def predict(self, text):
        """
        è¼¸å…¥æ–‡å­—ï¼Œå›å‚³ï¼šåŸæ–‡ã€é®è“‹å¾Œæ–‡å­—ã€å¯¦é«”åˆ—è¡¨
        """
        # 1. AI æ¨è«–
        raw_results = self.nlp_pipeline(text)
        
        # 2. å¾Œè™•ç† (Processor Class)
        processor = PIIProcessor(text, raw_results)
        final_entities = processor.process()
        masked_text = processor.get_masked_text()
        
        return {
            "original": text,
            "masked": masked_text,
            "entities": final_entities
        }

# ===========================
# ğŸ§ª æ¸¬è©¦å€å¡Š
# ===========================
if __name__ == "__main__":
    pii_pipe = PIIPipeline()
    
    test_texts = [
        "Li Ka-shing resides at 12/F, Man Yee Building. ID: R123456(7)",
        "ä»–ä»Šå¹´ 31 æ­²ï¼Œä½åœ¨è§€å¡˜é“ 99 è™Ÿã€‚",
        "At the age of 82.",
        "æˆ‘çš„è»Šç‰Œä¿‚ AB1234ï¼ŒéŠ€è¡Œæˆ¶å£ 123-456-789ã€‚"
    ]
    
    print("\n" + "="*50)
    print("ğŸš€ PII é®è“‹æ¸¬è©¦é–‹å§‹")
    print("="*50)
    
    for text in test_texts:
        result = pii_pipe.predict(text)
        print(f"ğŸ“„ åŸæ–‡: {result['original']}")
        print(f"ğŸ›¡ï¸ é®è“‹: {result['masked']}")
        print("-" * 30)