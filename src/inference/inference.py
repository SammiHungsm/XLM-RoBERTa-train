import json
import os
import torch
import re
from collections import defaultdict
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from peft import PeftModel

# ==========================================
# 1. é…ç½®èˆ‡æ¨¡å‹è¼‰å…¥
# ==========================================
BASE_MODEL_NAME = "Davlan/xlm-roberta-large-ner-hrl"
LORA_MODEL_PATH = "./final_lora_model" 

LABEL_LIST = [
    "O", "B-NAME", "I-NAME", "B-ADDRESS", "I-ADDRESS", "B-PHONE", "I-PHONE", 
    "B-ID", "I-ID", "B-ACCOUNT", "I-ACCOUNT", "B-LICENSE_PLATE", "I-LICENSE_PLATE",
    "B-ORG", "I-ORG"
]

label2id = {l: i for i, l in enumerate(LABEL_LIST)}
id2label = {i: l for l, i in label2id.items()}

print("ğŸš€ [Step 1] Loading Model...")

if torch.cuda.is_available():
    device_id = 0
    print(f"âœ… Using Device: GPU (CUDA: {torch.cuda.get_device_name(0)})")
elif torch.backends.mps.is_available():
    device_id = "mps"
    print("ğŸ Using Device: Mac GPU (MPS)")
else:
    device_id = -1
    print("ğŸ¢ Using Device: CPU")

try:
    tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL_PATH)
    base_model = AutoModelForTokenClassification.from_pretrained(
        BASE_MODEL_NAME, 
        num_labels=len(label2id), 
        id2label=id2label, 
        label2id=label2id, 
        ignore_mismatched_sizes=True
    )
    model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)
    model.eval()
    
    nlp = pipeline(
        "token-classification", 
        model=model, 
        tokenizer=tokenizer, 
        aggregation_strategy="simple",
        device=device_id
    )
    print("âœ… Model Loaded Successfully!")
except Exception as e:
    print(f"âŒ Model loading failed: {e}")
    exit()

# ==========================================
# 2. æ ¸å¿ƒæ¸…ç†èˆ‡é‚è¼¯
# ==========================================
def clean_and_process_entities(results, text):
    
    # --- Phase 0: å»é‡ç–Š (De-overlap) ---
    merged_entities = sorted(results, key=lambda x: (x['start'], -x['end']))
    no_overlap = []
    if merged_entities:
        last = merged_entities[0]
        for curr in merged_entities[1:]:
            if curr['start'] < last['end']:
                if (curr['end'] - curr['start']) > (last['end'] - last['start']):
                    last = curr
                elif (curr['end'] - curr['start']) == (last['end'] - last['start']):
                    if curr['score'] > last['score']: last = curr
            else:
                no_overlap.append(last)
                last = curr
        no_overlap.append(last)
    merged_entities = no_overlap

    # --- Phase 1: è¦å‰‡éæ¿¾ (Rule-based Filter) ---
    final_cleaned = []
    blacklist = ["å¥åœ¨", "ä¸è©³", "æœªçŸ¥", "ç„¡æ¥­", "é›¢ç•°", "å–®èº«", "ä¸ä¾¿", "æ•´åˆ", "è™•ç†", "éŒ¯èª¤", "é«˜åº¦", "é—Šåº¦"]
    cantonese_noise = ["é»", "ä¿‚", "æ‰“", "ä¹‹å‰", "ä¸»å¸­", "è·", "ä»²è¦", "æµ"]
    
    for ent in merged_entities:
        word = ent['word'].strip().strip("ã€Œã€ã€ã€ã€Šã€‹()ï¼ˆï¼‰ã€‚ï¼Œã€ï¼ï¼Ÿï¼šï¼›")
        ent['word'] = word
        label = ent['entity_group']
        
        # 1. åŸºæœ¬éæ¿¾
        if not word or ent['score'] < 0.45: continue
        if word in blacklist or word in cantonese_noise: continue
        if re.match(r'^[\W_]+$', word): continue

        # 2. URL ä¿è­· (çµ•å°ä¸è¦æ‰ URL è£¡çš„å­—)
        if any(x in word.lower() for x in ['http', 'www.', '.com', '.html', 'mingpao']): continue

        # 3. å–®ä½éæ¿¾ (åªé‡å° Phone/Account)
        if label in ['PHONE', 'ACCOUNT']:
            if re.search(r'\d+(cm|kg|km|m|g|ml|L|Hz|GB|MB|KB|ft|in)$', word, re.IGNORECASE): continue

        # 4. ç¬¦è™Ÿæ¸…ç† (Account/Phone åªèƒ½æœ‰æ•¸å­—å’Œç¬¦è™Ÿ)
        if label in ['PHONE', 'ACCOUNT']:
            cleaned = re.sub(r'[^\d\+\-\(\)\s]', '', word).strip()
            if len(cleaned) < 3: continue
            ent['word'] = cleaned

        # 5. ID/Plate æ¸…ç†ä¸­æ–‡
        if label in ['ID', 'LICENSE_PLATE']:
            cleaned = re.sub(r'[\u4e00-\u9fff]+', '', word).strip()
            if len(cleaned) < 2: continue
            ent['word'] = cleaned

        # 6. [Rule H] è‹±æ–‡åè‡ªå‹•è£œå…¨ (ä¿®å¾© "Sam" -> "Sammi")
        if label == 'NAME':
            # å¦‚æœçµå°¾æ˜¯è‹±æ–‡å­—æ¯
            if re.search(r'[a-zA-Z]$', ent['word']):
                remaining_text = text[ent['end']:]
                # æª¢æŸ¥å¾Œé¢æ˜¯å¦ç·Šæ¥è‹±æ–‡å­—æ¯
                suffix_match = re.match(r'^([a-z]+)', remaining_text)
                if suffix_match:
                    suffix = suffix_match.group(1)
                    ent['end'] += len(suffix)
                    ent['word'] += suffix
                    # print(f"ğŸ”§ Auto-completed name: {word} -> {ent['word']}")

        # 7. ID æ‹¬è™Ÿè£œå…¨
        if label == 'ID' and ent['end'] < len(text) and text[ent['end']] == ')':
             ent['end'] += 1; ent['word'] += ')'

        final_cleaned.append(ent)

    # --- Phase 2: æ­£å‰‡è£œæ¼ (Regex Fallback) ---
    existing_ranges = set()
    for ent in final_cleaned:
        for i in range(ent['start'], ent['end']): existing_ranges.add(i)

    # [Helper] æª¢æŸ¥æ˜¯å¦åœ¨ URL å…§
    def is_url_part(start, end, full_text):
        surroundings = ['/', '-', '.', '=', '?', '_']
        # æª¢æŸ¥å‰é¢
        if start > 0 and full_text[start-1] in surroundings: return True
        # æª¢æŸ¥å¾Œé¢
        if end < len(full_text) and full_text[end] in surroundings: return True
        return False

    fallback_patterns = [
        # 1. é¦™æ¸¯èº«ä»½è­‰ (å„ªå…ˆç´šæœ€é«˜ï¼é˜²æ­¢è¢« Phone/Account æ¶èµ°)
        # æ”¯æ´æ‹¬è™Ÿ A123456(7) æˆ– A1234567
        {"name": "ID", "pattern": r'\b[A-Z]{1,2}\d{6}\(?[0-9A]\)?\b'},
        
        # 2. é¦™æ¸¯é›»è©± (8ä½)
        {"name": "PHONE", "pattern": r'\b[23569]\d{7}\b'},
        
        # 3. éŠ€è¡Œæˆ¶å£ (10-12ä½)
        {"name": "ACCOUNT", "pattern": r'\b\d{10,12}\b'}
    ]

    for rule in fallback_patterns:
        for match in re.finditer(rule["pattern"], text):
            start, end = match.span()
            
            # [Fix 1] æª¢æŸ¥é‡ç–Š (å¦‚æœ AI å·²ç¶“æ‰äº†é€™å€‹ä½ç½®ï¼Œè·³é)
            if any(i in existing_ranges for i in range(start, end)):
                # ç‰¹ä¾‹ï¼šå¦‚æœæ˜¯ IDï¼Œä¸” AI æ‰å¾—ä¸å®Œæ•´ (ä¾‹å¦‚åªæ‰äº†æ•¸å­—)ï¼Œæˆ‘å€‘ç”¨ Regex è¦†è“‹å®ƒï¼
                if rule["name"] in ["ID", "ACCOUNT"]:
                    # ç§»é™¤èˆŠçš„ AI çµæœï¼Œæ”¹ç”¨ Regex (æ›´æº–ç¢º)
                    final_cleaned = [e for e in final_cleaned if not (e['start'] >= start and e['end'] <= end)]
                    # æ¸…é™¤èˆŠ Range
                    # (é€™è£¡ç°¡åŒ–è™•ç†ï¼šç›´æ¥åŠ å…¥æ–° IDï¼Œå¾Œé¢ç·¨è™Ÿæœƒè‡ªå‹•è™•ç†)
                else:
                    continue
            
            # [Fix 2] URL ä¿è­· (é‡å° Phone/Account)
            if rule["name"] in ["PHONE", "ACCOUNT"]:
                if is_url_part(start, end, text):
                    # print(f"ğŸ›¡ï¸ Ignored URL part: {match.group()}")
                    continue

            final_cleaned.append({
                "entity_group": rule["name"], "word": match.group(),
                "start": start, "end": end, "score": 1.0, "numbered_tag": ""
            })
            for i in range(start, end): existing_ranges.add(i)

    # --- Phase 3: ç·¨è™Ÿ ---
    final_output = []
    counters = defaultdict(int)
    registry = {}
    
    final_cleaned.sort(key=lambda x: x['start'])
    
    for ent in final_cleaned:
        label = ent['entity_group']
        key = (label, ent['word'])
        if key in registry:
            seq = registry[key]
        else:
            counters[label] += 1
            seq = counters[label]
            registry[key] = seq
        ent['numbered_tag'] = f"{label}-{seq:02d}"
        final_output.append(ent)
        
    return final_output

def mask_text(text, entities):
    masked = text
    # å¾å¾Œå¾€å‰æ›¿æ›ï¼Œé¿å…ç´¢å¼•åç§»
    for ent in sorted(entities, key=lambda x: x['start'], reverse=True):
        tag = f"[{ent['numbered_tag']}]"
        masked = masked[:ent['start']] + tag + masked[ent['end']:]
    return masked

# ==========================================
# 3. åŸ·è¡Œæ¸¬è©¦
# ==========================================
if __name__ == "__main__":
    # å¼·åˆ¶æ¸¬è©¦ä¸€äº› Edge Case
    test_inputs = [
        "æ“šä¸­åœ‹åœ‹å®¶éµè·¯é›†åœ˜æœ‰é™å…¬å¸ä»Šï¼ˆ4æ—¥ï¼‰æŠ«éœ²ï¼ŒåŸæ–‡ç¶²å€ï¼šhttps://news.mingpao.com/ins/article/20260104/s00004",
        "æå˜‰èª ä½åœ¨é¦™æ¸¯ä¸­ç’°ï¼Œé›»è©± 9123 4567ã€‚",
        "æˆ‘çš„ ID æ˜¯ A123456(7)ï¼Œæˆ¶å£ 123-456-789ã€‚",
        "Sammi ä¹‹å‰æ‰“éé»ã€‚",
        "Bank Account = 274542182882"
    ]
    
    if os.path.exists("train/test_data.json"):
        with open("train/test_data.json", "r", encoding="utf-8") as f:
            file_inputs = json.load(f)
            # å°‡æ–‡ä»¶å…§å®¹åŠ åˆ°æ¸¬è©¦åˆ—è¡¨å¾Œé¢
            test_inputs.extend(file_inputs)

    print("="*60)
    # å»é‡
    test_inputs = list(dict.fromkeys(test_inputs)) 
    
    for text in test_inputs:
        print(f"\nğŸ“ Input: {text}")
        results = nlp(text)
        processed = clean_and_process_entities(results, text)
        masked = mask_text(text, processed)
        print(f"ğŸ­ Masked: {masked}")
        for e in processed:
            print(f"   - [{e['numbered_tag']}] {e['word']} ({e['score']:.1%})")
    print("="*60)