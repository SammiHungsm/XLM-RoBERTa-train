import os
import json
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from peft import PeftModel

# åŒ¯å…¥å°ˆæ¡ˆé…ç½®èˆ‡å·¥å…·
from src.config import BASE_MODEL_NAME, LORA_MODEL_PATH, ID2LABEL, LABEL2ID
from src.inference.utils import clean_and_process_entities, mask_text

def run_inference():
    print("ğŸš€ [1/3] Loading Model and LoRA Adapter...")
    
    # è‡ªå‹•åµæ¸¬è¨­å‚™
    device = 0 if torch.cuda.is_available() else -1
    
    # è¼‰å…¥åˆ†è©å™¨
    tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL_PATH)
    
    # è¼‰å…¥åŸºç¤æ¨¡å‹ä¸¦æ›´æ› Head (é‡å° 15 é¡æ¨™ç±¤)
    base = AutoModelForTokenClassification.from_pretrained(
        BASE_MODEL_NAME, 
        num_labels=len(LABEL2ID), 
        id2label=ID2LABEL, 
        label2id=LABEL2ID, 
        ignore_mismatched_sizes=True
    )
    
    # æ›è¼‰å¾®èª¿å¾Œçš„ LoRA æ¬Šé‡
    model = PeftModel.from_pretrained(base, LORA_MODEL_PATH).eval()
    
    # å°è£ Pipeline
    nlp = pipeline(
        "token-classification", 
        model=model, 
        tokenizer=tokenizer, 
        aggregation_strategy="simple", 
        device=device
    )

    # å®šä½æ¸¬è©¦æ•¸æ“šè·¯å¾‘
    # é€™è£¡ä½¿ç”¨ Path(__file__) ç¢ºä¿ç›¸å°æ–¼ç•¶å‰è…³æœ¬å®šä½æª”æ¡ˆ
    current_dir = Path(__file__).parent
    input_file = current_dir / "test_data.json"
    output_file = Path("inference_results.json")

    print(f"ğŸ“‚ [2/3] Reading input from: {input_file}")
    
    if input_file.exists():
        with open(input_file, "r", encoding="utf-8") as f:
            test_data = json.load(f)
            if isinstance(test_data, dict):
                test_data = test_data.get("data", [])
    else:
        print(f"âš ï¸ Warning: {input_file} not found. Using default test cases.")
        test_data = ["æå˜‰èª ä½åœ¨é¦™æ¸¯ä¸­ç’°ï¼Œé›»è©±æ˜¯ 98765432ã€‚", "æˆ‘çš„ ID æ˜¯ A123456(7)ã€‚"]

    print(f"ğŸ§ª [3/3] Processing {len(test_data)} samples...")

    output = []
    
    # é—œé–‰æ¢¯åº¦è¨ˆç®—ï¼Œç¯€çœå…§å­˜ä¸¦åŠ é€Ÿæ¨è«–
    with torch.no_grad():
        for idx, text in enumerate(test_data):
            # 1. AI æ¨¡å‹é æ¸¬
            raw_predictions = nlp(text)
            
            # 2. å¾Œè™•ç†ï¼šæ¸…ç†ã€å»é‡ç–Šã€Regex è£œæ¼ã€ç·¨è™Ÿ
            # é€™è£¡çš„ clean_and_process_entities å·²è™•ç† float32 è½‰æ›å•é¡Œ
            ents = clean_and_process_entities(raw_predictions, text)
            
            # 3. ç”Ÿæˆæ©ç¢¼æ–‡æœ¬
            masked = mask_text(text, ents)
            print(f"\n[#{idx}] åŸæ–‡: {text}")
            print(f"[#{idx}] é®è“‹: {masked}")
            output.append({
                "id": idx,
                "original": text,
                "masked": masked,
                "entities": ents
            })
            
            # æ¯ 100 æ¢å°ä¸€æ¬¡é€²åº¦
            if (idx + 1) % 100 == 0:
                print(f"â³ Progress: {idx + 1}/{len(test_data)}")

    # 4. å„²å­˜çµæœ
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Inference Completed! Results saved to: {output_file.absolute()}")

if __name__ == "__main__":
    run_inference()